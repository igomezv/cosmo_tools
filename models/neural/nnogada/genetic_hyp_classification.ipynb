{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>delta</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135.689107</td>\n",
       "      <td>32.494632</td>\n",
       "      <td>23.87882</td>\n",
       "      <td>22.27530</td>\n",
       "      <td>20.39501</td>\n",
       "      <td>19.16573</td>\n",
       "      <td>18.79371</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144.826101</td>\n",
       "      <td>31.274185</td>\n",
       "      <td>24.77759</td>\n",
       "      <td>22.83188</td>\n",
       "      <td>22.58444</td>\n",
       "      <td>21.16812</td>\n",
       "      <td>21.61427</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142.188790</td>\n",
       "      <td>35.582444</td>\n",
       "      <td>25.26307</td>\n",
       "      <td>22.66389</td>\n",
       "      <td>20.60976</td>\n",
       "      <td>19.34857</td>\n",
       "      <td>18.94827</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>338.741038</td>\n",
       "      <td>-0.402828</td>\n",
       "      <td>22.13682</td>\n",
       "      <td>23.77656</td>\n",
       "      <td>21.61162</td>\n",
       "      <td>20.50454</td>\n",
       "      <td>19.25010</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345.282593</td>\n",
       "      <td>21.183866</td>\n",
       "      <td>19.43718</td>\n",
       "      <td>17.58028</td>\n",
       "      <td>16.49747</td>\n",
       "      <td>15.97711</td>\n",
       "      <td>15.54461</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        alpha      delta         u         g         r         i         z  \\\n",
       "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
       "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
       "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
       "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
       "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
       "\n",
       "    class  \n",
       "0  GALAXY  \n",
       "1  GALAXY  \n",
       "2  GALAXY  \n",
       "3  GALAXY  \n",
       "4  GALAXY  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/igomezv/nnogada/main/data/star_classification.csv\"\n",
    "data = pd.read_csv(url)\n",
    "cols = ['alpha','delta','u','g','r','i','z','class']\n",
    "data = data[cols]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        alpha      delta         u         g         r         i         z  \\\n",
      "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
      "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
      "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
      "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
      "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
      "\n",
      "   class  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n"
     ]
    }
   ],
   "source": [
    "data[\"class\"]=[0 if i == \"GALAXY\" else 1 if i == \"STAR\" else 2 for i in data[\"class\"]]\n",
    "print(data.head())\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function layers\n",
    "\n",
    "# f1 = lambda x: Dense(x, activation='relu')      #ReLU\n",
    "# f2 = lambda x: Dense(x, activation='elu')       #ELU\n",
    "# f3 = lambda x: keras.layers.LeakyReLU(0.3)      #LReLU\n",
    "# f4 = lambda x: Dense(x, kernel_initializer='lecun_normal', activation='selu')   #SELU\n",
    "\n",
    "# f_names = [\"ReLU\", \"ELU\", \"LReLU\", \"SELU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([3, 4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([200, 100]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-5,1e-4,1e-3, 0.01])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([128,256])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=50,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=1)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 50\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into X and Y and implement hot_ones in Y\n",
    "def prepare_dataset(data):\n",
    "    X, Y = np.empty((0)), np.empty((0))\n",
    "    X = data[:,0:7]\n",
    "    Y = data[:,7]\n",
    "    Y = to_categorical(Y, num_classes=3)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation and test sets\n",
    "X,Y = prepare_dataset(data)\n",
    "\n",
    "# Defines ratios, w.r.t. whole dataset.\n",
    "ratio_train = 0.8\n",
    "ratio_val = 0.1\n",
    "ratio_test = 0.1\n",
    "\n",
    "# Produces test split.\n",
    "x_, X_test, y_, Y_test = split(X, Y, test_size = ratio_test, random_state=0)\n",
    "\n",
    "# Adjusts val ratio, w.r.t. remaining dataset.\n",
    "ratio_remaining = 1 - ratio_test\n",
    "ratio_val_adjusted = ratio_val / ratio_remaining\n",
    "\n",
    "# Produces train and val splits.\n",
    "X_train, X_val, Y_train, Y_val = split(x_, y_, test_size=ratio_val_adjusted, random_state=0)\n",
    "\n",
    "# Normalize and scale the input sets.\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "X_val   = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:1])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[1:2])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[2:4])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[4:5])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(num_units, input_shape=(int(X_train.shape[1]),)))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(3, activation=tf.nn.softmax))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=1, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Accuracy:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.2      # probability for crossover\n",
    "    P_MUTATION = 0.5         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1  # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 688us/step - loss: 0.3295 - accuracy: 0.8801\n",
      "Accuracy: 0.8801000118255615 , Elapsed time: 48.08710741996765\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 735us/step - loss: 0.3386 - accuracy: 0.8801\n",
      "Accuracy: 0.8801000118255615 , Elapsed time: 65.18973326683044\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 700us/step - loss: 0.3411 - accuracy: 0.8766\n",
      "Accuracy: 0.8766000270843506 , Elapsed time: 48.32681107521057\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 734us/step - loss: 0.3304 - accuracy: 0.8791\n",
      "Accuracy: 0.8791000247001648 , Elapsed time: 35.234272718429565\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 661us/step - loss: 0.3687 - accuracy: 0.8673\n",
      "Accuracy: 0.8672999739646912 , Elapsed time: 47.82185673713684\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 775us/step - loss: 0.3439 - accuracy: 0.8724\n",
      "Accuracy: 0.8723999857902527 , Elapsed time: 65.18518829345703\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 607us/step - loss: 0.3841 - accuracy: 0.8564\n",
      "Accuracy: 0.8564000129699707 , Elapsed time: 42.97129797935486\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 660us/step - loss: 0.3206 - accuracy: 0.8820\n",
      "Accuracy: 0.8820000290870667 , Elapsed time: 31.07162356376648\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin     \tavg     \tmax     \n",
      "0  \t8     \t0.320609\t0.344608\t0.384136\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 652us/step - loss: 0.3663 - accuracy: 0.8663\n",
      "Accuracy: 0.8662999868392944 , Elapsed time: 49.027743101119995\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 708us/step - loss: 0.3326 - accuracy: 0.8780\n",
      "Accuracy: 0.878000020980835 , Elapsed time: 36.41144800186157\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 685us/step - loss: 0.3231 - accuracy: 0.8763\n",
      "Accuracy: 0.8762999773025513 , Elapsed time: 31.17573070526123\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t3     \t0.320609\t0.333946\t0.366317\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 682us/step - loss: 0.3610 - accuracy: 0.8672\n",
      "Accuracy: 0.8672000169754028 , Elapsed time: 30.809590578079224\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 708us/step - loss: 0.3430 - accuracy: 0.8717\n",
      "Accuracy: 0.8716999888420105 , Elapsed time: 32.61546206474304\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 649us/step - loss: 0.3743 - accuracy: 0.8599\n",
      "Accuracy: 0.8598999977111816 , Elapsed time: 49.384833574295044\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t3     \t0.320609\t0.339436\t0.37428 \n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 692us/step - loss: 0.3249 - accuracy: 0.8810\n",
      "Accuracy: 0.8809999823570251 , Elapsed time: 44.266276359558105\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 662us/step - loss: 0.3240 - accuracy: 0.8794\n",
      "Accuracy: 0.8794000148773193 , Elapsed time: 30.919562578201294\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 804us/step - loss: 0.3247 - accuracy: 0.8835\n",
      "Accuracy: 0.8834999799728394 , Elapsed time: 32.28601288795471\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 766us/step - loss: 0.3345 - accuracy: 0.8772\n",
      "Accuracy: 0.8772000074386597 , Elapsed time: 43.750598192214966\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 674us/step - loss: 0.3404 - accuracy: 0.8761\n",
      "Accuracy: 0.8761000037193298 , Elapsed time: 31.271491289138794\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t5     \t0.320609\t0.329018\t0.340438\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 680us/step - loss: 0.3610 - accuracy: 0.8675\n",
      "Accuracy: 0.8675000071525574 , Elapsed time: 30.97727108001709\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 646us/step - loss: 0.3390 - accuracy: 0.8752\n",
      "Accuracy: 0.8751999735832214 , Elapsed time: 43.55867099761963\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 704us/step - loss: 0.3287 - accuracy: 0.8790\n",
      "Accuracy: 0.8790000081062317 , Elapsed time: 27.052422523498535\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 684us/step - loss: 0.3388 - accuracy: 0.8709\n",
      "Accuracy: 0.8708999752998352 , Elapsed time: 31.356168746948242\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 693us/step - loss: 0.3289 - accuracy: 0.8806\n",
      "Accuracy: 0.8805999755859375 , Elapsed time: 31.226248025894165\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t5     \t0.320609\t0.333331\t0.361049\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 847us/step - loss: 0.3535 - accuracy: 0.8707\n",
      "Accuracy: 0.8707000017166138 , Elapsed time: 43.42190670967102\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 704us/step - loss: 0.3161 - accuracy: 0.8834\n",
      "Accuracy: 0.883400022983551 , Elapsed time: 37.066906213760376\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 697us/step - loss: 0.3673 - accuracy: 0.8643\n",
      "Accuracy: 0.864300012588501 , Elapsed time: 31.548205614089966\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 638us/step - loss: 0.3749 - accuracy: 0.8603\n",
      "Accuracy: 0.8603000044822693 , Elapsed time: 43.417075395584106\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 673us/step - loss: 0.3232 - accuracy: 0.8814\n",
      "Accuracy: 0.8813999891281128 , Elapsed time: 30.701828241348267\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 663us/step - loss: 0.3307 - accuracy: 0.8834\n",
      "Accuracy: 0.883400022983551 , Elapsed time: 31.549854516983032\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t6     \t0.31611 \t0.338375\t0.374905\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 705us/step - loss: 0.3236 - accuracy: 0.8800\n",
      "Accuracy: 0.8799999952316284 , Elapsed time: 31.920700788497925\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 750us/step - loss: 0.3623 - accuracy: 0.8667\n",
      "Accuracy: 0.8666999936103821 , Elapsed time: 43.220914125442505\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 690us/step - loss: 0.3410 - accuracy: 0.8791\n",
      "Accuracy: 0.8791000247001648 , Elapsed time: 30.91136407852173\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t3     \t0.31611 \t0.329583\t0.362285\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 736us/step - loss: 0.3326 - accuracy: 0.8796\n",
      "Accuracy: 0.8795999884605408 , Elapsed time: 36.97207951545715\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t1     \t0.31611 \t0.327645\t0.362285\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 684us/step - loss: 0.3390 - accuracy: 0.8724\n",
      "Accuracy: 0.8723999857902527 , Elapsed time: 45.78027009963989\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 675us/step - loss: 0.3877 - accuracy: 0.8581\n",
      "Accuracy: 0.8580999970436096 , Elapsed time: 31.19171714782715\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 795us/step - loss: 0.3385 - accuracy: 0.8758\n",
      "Accuracy: 0.8758000135421753 , Elapsed time: 32.38931727409363\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 700us/step - loss: 0.3338 - accuracy: 0.8796\n",
      "Accuracy: 0.8795999884605408 , Elapsed time: 36.635587215423584\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 703us/step - loss: 0.3296 - accuracy: 0.8807\n",
      "Accuracy: 0.8806999921798706 , Elapsed time: 36.27057695388794\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t5     \t0.31611 \t0.335553\t0.387743\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 739us/step - loss: 0.3272 - accuracy: 0.8804\n",
      "Accuracy: 0.8804000020027161 , Elapsed time: 36.381836891174316\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 712us/step - loss: 0.3358 - accuracy: 0.8801\n",
      "Accuracy: 0.8801000118255615 , Elapsed time: 37.35611033439636\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 42 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 681us/step - loss: 0.3782 - accuracy: 0.8638\n",
      "Accuracy: 0.8637999892234802 , Elapsed time: 30.838213682174683\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 43 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 701us/step - loss: 0.3263 - accuracy: 0.8794\n",
      "Accuracy: 0.8794000148773193 , Elapsed time: 35.4190948009491\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 44 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 695us/step - loss: 0.3268 - accuracy: 0.8791\n",
      "Accuracy: 0.8791000247001648 , Elapsed time: 36.01067614555359\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t5     \t0.31611 \t0.331269\t0.378244\n",
      "\n",
      "--------------- Starting trial: 45 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 732us/step - loss: 0.3219 - accuracy: 0.8836\n",
      "Accuracy: 0.8835999965667725 , Elapsed time: 35.95262026786804\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 46 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 787us/step - loss: 0.3242 - accuracy: 0.8804\n",
      "Accuracy: 0.8804000020027161 , Elapsed time: 36.38557004928589\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 47 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 718us/step - loss: 0.3310 - accuracy: 0.8815\n",
      "Accuracy: 0.8815000057220459 , Elapsed time: 31.93661594390869\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 48 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 748us/step - loss: 0.3274 - accuracy: 0.8801\n",
      "Accuracy: 0.8801000118255615 , Elapsed time: 43.67160081863403\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 49 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 770us/step - loss: 0.5109 - accuracy: 0.8054\n",
      "Accuracy: 0.805400013923645 , Elapsed time: 42.45824074745178\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t5     \t0.31611 \t0.349203\t0.510861\n",
      "-- Best Individual =  [0, 0, 1, 0, 1]\n",
      "-- Best Fitness =  0.31610992550849915\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 31.158698364098868 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 8  # max of individuals per generation\n",
    "max_generations = 10   # number of generations\n",
    "gene_length = 5      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001 , Batch size: 256\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:4])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[4:5])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    \n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1], \n",
    "          ', Learning rate:', best_learning_rate[-1], ', Batch size:', best_batch_size[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.321867</td>\n",
       "      <td>0.8836</td>\n",
       "      <td>35.952620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.324743</td>\n",
       "      <td>0.8835</td>\n",
       "      <td>32.286013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.330729</td>\n",
       "      <td>0.8834</td>\n",
       "      <td>31.549855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.316110</td>\n",
       "      <td>0.8834</td>\n",
       "      <td>37.066906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.320609</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>31.071624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.331032</td>\n",
       "      <td>0.8815</td>\n",
       "      <td>31.936616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.323203</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>30.701828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.324878</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>44.266276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.329561</td>\n",
       "      <td>0.8807</td>\n",
       "      <td>36.270577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.328934</td>\n",
       "      <td>0.8806</td>\n",
       "      <td>31.226248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.327204</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>36.381837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.324202</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>36.385570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.335809</td>\n",
       "      <td>0.8801</td>\n",
       "      <td>37.356110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.327444</td>\n",
       "      <td>0.8801</td>\n",
       "      <td>43.671601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.329479</td>\n",
       "      <td>0.8801</td>\n",
       "      <td>48.087107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.338619</td>\n",
       "      <td>0.8801</td>\n",
       "      <td>65.189733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.323614</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>31.920701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.333754</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>36.635587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.332591</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>36.972080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.323972</td>\n",
       "      <td>0.8794</td>\n",
       "      <td>30.919563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.326299</td>\n",
       "      <td>0.8794</td>\n",
       "      <td>35.419095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.340978</td>\n",
       "      <td>0.8791</td>\n",
       "      <td>30.911364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.330425</td>\n",
       "      <td>0.8791</td>\n",
       "      <td>35.234273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.326759</td>\n",
       "      <td>0.8791</td>\n",
       "      <td>36.010676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.328720</td>\n",
       "      <td>0.8790</td>\n",
       "      <td>27.052423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.332580</td>\n",
       "      <td>0.8780</td>\n",
       "      <td>36.411448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.334499</td>\n",
       "      <td>0.8772</td>\n",
       "      <td>43.750598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.341073</td>\n",
       "      <td>0.8766</td>\n",
       "      <td>48.326811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.323117</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>31.175731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.340438</td>\n",
       "      <td>0.8761</td>\n",
       "      <td>31.271491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.338519</td>\n",
       "      <td>0.8758</td>\n",
       "      <td>32.389317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.338954</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>43.558671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.339010</td>\n",
       "      <td>0.8724</td>\n",
       "      <td>45.780270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.343860</td>\n",
       "      <td>0.8724</td>\n",
       "      <td>65.185188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.343042</td>\n",
       "      <td>0.8717</td>\n",
       "      <td>32.615462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.338760</td>\n",
       "      <td>0.8709</td>\n",
       "      <td>31.356169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.353549</td>\n",
       "      <td>0.8707</td>\n",
       "      <td>43.421907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.361049</td>\n",
       "      <td>0.8675</td>\n",
       "      <td>30.977271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.368664</td>\n",
       "      <td>0.8673</td>\n",
       "      <td>47.821857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.361011</td>\n",
       "      <td>0.8672</td>\n",
       "      <td>30.809591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>256</td>\n",
       "      <td>0.362285</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>43.220914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.366317</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>49.027743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.367284</td>\n",
       "      <td>0.8643</td>\n",
       "      <td>31.548206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.378244</td>\n",
       "      <td>0.8638</td>\n",
       "      <td>30.838214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.374905</td>\n",
       "      <td>0.8603</td>\n",
       "      <td>43.417075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.374280</td>\n",
       "      <td>0.8599</td>\n",
       "      <td>49.384834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>256</td>\n",
       "      <td>0.387743</td>\n",
       "      <td>0.8581</td>\n",
       "      <td>31.191717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.384136</td>\n",
       "      <td>0.8564</td>\n",
       "      <td>42.971298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>256</td>\n",
       "      <td>0.510861</td>\n",
       "      <td>0.8054</td>\n",
       "      <td>42.458241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss  Accuracy  \\\n",
       "0             3        200        0.00100         256  0.321867    0.8836   \n",
       "1             4        100        0.01000         256  0.324743    0.8835   \n",
       "2             4        100        0.01000         256  0.330729    0.8834   \n",
       "3             3        200        0.00100         256  0.316110    0.8834   \n",
       "4             4        100        0.01000         256  0.320609    0.8820   \n",
       "5             4        100        0.01000         256  0.331032    0.8815   \n",
       "6             4        100        0.01000         256  0.323203    0.8814   \n",
       "7             3        100        0.01000         128  0.324878    0.8810   \n",
       "8             3        200        0.00100         256  0.329561    0.8807   \n",
       "9             4        100        0.01000         256  0.328934    0.8806   \n",
       "10            3        200        0.00100         256  0.327204    0.8804   \n",
       "11            3        200        0.00100         256  0.324202    0.8804   \n",
       "12            3        200        0.00100         256  0.335809    0.8801   \n",
       "13            4        200        0.00100         256  0.327444    0.8801   \n",
       "14            4        100        0.01000         128  0.329479    0.8801   \n",
       "15            4        200        0.01000         128  0.338619    0.8801   \n",
       "16            4        100        0.01000         256  0.323614    0.8800   \n",
       "17            3        200        0.00100         256  0.333754    0.8796   \n",
       "18            3        200        0.00100         256  0.332591    0.8796   \n",
       "19            4        100        0.01000         256  0.323972    0.8794   \n",
       "20            3        200        0.00100         256  0.326299    0.8794   \n",
       "21            4        100        0.01000         256  0.340978    0.8791   \n",
       "22            3        200        0.01000         256  0.330425    0.8791   \n",
       "23            3        200        0.00100         256  0.326759    0.8791   \n",
       "24            3        100        0.00100         256  0.328720    0.8790   \n",
       "25            3        200        0.01000         256  0.332580    0.8780   \n",
       "26            4        200        0.01000         256  0.334499    0.8772   \n",
       "27            4        100        0.01000         128  0.341073    0.8766   \n",
       "28            4        100        0.01000         256  0.323117    0.8763   \n",
       "29            4        100        0.01000         256  0.340438    0.8761   \n",
       "30            4        100        0.01000         256  0.338519    0.8758   \n",
       "31            3        100        0.01000         128  0.338954    0.8752   \n",
       "32            3        100        0.00100         128  0.339010    0.8724   \n",
       "33            4        200        0.01000         128  0.343860    0.8724   \n",
       "34            4        100        0.01000         256  0.343042    0.8717   \n",
       "35            4        100        0.01000         256  0.338760    0.8709   \n",
       "36            4        200        0.01000         256  0.353549    0.8707   \n",
       "37            4        100        0.01000         256  0.361049    0.8675   \n",
       "38            4        100        0.00010         128  0.368664    0.8673   \n",
       "39            4        100        0.01000         256  0.361011    0.8672   \n",
       "40            4        200        0.00010         256  0.362285    0.8667   \n",
       "41            4        100        0.00010         128  0.366317    0.8663   \n",
       "42            4        100        0.01000         256  0.367284    0.8643   \n",
       "43            4        100        0.01000         256  0.378244    0.8638   \n",
       "44            3        100        0.01000         128  0.374905    0.8603   \n",
       "45            4        100        0.01000         128  0.374280    0.8599   \n",
       "46            4        100        0.00010         256  0.387743    0.8581   \n",
       "47            3        100        0.00010         128  0.384136    0.8564   \n",
       "48            4        200        0.00001         256  0.510861    0.8054   \n",
       "\n",
       "    Elapsed time  \n",
       "0      35.952620  \n",
       "1      32.286013  \n",
       "2      31.549855  \n",
       "3      37.066906  \n",
       "4      31.071624  \n",
       "5      31.936616  \n",
       "6      30.701828  \n",
       "7      44.266276  \n",
       "8      36.270577  \n",
       "9      31.226248  \n",
       "10     36.381837  \n",
       "11     36.385570  \n",
       "12     37.356110  \n",
       "13     43.671601  \n",
       "14     48.087107  \n",
       "15     65.189733  \n",
       "16     31.920701  \n",
       "17     36.635587  \n",
       "18     36.972080  \n",
       "19     30.919563  \n",
       "20     35.419095  \n",
       "21     30.911364  \n",
       "22     35.234273  \n",
       "23     36.010676  \n",
       "24     27.052423  \n",
       "25     36.411448  \n",
       "26     43.750598  \n",
       "27     48.326811  \n",
       "28     31.175731  \n",
       "29     31.271491  \n",
       "30     32.389317  \n",
       "31     43.558671  \n",
       "32     45.780270  \n",
       "33     65.185188  \n",
       "34     32.615462  \n",
       "35     31.356169  \n",
       "36     43.421907  \n",
       "37     30.977271  \n",
       "38     47.821857  \n",
       "39     30.809591  \n",
       "40     43.220914  \n",
       "41     49.027743  \n",
       "42     31.548206  \n",
       "43     30.838214  \n",
       "44     43.417075  \n",
       "45     49.384834  \n",
       "46     31.191717  \n",
       "47     42.971298  \n",
       "48     42.458241  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_sdss_1.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Accuracy\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Accuracy\", \"Elapsed time\"], ascending=[0,1], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 31.156 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
