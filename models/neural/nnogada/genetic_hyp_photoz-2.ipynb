{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 19:33:16.037019: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-09 19:33:16.122434: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-09 19:33:16.122450: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-09 19:33:16.718203: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-09 19:33:16.718268: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-09 19:33:16.718275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import astropy\n",
    "from astropy.io import fits\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open('https://github.com/igomezv/cosmo_tools/blob/main/COB_22/Viviana_Acquaviva/DEEP2_uniq_Terapix_Subaru_v1.fits?raw=true') as data:\n",
    "    df = pd.DataFrame(np.array(data[1].data).byteswap().newbyteorder()) #see https://numpy.org/devdocs/user/basics.byteswapping.html#changing-byte-ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objno_deep2</th>\n",
       "      <th>ra_deep2</th>\n",
       "      <th>dec_deep2</th>\n",
       "      <th>magb</th>\n",
       "      <th>magr</th>\n",
       "      <th>magi</th>\n",
       "      <th>pgal</th>\n",
       "      <th>sfd_ebv</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "      <th>...</th>\n",
       "      <th>ra_subaru</th>\n",
       "      <th>dec_subaru</th>\n",
       "      <th>y</th>\n",
       "      <th>yerr</th>\n",
       "      <th>y_apercor</th>\n",
       "      <th>yerr_aper</th>\n",
       "      <th>yerr_apercor</th>\n",
       "      <th>y(sexflag)</th>\n",
       "      <th>y_radius_arcsec</th>\n",
       "      <th>subaru_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11001673</td>\n",
       "      <td>213.868704</td>\n",
       "      <td>51.956445</td>\n",
       "      <td>23.487745</td>\n",
       "      <td>23.143082</td>\n",
       "      <td>22.582092</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010943</td>\n",
       "      <td>b''</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>213.868626</td>\n",
       "      <td>51.956443</td>\n",
       "      <td>21.869627</td>\n",
       "      <td>0.060918</td>\n",
       "      <td>21.926356</td>\n",
       "      <td>0.041955</td>\n",
       "      <td>0.141778</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.656514</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11001699</td>\n",
       "      <td>213.810471</td>\n",
       "      <td>51.942316</td>\n",
       "      <td>22.067692</td>\n",
       "      <td>20.034674</td>\n",
       "      <td>19.545080</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.011014</td>\n",
       "      <td>b'GALAXY'</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>213.810455</td>\n",
       "      <td>51.942321</td>\n",
       "      <td>18.757229</td>\n",
       "      <td>0.005813</td>\n",
       "      <td>18.811085</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.050987</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.744269</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11001770</td>\n",
       "      <td>213.848431</td>\n",
       "      <td>51.948876</td>\n",
       "      <td>24.144438</td>\n",
       "      <td>24.103180</td>\n",
       "      <td>24.020006</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>b'GALAXY'</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11001800</td>\n",
       "      <td>213.831758</td>\n",
       "      <td>51.952548</td>\n",
       "      <td>25.336836</td>\n",
       "      <td>23.508480</td>\n",
       "      <td>23.081087</td>\n",
       "      <td>0.509809</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>b'GALAXY'</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>213.831766</td>\n",
       "      <td>51.952544</td>\n",
       "      <td>22.404269</td>\n",
       "      <td>0.088970</td>\n",
       "      <td>22.535600</td>\n",
       "      <td>0.053497</td>\n",
       "      <td>0.094733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.455820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11001860</td>\n",
       "      <td>213.832550</td>\n",
       "      <td>51.954174</td>\n",
       "      <td>24.382738</td>\n",
       "      <td>23.401484</td>\n",
       "      <td>22.572845</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>b'GALAXY'</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>213.832574</td>\n",
       "      <td>51.954175</td>\n",
       "      <td>22.242717</td>\n",
       "      <td>0.070760</td>\n",
       "      <td>22.100980</td>\n",
       "      <td>0.033256</td>\n",
       "      <td>0.073067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.442022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   objno_deep2    ra_deep2  dec_deep2       magb       magr       magi  \\\n",
       "0     11001673  213.868704  51.956445  23.487745  23.143082  22.582092   \n",
       "1     11001699  213.810471  51.942316  22.067692  20.034674  19.545080   \n",
       "2     11001770  213.848431  51.948876  24.144438  24.103180  24.020006   \n",
       "3     11001800  213.831758  51.952548  25.336836  23.508480  23.081087   \n",
       "4     11001860  213.832550  51.954174  24.382738  23.401484  22.572845   \n",
       "\n",
       "       pgal   sfd_ebv      class subclass  ...   ra_subaru dec_subaru  \\\n",
       "0  1.000000  0.010943        b''      b''  ...  213.868626  51.956443   \n",
       "1  3.000000  0.011014  b'GALAXY'      b''  ...  213.810455  51.942321   \n",
       "2  3.000000  0.010856  b'GALAXY'      b''  ...  -99.000000 -99.000000   \n",
       "3  0.509809  0.010823  b'GALAXY'      b''  ...  213.831766  51.952544   \n",
       "4  3.000000  0.010827  b'GALAXY'      b''  ...  213.832574  51.954175   \n",
       "\n",
       "           y       yerr  y_apercor  yerr_aper  yerr_apercor  y(sexflag)  \\\n",
       "0  21.869627   0.060918  21.926356   0.041955      0.141778         3.0   \n",
       "1  18.757229   0.005813  18.811085   0.004386      0.050987         3.0   \n",
       "2 -99.000000 -99.000000 -99.000000 -99.000000    -99.000000       -99.0   \n",
       "3  22.404269   0.088970  22.535600   0.053497      0.094733         0.0   \n",
       "4  22.242717   0.070760  22.100980   0.033256      0.073067         0.0   \n",
       "\n",
       "   y_radius_arcsec  subaru_source  \n",
       "0         0.656514              1  \n",
       "1         0.744269              1  \n",
       "2       -99.000000            -99  \n",
       "3         0.455820              1  \n",
       "4         0.442022              1  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_apercor</th>\n",
       "      <th>g_apercor</th>\n",
       "      <th>r_apercor</th>\n",
       "      <th>i_apercor</th>\n",
       "      <th>z_apercor</th>\n",
       "      <th>y_apercor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.543491</td>\n",
       "      <td>23.430495</td>\n",
       "      <td>23.100311</td>\n",
       "      <td>22.768970</td>\n",
       "      <td>22.223810</td>\n",
       "      <td>21.926356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.848978</td>\n",
       "      <td>28.989668</td>\n",
       "      <td>19.027422</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>18.811085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.324670</td>\n",
       "      <td>24.273606</td>\n",
       "      <td>24.150319</td>\n",
       "      <td>23.446252</td>\n",
       "      <td>23.574236</td>\n",
       "      <td>-99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>24.804309</td>\n",
       "      <td>23.636544</td>\n",
       "      <td>23.009222</td>\n",
       "      <td>22.689591</td>\n",
       "      <td>22.535600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.362068</td>\n",
       "      <td>24.136913</td>\n",
       "      <td>23.490342</td>\n",
       "      <td>22.777181</td>\n",
       "      <td>22.319676</td>\n",
       "      <td>22.100980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   u_apercor  g_apercor  r_apercor  i_apercor  z_apercor  y_apercor\n",
       "0  23.543491  23.430495  23.100311  22.768970  22.223810  21.926356\n",
       "1  30.848978  28.989668  19.027422  99.000000  99.000000  18.811085\n",
       "2  24.324670  24.273606  24.150319  23.446252  23.574236 -99.000000\n",
       "3  99.000000  24.804309  23.636544  23.009222  22.689591  22.535600\n",
       "4  24.362068  24.136913  23.490342  22.777181  22.319676  22.100980"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df[['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor']]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.115261\n",
       "1    0.290608\n",
       "2    0.605744\n",
       "3    1.306796\n",
       "4    0.957669\n",
       "Name: zhelio, dtype: float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df['zhelio']\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = df[['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor','zquality','cfhtls_source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16857, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mags = mags[mags['zquality'] >= 3]\n",
    "\n",
    "mags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = mags[mags > -10].dropna()\n",
    "mags = mags[mags < 90].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = mags[mags['cfhtls_source'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor']\n",
    "features_ext = mags.copy()\n",
    "for i, name1 in enumerate(params):\n",
    "    for j, name2 in enumerate(params):\n",
    "        if i >=j: continue #build only one pair, avoid zero colors\n",
    "        features_ext[name1 + '-' + name2] = features[name1] - features[name2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_apercor</th>\n",
       "      <th>g_apercor</th>\n",
       "      <th>r_apercor</th>\n",
       "      <th>i_apercor</th>\n",
       "      <th>z_apercor</th>\n",
       "      <th>y_apercor</th>\n",
       "      <th>zquality</th>\n",
       "      <th>cfhtls_source</th>\n",
       "      <th>u_apercor-g_apercor</th>\n",
       "      <th>u_apercor-r_apercor</th>\n",
       "      <th>...</th>\n",
       "      <th>g_apercor-r_apercor</th>\n",
       "      <th>g_apercor-i_apercor</th>\n",
       "      <th>g_apercor-z_apercor</th>\n",
       "      <th>g_apercor-y_apercor</th>\n",
       "      <th>r_apercor-i_apercor</th>\n",
       "      <th>r_apercor-z_apercor</th>\n",
       "      <th>r_apercor-y_apercor</th>\n",
       "      <th>i_apercor-z_apercor</th>\n",
       "      <th>i_apercor-y_apercor</th>\n",
       "      <th>z_apercor-y_apercor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>24.244393</td>\n",
       "      <td>23.979583</td>\n",
       "      <td>23.522136</td>\n",
       "      <td>22.911041</td>\n",
       "      <td>22.525773</td>\n",
       "      <td>22.329098</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264811</td>\n",
       "      <td>0.722258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457447</td>\n",
       "      <td>1.068542</td>\n",
       "      <td>1.453810</td>\n",
       "      <td>1.650485</td>\n",
       "      <td>0.611094</td>\n",
       "      <td>0.996362</td>\n",
       "      <td>1.193037</td>\n",
       "      <td>0.385268</td>\n",
       "      <td>0.581943</td>\n",
       "      <td>0.196675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>24.489104</td>\n",
       "      <td>23.916151</td>\n",
       "      <td>22.923651</td>\n",
       "      <td>21.873752</td>\n",
       "      <td>21.306495</td>\n",
       "      <td>21.251440</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.572953</td>\n",
       "      <td>1.565453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>2.042399</td>\n",
       "      <td>2.609656</td>\n",
       "      <td>2.664711</td>\n",
       "      <td>1.049899</td>\n",
       "      <td>1.617157</td>\n",
       "      <td>1.672212</td>\n",
       "      <td>0.567258</td>\n",
       "      <td>0.622312</td>\n",
       "      <td>0.055055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>24.873959</td>\n",
       "      <td>22.973893</td>\n",
       "      <td>21.465850</td>\n",
       "      <td>20.788420</td>\n",
       "      <td>20.462283</td>\n",
       "      <td>20.413696</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.900066</td>\n",
       "      <td>3.408108</td>\n",
       "      <td>...</td>\n",
       "      <td>1.508042</td>\n",
       "      <td>2.185473</td>\n",
       "      <td>2.511610</td>\n",
       "      <td>2.560196</td>\n",
       "      <td>0.677430</td>\n",
       "      <td>1.003568</td>\n",
       "      <td>1.052154</td>\n",
       "      <td>0.326138</td>\n",
       "      <td>0.374724</td>\n",
       "      <td>0.048586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>24.529042</td>\n",
       "      <td>24.338631</td>\n",
       "      <td>23.891189</td>\n",
       "      <td>23.206102</td>\n",
       "      <td>22.989344</td>\n",
       "      <td>23.112382</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190411</td>\n",
       "      <td>0.637853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447442</td>\n",
       "      <td>1.132529</td>\n",
       "      <td>1.349287</td>\n",
       "      <td>1.226250</td>\n",
       "      <td>0.685087</td>\n",
       "      <td>0.901845</td>\n",
       "      <td>0.778808</td>\n",
       "      <td>0.216758</td>\n",
       "      <td>0.093721</td>\n",
       "      <td>-0.123037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>23.641180</td>\n",
       "      <td>23.387447</td>\n",
       "      <td>22.975301</td>\n",
       "      <td>22.235199</td>\n",
       "      <td>21.809658</td>\n",
       "      <td>21.559483</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253733</td>\n",
       "      <td>0.665879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412146</td>\n",
       "      <td>1.152248</td>\n",
       "      <td>1.577789</td>\n",
       "      <td>1.827964</td>\n",
       "      <td>0.740102</td>\n",
       "      <td>1.165643</td>\n",
       "      <td>1.415818</td>\n",
       "      <td>0.425541</td>\n",
       "      <td>0.675717</td>\n",
       "      <td>0.250175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      u_apercor  g_apercor  r_apercor  i_apercor  z_apercor  y_apercor  \\\n",
       "1251  24.244393  23.979583  23.522136  22.911041  22.525773  22.329098   \n",
       "1253  24.489104  23.916151  22.923651  21.873752  21.306495  21.251440   \n",
       "1261  24.873959  22.973893  21.465850  20.788420  20.462283  20.413696   \n",
       "1271  24.529042  24.338631  23.891189  23.206102  22.989344  23.112382   \n",
       "1272  23.641180  23.387447  22.975301  22.235199  21.809658  21.559483   \n",
       "\n",
       "      zquality  cfhtls_source  u_apercor-g_apercor  u_apercor-r_apercor  ...  \\\n",
       "1251         4            0.0             0.264811             0.722258  ...   \n",
       "1253         4            0.0             0.572953             1.565453  ...   \n",
       "1261         3            0.0             1.900066             3.408108  ...   \n",
       "1271         4            0.0             0.190411             0.637853  ...   \n",
       "1272         4            0.0             0.253733             0.665879  ...   \n",
       "\n",
       "      g_apercor-r_apercor  g_apercor-i_apercor  g_apercor-z_apercor  \\\n",
       "1251             0.457447             1.068542             1.453810   \n",
       "1253             0.992500             2.042399             2.609656   \n",
       "1261             1.508042             2.185473             2.511610   \n",
       "1271             0.447442             1.132529             1.349287   \n",
       "1272             0.412146             1.152248             1.577789   \n",
       "\n",
       "      g_apercor-y_apercor  r_apercor-i_apercor  r_apercor-z_apercor  \\\n",
       "1251             1.650485             0.611094             0.996362   \n",
       "1253             2.664711             1.049899             1.617157   \n",
       "1261             2.560196             0.677430             1.003568   \n",
       "1271             1.226250             0.685087             0.901845   \n",
       "1272             1.827964             0.740102             1.165643   \n",
       "\n",
       "      r_apercor-y_apercor  i_apercor-z_apercor  i_apercor-y_apercor  \\\n",
       "1251             1.193037             0.385268             0.581943   \n",
       "1253             1.672212             0.567258             0.622312   \n",
       "1261             1.052154             0.326138             0.374724   \n",
       "1271             0.778808             0.216758             0.093721   \n",
       "1272             1.415818             0.425541             0.675717   \n",
       "\n",
       "      z_apercor-y_apercor  \n",
       "1251             0.196675  \n",
       "1253             0.055055  \n",
       "1261             0.048586  \n",
       "1271            -0.123037  \n",
       "1272             0.250175  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target[features_ext.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_paper = features_ext[['u_apercor-g_apercor','g_apercor-r_apercor', \\\n",
    "            'r_apercor-i_apercor','i_apercor-z_apercor','z_apercor-y_apercor','i_apercor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(features_paper, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([100,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,1e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([8, 16, 32 ,64])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:1])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[1:2])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[3:4])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[4:6])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.4         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 19:33:53.420071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-09 19:33:53.420263: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-09 19:33:53.420317: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-09 19:33:53.420362: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-09 19:33:53.420407: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-09 19:33:53.420451: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-09 19:33:53.420495: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-09 19:33:53.420539: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-09 19:33:53.420584: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-09 19:33:53.420591: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-09 19:33:53.420858: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 653us/step - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Loss: 0.036365754902362823 , Elapsed time: 63.24552130699158\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Loss: 0.035126883536577225 , Elapsed time: 31.39256978034973\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 682us/step - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Loss: 0.03743220493197441 , Elapsed time: 67.65725088119507\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 957us/step - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Loss: 0.03470734506845474 , Elapsed time: 104.59544515609741\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 823us/step - loss: 0.0439 - mean_squared_error: 0.0439\n",
      "Loss: 0.04386500269174576 , Elapsed time: 52.992491245269775\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Loss: 0.029621794819831848 , Elapsed time: 123.30752778053284\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 658us/step - loss: 0.0376 - mean_squared_error: 0.0376\n",
      "Loss: 0.037555478513240814 , Elapsed time: 26.22867774963379\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 933us/step - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Loss: 0.017488323152065277 , Elapsed time: 98.5668592453003\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax     \n",
      "0  \t8     \t0.0174883\t0.0340203\t0.043865\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 711us/step - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Loss: 0.02655155397951603 , Elapsed time: 88.48322319984436\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 681us/step - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Loss: 0.022755373269319534 , Elapsed time: 63.83691954612732\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t2     \t0.0174883\t0.0310719\t0.0375555\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 717us/step - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Loss: 0.030389804393053055 , Elapsed time: 21.410725831985474\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.0530 - mean_squared_error: 0.0530\n",
      "Loss: 0.05298037827014923 , Elapsed time: 70.53940200805664\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 737us/step - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Loss: 0.019023697823286057 , Elapsed time: 70.83457803726196\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 684us/step - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Loss: 0.019356844946742058 , Elapsed time: 66.58594059944153\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t4     \t0.0174883\t0.0298153\t0.0529804\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 713us/step - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Loss: 0.0247858464717865 , Elapsed time: 78.80444359779358\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 688us/step - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Loss: 0.025756599381566048 , Elapsed time: 64.66580390930176\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 620us/step - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Loss: 0.03784257546067238 , Elapsed time: 64.52086281776428\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Loss: 0.022464144974946976 , Elapsed time: 77.57017779350281\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Loss: 0.03003435954451561 , Elapsed time: 78.12419509887695\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t5     \t0.0174883\t0.0254934\t0.0378426\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Loss: 0.025048648938536644 , Elapsed time: 77.5834002494812\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 697us/step - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Loss: 0.019229436293244362 , Elapsed time: 121.17271256446838\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t2     \t0.0174883\t0.021191 \t0.0250486\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 683us/step - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Loss: 0.016719035804271698 , Elapsed time: 70.32404255867004\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 704us/step - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Loss: 0.0247796643525362 , Elapsed time: 71.6182234287262\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 706us/step - loss: 0.0125 - mean_squared_error: 0.0125\n",
      "Loss: 0.01249003503471613 , Elapsed time: 67.08178448677063\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 705us/step - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Loss: 0.020285481587052345 , Elapsed time: 71.4871301651001\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.01249  \t0.0190857\t0.0247797\n",
      "-- Best Individual =  [1, 0, 1, 1, 0, 1]\n",
      "-- Best Fitness =  0.01249003503471613\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 29.879424544175468 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 8   # max of individuals per generation\n",
    "max_generations = 5  # number of generations\n",
    "gene_length = 6      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 4 , Number of neurons: 100\n",
      "Batch size 16 , Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.012490</td>\n",
       "      <td>0.012490</td>\n",
       "      <td>67.081784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.016719</td>\n",
       "      <td>0.016719</td>\n",
       "      <td>70.324043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.017488</td>\n",
       "      <td>0.017488</td>\n",
       "      <td>98.566859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.019024</td>\n",
       "      <td>0.019024</td>\n",
       "      <td>70.834578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.019229</td>\n",
       "      <td>0.019229</td>\n",
       "      <td>121.172713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>66.585941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.020285</td>\n",
       "      <td>0.020285</td>\n",
       "      <td>71.487130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.022464</td>\n",
       "      <td>0.022464</td>\n",
       "      <td>77.570178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.022755</td>\n",
       "      <td>0.022755</td>\n",
       "      <td>63.836920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.024780</td>\n",
       "      <td>0.024780</td>\n",
       "      <td>71.618223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.024786</td>\n",
       "      <td>0.024786</td>\n",
       "      <td>78.804444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>77.583400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.025757</td>\n",
       "      <td>0.025757</td>\n",
       "      <td>64.665804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.026552</td>\n",
       "      <td>0.026552</td>\n",
       "      <td>88.483223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.029622</td>\n",
       "      <td>0.029622</td>\n",
       "      <td>123.307528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.030034</td>\n",
       "      <td>0.030034</td>\n",
       "      <td>78.124195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>0.030390</td>\n",
       "      <td>0.030390</td>\n",
       "      <td>21.410726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.034707</td>\n",
       "      <td>0.034707</td>\n",
       "      <td>104.595445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.035127</td>\n",
       "      <td>0.035127</td>\n",
       "      <td>31.392570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.036366</td>\n",
       "      <td>0.036366</td>\n",
       "      <td>63.245521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.037432</td>\n",
       "      <td>0.037432</td>\n",
       "      <td>67.657251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>0.037555</td>\n",
       "      <td>0.037555</td>\n",
       "      <td>26.228678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.037843</td>\n",
       "      <td>0.037843</td>\n",
       "      <td>64.520863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>0.043865</td>\n",
       "      <td>0.043865</td>\n",
       "      <td>52.992491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.052980</td>\n",
       "      <td>0.052980</td>\n",
       "      <td>70.539402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             4        100         0.0010          16  0.012490  0.012490   \n",
       "1             3        200         0.0010          16  0.016719  0.016719   \n",
       "2             4        200         0.0010          16  0.017488  0.017488   \n",
       "3             3        200         0.0010          16  0.019024  0.019024   \n",
       "4             3        100         0.0010           8  0.019229  0.019229   \n",
       "5             4        100         0.0010          16  0.019357  0.019357   \n",
       "6             3        200         0.0010          16  0.020285  0.020285   \n",
       "7             4        200         0.0010          16  0.022464  0.022464   \n",
       "8             3        100         0.0010          16  0.022755  0.022755   \n",
       "9             3        200         0.0010          16  0.024780  0.024780   \n",
       "10            4        200         0.0010          16  0.024786  0.024786   \n",
       "11            4        200         0.0010          16  0.025049  0.025049   \n",
       "12            3        100         0.0010          16  0.025757  0.025757   \n",
       "13            4        200         0.0001          16  0.026552  0.026552   \n",
       "14            3        100         0.0010           8  0.029622  0.029622   \n",
       "15            4        200         0.0010          16  0.030034  0.030034   \n",
       "16            4        100         0.0010          64  0.030390  0.030390   \n",
       "17            4        200         0.0010          16  0.034707  0.034707   \n",
       "18            4        200         0.0001          64  0.035127  0.035127   \n",
       "19            3        100         0.0001          16  0.036366  0.036366   \n",
       "20            4        100         0.0001          16  0.037432  0.037432   \n",
       "21            3        100         0.0010          64  0.037555  0.037555   \n",
       "22            3        100         0.0001          16  0.037843  0.037843   \n",
       "23            4        200         0.0001          32  0.043865  0.043865   \n",
       "24            3        200         0.0001          16  0.052980  0.052980   \n",
       "\n",
       "    Elapsed time  \n",
       "0      67.081784  \n",
       "1      70.324043  \n",
       "2      98.566859  \n",
       "3      70.834578  \n",
       "4     121.172713  \n",
       "5      66.585941  \n",
       "6      71.487130  \n",
       "7      77.570178  \n",
       "8      63.836920  \n",
       "9      71.618223  \n",
       "10     78.804444  \n",
       "11     77.583400  \n",
       "12     64.665804  \n",
       "13     88.483223  \n",
       "14    123.307528  \n",
       "15     78.124195  \n",
       "16     21.410726  \n",
       "17    104.595445  \n",
       "18     31.392570  \n",
       "19     63.245521  \n",
       "20     67.657251  \n",
       "21     26.228678  \n",
       "22     64.520863  \n",
       "23     52.992491  \n",
       "24     70.539402  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_photoz2.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 29.877 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
