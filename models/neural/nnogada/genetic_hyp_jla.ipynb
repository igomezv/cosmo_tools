{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,5e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.5         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isidro/.local/lib/python3.8/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/isidro/.local/lib/python3.8/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1281 - mean_squared_error: 0.1281\n",
      "Loss: 0.1280781477689743 , Elapsed time: 76.85638570785522\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0511 - mean_squared_error: 0.0511\n",
      "Loss: 0.05109407752752304 , Elapsed time: 15.270228385925293\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0630 - mean_squared_error: 0.0630\n",
      "Loss: 0.06300394237041473 , Elapsed time: 24.09848976135254\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0698 - mean_squared_error: 0.0698\n",
      "Loss: 0.06978755444288254 , Elapsed time: 44.644742012023926\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Loss: 0.03585032373666763 , Elapsed time: 55.285146713256836\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax     \n",
      "0  \t5     \t0.0358503\t0.0695628\t0.128078\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "Loss: 0.0426950566470623 , Elapsed time: 67.02639555931091\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0435 - mean_squared_error: 0.0435\n",
      "Loss: 0.04353853315114975 , Elapsed time: 78.45247650146484\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Loss: 0.03670220822095871 , Elapsed time: 69.47925686836243\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Loss: 0.03680792823433876 , Elapsed time: 66.80800151824951\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.0358503\t0.0391188\t0.0435385\n",
      "2  \t0     \t0.0358503\t0.0363826\t0.0368079\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1435 - mean_squared_error: 0.1435\n",
      "Loss: 0.14351172745227814 , Elapsed time: 64.02502703666687\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t1     \t0.0358503\t0.057553 \t0.143512 \n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Loss: 0.03505794331431389 , Elapsed time: 75.74163913726807\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Loss: 0.03513137251138687 , Elapsed time: 73.38606309890747\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Loss: 0.03476425260305405 , Elapsed time: 77.88230180740356\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t3     \t0.0347643\t0.0353308\t0.0358503\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Loss: 0.03640782833099365 , Elapsed time: 71.32139587402344\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Loss: 0.03769592568278313 , Elapsed time: 75.79213261604309\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Loss: 0.03688792139291763 , Elapsed time: 70.2564446926117\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t3     \t0.0358503\t0.0365385\t0.0376959\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Loss: 0.037432488054037094 , Elapsed time: 79.15916538238525\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Loss: 0.035341814160346985 , Elapsed time: 80.31239748001099\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Loss: 0.03569547086954117 , Elapsed time: 85.92447066307068\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t3     \t0.0353418\t0.0360341\t0.0374325\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Loss: 0.03692357614636421 , Elapsed time: 92.10738277435303\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 1s 4ms/step - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Loss: 0.038559041917324066 , Elapsed time: 79.21642994880676\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0352 - mean_squared_error: 0.0352\n",
      "Loss: 0.035180091857910156 , Elapsed time: 94.88081765174866\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0603 - mean_squared_error: 0.0603\n",
      "Loss: 0.060298480093479156 , Elapsed time: 27.565565824508667\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.0351801\t0.0412606\t0.0602985\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0384 - mean_squared_error: 0.0384\n",
      "Loss: 0.03843194991350174 , Elapsed time: 78.7186348438263\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Loss: 0.036276109516620636 , Elapsed time: 24.747862339019775\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5898 - mean_squared_error: 0.5898\n",
      "Loss: 0.5898245573043823 , Elapsed time: 20.07557225227356\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t3     \t0.0351801\t0.147327 \t0.589825 \n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Loss: 0.035697925835847855 , Elapsed time: 143.081964969635\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Loss: 0.032443348318338394 , Elapsed time: 180.74810814857483\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t2     \t0.0324433\t0.0357354\t0.0384319\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0461 - mean_squared_error: 0.0461\n",
      "Loss: 0.04613999277353287 , Elapsed time: 130.38778138160706\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Loss: 0.030406981706619263 , Elapsed time: 98.37341403961182\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Loss: 0.03352130576968193 , Elapsed time: 218.03841423988342\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Loss: 0.034278418868780136 , Elapsed time: 107.1257586479187\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t4     \t0.030407 \t0.035358 \t0.04614  \n",
      "-- Best Individual =  [1, 0, 0, 1, 0, 0, 1]\n",
      "-- Best Fitness =  0.030406981706619263\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 42.45649555126826 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 2 , Number of neurons: 50\n",
      "Batch size 4 , Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030407</td>\n",
       "      <td>0.030407</td>\n",
       "      <td>98.373414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032443</td>\n",
       "      <td>0.032443</td>\n",
       "      <td>180.748108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033521</td>\n",
       "      <td>0.033521</td>\n",
       "      <td>218.038414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034278</td>\n",
       "      <td>0.034278</td>\n",
       "      <td>107.125759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034764</td>\n",
       "      <td>0.034764</td>\n",
       "      <td>77.882302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035058</td>\n",
       "      <td>0.035058</td>\n",
       "      <td>75.741639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035131</td>\n",
       "      <td>0.035131</td>\n",
       "      <td>73.386063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035180</td>\n",
       "      <td>0.035180</td>\n",
       "      <td>94.880818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035342</td>\n",
       "      <td>0.035342</td>\n",
       "      <td>80.312397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035695</td>\n",
       "      <td>0.035695</td>\n",
       "      <td>85.924471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>143.081965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035850</td>\n",
       "      <td>0.035850</td>\n",
       "      <td>55.285147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.036276</td>\n",
       "      <td>0.036276</td>\n",
       "      <td>24.747862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036408</td>\n",
       "      <td>0.036408</td>\n",
       "      <td>71.321396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036702</td>\n",
       "      <td>0.036702</td>\n",
       "      <td>69.479257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036808</td>\n",
       "      <td>0.036808</td>\n",
       "      <td>66.808002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036888</td>\n",
       "      <td>0.036888</td>\n",
       "      <td>70.256445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036924</td>\n",
       "      <td>0.036924</td>\n",
       "      <td>92.107383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037432</td>\n",
       "      <td>0.037432</td>\n",
       "      <td>79.159165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037696</td>\n",
       "      <td>0.037696</td>\n",
       "      <td>75.792133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038432</td>\n",
       "      <td>0.038432</td>\n",
       "      <td>78.718635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038559</td>\n",
       "      <td>0.038559</td>\n",
       "      <td>79.216430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.042695</td>\n",
       "      <td>0.042695</td>\n",
       "      <td>67.026396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.043539</td>\n",
       "      <td>0.043539</td>\n",
       "      <td>78.452477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.046140</td>\n",
       "      <td>0.046140</td>\n",
       "      <td>130.387781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.051094</td>\n",
       "      <td>0.051094</td>\n",
       "      <td>15.270228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.060298</td>\n",
       "      <td>0.060298</td>\n",
       "      <td>27.565566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.063004</td>\n",
       "      <td>0.063004</td>\n",
       "      <td>24.098490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.069788</td>\n",
       "      <td>0.069788</td>\n",
       "      <td>44.644742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.128078</td>\n",
       "      <td>0.128078</td>\n",
       "      <td>76.856386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.143512</td>\n",
       "      <td>0.143512</td>\n",
       "      <td>64.025027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.589825</td>\n",
       "      <td>0.589825</td>\n",
       "      <td>20.075572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             3        100         0.0001           4  0.030407  0.030407   \n",
       "1             3        100         0.0001           2  0.032443  0.032443   \n",
       "2             4        100         0.0001           2  0.033521  0.033521   \n",
       "3             3        100         0.0001           4  0.034278  0.034278   \n",
       "4             2        100         0.0001           4  0.034764  0.034764   \n",
       "5             2        100         0.0001           4  0.035058  0.035058   \n",
       "6             2        100         0.0001           4  0.035131  0.035131   \n",
       "7             4        100         0.0001           4  0.035180  0.035180   \n",
       "8             2        200         0.0001           4  0.035342  0.035342   \n",
       "9             2        100         0.0001           4  0.035695  0.035695   \n",
       "10            2        200         0.0001           4  0.035698  0.035698   \n",
       "11            2        100         0.0001           4  0.035850  0.035850   \n",
       "12            4        100         0.0001          16  0.036276  0.036276   \n",
       "13            2        100         0.0001           4  0.036408  0.036408   \n",
       "14            2        200         0.0001           4  0.036702  0.036702   \n",
       "15            2        100         0.0001           4  0.036808  0.036808   \n",
       "16            2        200         0.0001           4  0.036888  0.036888   \n",
       "17            2        200         0.0001           4  0.036924  0.036924   \n",
       "18            2        100         0.0001           4  0.037432  0.037432   \n",
       "19            2        100         0.0001           4  0.037696  0.037696   \n",
       "20            2        200         0.0001           4  0.038432  0.038432   \n",
       "21            2        100         0.0001           4  0.038559  0.038559   \n",
       "22            2        100         0.0050           4  0.042695  0.042695   \n",
       "23            4        100         0.0050           4  0.043539  0.043539   \n",
       "24            1        200         0.0001           2  0.046140  0.046140   \n",
       "25            2        200         0.0001          16  0.051094  0.051094   \n",
       "26            2        100         0.0001          16  0.060298  0.060298   \n",
       "27            4        200         0.0050          16  0.063004  0.063004   \n",
       "28            1        100         0.0001           4  0.069788  0.069788   \n",
       "29            3        100         0.0050           2  0.128078  0.128078   \n",
       "30            1        100         0.0001           4  0.143512  0.143512   \n",
       "31            1        200         0.0001          16  0.589825  0.589825   \n",
       "\n",
       "    Elapsed time  \n",
       "0      98.373414  \n",
       "1     180.748108  \n",
       "2     218.038414  \n",
       "3     107.125759  \n",
       "4      77.882302  \n",
       "5      75.741639  \n",
       "6      73.386063  \n",
       "7      94.880818  \n",
       "8      80.312397  \n",
       "9      85.924471  \n",
       "10    143.081965  \n",
       "11     55.285147  \n",
       "12     24.747862  \n",
       "13     71.321396  \n",
       "14     69.479257  \n",
       "15     66.808002  \n",
       "16     70.256445  \n",
       "17     92.107383  \n",
       "18     79.159165  \n",
       "19     75.792133  \n",
       "20     78.718635  \n",
       "21     79.216430  \n",
       "22     67.026396  \n",
       "23     78.452477  \n",
       "24    130.387781  \n",
       "25     15.270228  \n",
       "26     27.565566  \n",
       "27     24.098490  \n",
       "28     44.644742  \n",
       "29     76.856386  \n",
       "30     64.025027  \n",
       "31     20.075572  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla1.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 42.446 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
