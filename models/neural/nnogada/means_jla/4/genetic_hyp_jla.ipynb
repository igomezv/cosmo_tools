{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 17:43:42.713788: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-18 17:43:42.837964: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:43:42.838023: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-18 17:43:43.733152: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:43:43.733225: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:43:43.733232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,1e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.5         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 17:43:44.744004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 17:43:44.744221: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:43:44.744305: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:43:44.744362: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:43:44.744415: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:43:44.744467: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:43:44.744519: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:43:44.744571: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:43:44.744623: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:43:44.744633: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-18 17:43:44.745063: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0742 - mean_squared_error: 0.0742\n",
      "Loss: 0.07424338161945343 , Elapsed time: 143.10853552818298\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0501 - mean_squared_error: 0.0501\n",
      "Loss: 0.05007270351052284 , Elapsed time: 26.584853410720825\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0760 - mean_squared_error: 0.0760\n",
      "Loss: 0.0760226622223854 , Elapsed time: 39.60939049720764\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0998 - mean_squared_error: 0.0998\n",
      "Loss: 0.09980136156082153 , Elapsed time: 57.985594511032104\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Loss: 0.03550804778933525 , Elapsed time: 57.46943545341492\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin     \tavg      \tmax      \n",
      "0  \t5     \t0.035508\t0.0671296\t0.0998014\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0626 - mean_squared_error: 0.0626\n",
      "Loss: 0.06260192394256592 , Elapsed time: 21.253113269805908\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Loss: 0.03485966846346855 , Elapsed time: 83.16298341751099\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1021 - mean_squared_error: 0.1021\n",
      "Loss: 0.10205744206905365 , Elapsed time: 143.34201765060425\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0501 - mean_squared_error: 0.0501\n",
      "Loss: 0.05013416334986687 , Elapsed time: 83.33401346206665\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.0348597\t0.0570322\t0.102057 \n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Loss: 0.0355040617287159 , Elapsed time: 68.56294274330139\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t1     \t0.0348597\t0.0674161\t0.102057 \n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Loss: 0.03952507674694061 , Elapsed time: 79.10074353218079\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Loss: 0.03546235337853432 , Elapsed time: 83.24517607688904\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t2     \t0.0348597\t0.036171 \t0.0395251\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0531 - mean_squared_error: 0.0531\n",
      "Loss: 0.05312187224626541 , Elapsed time: 69.06051278114319\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Loss: 0.035664621740579605 , Elapsed time: 77.79771256446838\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Loss: 0.03395700082182884 , Elapsed time: 83.18549680709839\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "Loss: 0.03714777156710625 , Elapsed time: 74.79953384399414\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t4     \t0.033957 \t0.0389502\t0.0531219\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Loss: 0.032896872609853745 , Elapsed time: 87.89752912521362\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Loss: 0.035326045006513596 , Elapsed time: 73.52814531326294\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Loss: 0.03360923379659653 , Elapsed time: 77.85002517700195\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Loss: 0.034949447959661484 , Elapsed time: 81.02792954444885\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.0328969\t0.0343283\t0.035326 \n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Loss: 0.03765648603439331 , Elapsed time: 83.58637833595276\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Loss: 0.03594803065061569 , Elapsed time: 106.47699999809265\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Loss: 0.034986577928066254 , Elapsed time: 83.21677803993225\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Loss: 0.030488271266222 , Elapsed time: 94.25449585914612\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t4     \t0.0304883\t0.0343952\t0.0376565\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Loss: 0.03332389518618584 , Elapsed time: 91.04442286491394\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Loss: 0.0341610424220562 , Elapsed time: 144.07219171524048\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Loss: 0.03274592384696007 , Elapsed time: 75.08294796943665\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t3     \t0.0327459\t0.0336229\t0.0349866\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0442 - mean_squared_error: 0.0442\n",
      "Loss: 0.04418462514877319 , Elapsed time: 66.49745774269104\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Loss: 0.03315998241305351 , Elapsed time: 71.7825436592102\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Loss: 0.034734662622213364 , Elapsed time: 70.10870099067688\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Loss: 0.03384767100214958 , Elapsed time: 75.04614591598511\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t4     \t0.0328969\t0.0357648\t0.0441846\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Loss: 0.03780480474233627 , Elapsed time: 56.3819534778595\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Loss: 0.033807020634412766 , Elapsed time: 65.56868100166321\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Loss: 0.033545318990945816 , Elapsed time: 83.81614756584167\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Loss: 0.034672558307647705 , Elapsed time: 68.76098656654358\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t4     \t0.0328969\t0.0345453\t0.0378048\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Loss: 0.03237563371658325 , Elapsed time: 60.81695294380188\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Loss: 0.032603178173303604 , Elapsed time: 53.11623406410217\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0650 - mean_squared_error: 0.0650\n",
      "Loss: 0.06504213064908981 , Elapsed time: 74.52515769004822\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t3     \t0.0323756\t0.0391629\t0.0650421\n",
      "-- Best Individual =  [1, 1, 1, 0, 0, 0, 1]\n",
      "-- Best Fitness =  0.032896872609853745\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 48.93863784472148 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 2 , Number of neurons: 100\n",
      "Batch size 2 , Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030488</td>\n",
       "      <td>0.030488</td>\n",
       "      <td>94.254496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032376</td>\n",
       "      <td>0.032376</td>\n",
       "      <td>60.816953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032603</td>\n",
       "      <td>0.032603</td>\n",
       "      <td>53.116234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032746</td>\n",
       "      <td>0.032746</td>\n",
       "      <td>75.082948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032897</td>\n",
       "      <td>0.032897</td>\n",
       "      <td>87.897529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033160</td>\n",
       "      <td>0.033160</td>\n",
       "      <td>71.782544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033324</td>\n",
       "      <td>0.033324</td>\n",
       "      <td>91.044423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033545</td>\n",
       "      <td>0.033545</td>\n",
       "      <td>83.816148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033609</td>\n",
       "      <td>0.033609</td>\n",
       "      <td>77.850025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033807</td>\n",
       "      <td>0.033807</td>\n",
       "      <td>65.568681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033848</td>\n",
       "      <td>0.033848</td>\n",
       "      <td>75.046146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033957</td>\n",
       "      <td>0.033957</td>\n",
       "      <td>83.185497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034161</td>\n",
       "      <td>0.034161</td>\n",
       "      <td>144.072192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034673</td>\n",
       "      <td>0.034673</td>\n",
       "      <td>68.760987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034735</td>\n",
       "      <td>0.034735</td>\n",
       "      <td>70.108701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034860</td>\n",
       "      <td>0.034860</td>\n",
       "      <td>83.162983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034949</td>\n",
       "      <td>0.034949</td>\n",
       "      <td>81.027930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034987</td>\n",
       "      <td>0.034987</td>\n",
       "      <td>83.216778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035326</td>\n",
       "      <td>0.035326</td>\n",
       "      <td>73.528145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035462</td>\n",
       "      <td>0.035462</td>\n",
       "      <td>83.245176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035504</td>\n",
       "      <td>0.035504</td>\n",
       "      <td>68.562943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035508</td>\n",
       "      <td>0.035508</td>\n",
       "      <td>57.469435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035665</td>\n",
       "      <td>0.035665</td>\n",
       "      <td>77.797713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035948</td>\n",
       "      <td>0.035948</td>\n",
       "      <td>106.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037148</td>\n",
       "      <td>0.037148</td>\n",
       "      <td>74.799534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037656</td>\n",
       "      <td>0.037656</td>\n",
       "      <td>83.586378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037805</td>\n",
       "      <td>0.037805</td>\n",
       "      <td>56.381953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.039525</td>\n",
       "      <td>0.039525</td>\n",
       "      <td>79.100744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.044185</td>\n",
       "      <td>0.044185</td>\n",
       "      <td>66.497458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.050073</td>\n",
       "      <td>0.050073</td>\n",
       "      <td>26.584853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.050134</td>\n",
       "      <td>0.050134</td>\n",
       "      <td>83.334013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.053122</td>\n",
       "      <td>0.053122</td>\n",
       "      <td>69.060513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.062602</td>\n",
       "      <td>0.062602</td>\n",
       "      <td>21.253113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.065042</td>\n",
       "      <td>0.065042</td>\n",
       "      <td>74.525158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.074243</td>\n",
       "      <td>0.074243</td>\n",
       "      <td>143.108536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.076023</td>\n",
       "      <td>0.076023</td>\n",
       "      <td>39.609390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.099801</td>\n",
       "      <td>0.099801</td>\n",
       "      <td>57.985595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.102057</td>\n",
       "      <td>0.102057</td>\n",
       "      <td>143.342018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             4        150         0.0001           4  0.030488  0.030488   \n",
       "1             4        150         0.0001           4  0.032376  0.032376   \n",
       "2             4        150         0.0001           4  0.032603  0.032603   \n",
       "3             4        150         0.0001           4  0.032746  0.032746   \n",
       "4             4        150         0.0001           4  0.032897  0.032897   \n",
       "5             4        150         0.0001           4  0.033160  0.033160   \n",
       "6             4        150         0.0001           4  0.033324  0.033324   \n",
       "7             4        150         0.0001           4  0.033545  0.033545   \n",
       "8             2        200         0.0001           4  0.033609  0.033609   \n",
       "9             4        150         0.0001           4  0.033807  0.033807   \n",
       "10            4        150         0.0001           4  0.033848  0.033848   \n",
       "11            2        200         0.0001           4  0.033957  0.033957   \n",
       "12            4        200         0.0001           4  0.034161  0.034161   \n",
       "13            4        150         0.0001           4  0.034673  0.034673   \n",
       "14            4        150         0.0001           4  0.034735  0.034735   \n",
       "15            2        200         0.0001           4  0.034860  0.034860   \n",
       "16            2        200         0.0001           4  0.034949  0.034949   \n",
       "17            2        200         0.0001           4  0.034987  0.034987   \n",
       "18            2        200         0.0001           4  0.035326  0.035326   \n",
       "19            2        200         0.0001           4  0.035462  0.035462   \n",
       "20            2        200         0.0001           4  0.035504  0.035504   \n",
       "21            2        100         0.0001           4  0.035508  0.035508   \n",
       "22            2        200         0.0001           4  0.035665  0.035665   \n",
       "23            4        200         0.0001           4  0.035948  0.035948   \n",
       "24            2        200         0.0001           4  0.037148  0.037148   \n",
       "25            2        200         0.0001           4  0.037656  0.037656   \n",
       "26            2        150         0.0001           4  0.037805  0.037805   \n",
       "27            2        200         0.0001           4  0.039525  0.039525   \n",
       "28            4        150         0.0010           4  0.044185  0.044185   \n",
       "29            2        200         0.0001          16  0.050073  0.050073   \n",
       "30            3        100         0.0010           4  0.050134  0.050134   \n",
       "31            2        200         0.0010           4  0.053122  0.053122   \n",
       "32            2        100         0.0001          16  0.062602  0.062602   \n",
       "33            1        150         0.0001           2  0.065042  0.065042   \n",
       "34            3        100         0.0010           2  0.074243  0.074243   \n",
       "35            4        200         0.0010          16  0.076023  0.076023   \n",
       "36            1        100         0.0001           4  0.099801  0.099801   \n",
       "37            3        100         0.0010           2  0.102057  0.102057   \n",
       "\n",
       "    Elapsed time  \n",
       "0      94.254496  \n",
       "1      60.816953  \n",
       "2      53.116234  \n",
       "3      75.082948  \n",
       "4      87.897529  \n",
       "5      71.782544  \n",
       "6      91.044423  \n",
       "7      83.816148  \n",
       "8      77.850025  \n",
       "9      65.568681  \n",
       "10     75.046146  \n",
       "11     83.185497  \n",
       "12    144.072192  \n",
       "13     68.760987  \n",
       "14     70.108701  \n",
       "15     83.162983  \n",
       "16     81.027930  \n",
       "17     83.216778  \n",
       "18     73.528145  \n",
       "19     83.245176  \n",
       "20     68.562943  \n",
       "21     57.469435  \n",
       "22     77.797713  \n",
       "23    106.477000  \n",
       "24     74.799534  \n",
       "25     83.586378  \n",
       "26     56.381953  \n",
       "27     79.100744  \n",
       "28     66.497458  \n",
       "29     26.584853  \n",
       "30     83.334013  \n",
       "31     69.060513  \n",
       "32     21.253113  \n",
       "33     74.525158  \n",
       "34    143.108536  \n",
       "35     39.609390  \n",
       "36     57.985595  \n",
       "37    143.342018  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla1.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 48.934 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
