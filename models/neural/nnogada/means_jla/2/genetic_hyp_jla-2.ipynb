{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 00:38:45.126690: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-18 00:38:45.312624: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:45.312659: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-18 00:38:46.524243: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:46.524385: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:46.524399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,1e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.2         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 00:38:47.839540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 00:38:47.839875: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:47.839996: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:47.840072: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:47.840146: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:47.840230: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:47.840302: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:47.840373: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:47.840443: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:47.840458: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-18 00:38:47.841889: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0725 - mean_squared_error: 0.0725\n",
      "Loss: 0.07250197976827621 , Elapsed time: 203.66990661621094\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0520 - mean_squared_error: 0.0520\n",
      "Loss: 0.05196787044405937 , Elapsed time: 37.61195611953735\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0701 - mean_squared_error: 0.0701\n",
      "Loss: 0.0700925812125206 , Elapsed time: 58.111992835998535\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0923 - mean_squared_error: 0.0923\n",
      "Loss: 0.09225916117429733 , Elapsed time: 71.2747871875763\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Loss: 0.03618244826793671 , Elapsed time: 143.30927085876465\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax      \n",
      "0  \t5     \t0.0361824\t0.0646008\t0.0922592\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Loss: 0.03613127022981644 , Elapsed time: 98.88534569740295\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0707 - mean_squared_error: 0.0707\n",
      "Loss: 0.07068085670471191 , Elapsed time: 42.59985613822937\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0637 - mean_squared_error: 0.0637\n",
      "Loss: 0.06369341164827347 , Elapsed time: 32.97786569595337\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Loss: 0.03561698645353317 , Elapsed time: 89.53056049346924\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.035617 \t0.048461 \t0.0706809\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Loss: 0.034501783549785614 , Elapsed time: 155.33641147613525\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Loss: 0.035939764231443405 , Elapsed time: 143.49477577209473\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2255 - mean_squared_error: 0.2255\n",
      "Loss: 0.2255421131849289 , Elapsed time: 42.348848819732666\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Loss: 0.0363629013299942 , Elapsed time: 92.6691358089447\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t4     \t0.0345018\t0.0735927\t0.225542 \n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Loss: 0.03545469045639038 , Elapsed time: 143.40385556221008\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Loss: 0.033558715134859085 , Elapsed time: 133.88060641288757\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Loss: 0.03736937418580055 , Elapsed time: 143.71662950515747\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t3     \t0.0335587\t0.0353003\t0.0373694\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0471 - mean_squared_error: 0.0471\n",
      "Loss: 0.047097884118556976 , Elapsed time: 100.33120512962341\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Loss: 0.03292917087674141 , Elapsed time: 144.45478892326355\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Loss: 0.03403971344232559 , Elapsed time: 123.07790923118591\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Loss: 0.039461400359869 , Elapsed time: 203.9752688407898\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t4     \t0.0329292\t0.0374174\t0.0470979\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Loss: 0.035629600286483765 , Elapsed time: 143.44430947303772\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0470 - mean_squared_error: 0.0470\n",
      "Loss: 0.04704540967941284 , Elapsed time: 160.60350489616394\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t2     \t0.0335587\t0.0390313\t0.0470454\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Loss: 0.034466471523046494 , Elapsed time: 144.16199564933777\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0411 - mean_squared_error: 0.0411\n",
      "Loss: 0.04106856510043144 , Elapsed time: 205.39950513839722\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Loss: 0.03350651636719704 , Elapsed time: 264.7869634628296\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t3     \t0.0335065\t0.035646 \t0.0410686\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Loss: 0.035635482519865036 , Elapsed time: 84.56554532051086\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Loss: 0.03261104226112366 , Elapsed time: 167.09904956817627\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Loss: 0.03514480963349342 , Elapsed time: 112.71631217002869\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t3     \t0.032611 \t0.0340913\t0.0356355\n",
      "8  \t0     \t0.032611 \t0.0336551\t0.0351448\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Loss: 0.03370577096939087 , Elapsed time: 161.94287776947021\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Loss: 0.03608423471450806 , Elapsed time: 157.24988532066345\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Loss: 0.03497336059808731 , Elapsed time: 141.86711025238037\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t3     \t0.032611 \t0.0341762\t0.0360842\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Loss: 0.03335512802004814 , Elapsed time: 138.0138750076294\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Loss: 0.0350324921309948 , Elapsed time: 116.1058759689331\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0352 - mean_squared_error: 0.0352\n",
      "Loss: 0.03515760228037834 , Elapsed time: 143.875807762146\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Loss: 0.033747173845767975 , Elapsed time: 122.30443096160889\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t4     \t0.0333551\t0.0341598\t0.0351576\n",
      "-- Best Individual =  [1, 1, 1, 0, 0, 0, 0]\n",
      "-- Best Fitness =  0.03350651636719704\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 74.4841334581375 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 2 , Number of neurons: 100\n",
      "Batch size 2 , Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032611</td>\n",
       "      <td>0.032611</td>\n",
       "      <td>167.099050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032929</td>\n",
       "      <td>0.032929</td>\n",
       "      <td>144.454789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033355</td>\n",
       "      <td>0.033355</td>\n",
       "      <td>138.013875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033507</td>\n",
       "      <td>0.033507</td>\n",
       "      <td>264.786963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033559</td>\n",
       "      <td>0.033559</td>\n",
       "      <td>133.880606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033706</td>\n",
       "      <td>0.033706</td>\n",
       "      <td>161.942878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033747</td>\n",
       "      <td>0.033747</td>\n",
       "      <td>122.304431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034040</td>\n",
       "      <td>0.034040</td>\n",
       "      <td>123.077909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034466</td>\n",
       "      <td>0.034466</td>\n",
       "      <td>144.161996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034502</td>\n",
       "      <td>0.034502</td>\n",
       "      <td>155.336411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034973</td>\n",
       "      <td>0.034973</td>\n",
       "      <td>141.867110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035032</td>\n",
       "      <td>0.035032</td>\n",
       "      <td>116.105876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035145</td>\n",
       "      <td>0.035145</td>\n",
       "      <td>112.716312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>143.875808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035455</td>\n",
       "      <td>0.035455</td>\n",
       "      <td>143.403856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035617</td>\n",
       "      <td>0.035617</td>\n",
       "      <td>89.530560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035630</td>\n",
       "      <td>0.035630</td>\n",
       "      <td>143.444309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035635</td>\n",
       "      <td>0.035635</td>\n",
       "      <td>84.565545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035940</td>\n",
       "      <td>0.035940</td>\n",
       "      <td>143.494776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.036084</td>\n",
       "      <td>0.036084</td>\n",
       "      <td>157.249885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036131</td>\n",
       "      <td>0.036131</td>\n",
       "      <td>98.885346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036182</td>\n",
       "      <td>0.036182</td>\n",
       "      <td>143.309271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036363</td>\n",
       "      <td>0.036363</td>\n",
       "      <td>92.669136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037369</td>\n",
       "      <td>0.037369</td>\n",
       "      <td>143.716630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.039461</td>\n",
       "      <td>0.039461</td>\n",
       "      <td>203.975269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.041069</td>\n",
       "      <td>0.041069</td>\n",
       "      <td>205.399505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.047045</td>\n",
       "      <td>0.047045</td>\n",
       "      <td>160.603505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.047098</td>\n",
       "      <td>0.047098</td>\n",
       "      <td>100.331205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.051968</td>\n",
       "      <td>0.051968</td>\n",
       "      <td>37.611956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.063693</td>\n",
       "      <td>0.063693</td>\n",
       "      <td>32.977866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.070093</td>\n",
       "      <td>0.070093</td>\n",
       "      <td>58.111993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.070681</td>\n",
       "      <td>0.070681</td>\n",
       "      <td>42.599856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.072502</td>\n",
       "      <td>0.072502</td>\n",
       "      <td>203.669907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.092259</td>\n",
       "      <td>0.092259</td>\n",
       "      <td>71.274787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.225542</td>\n",
       "      <td>0.225542</td>\n",
       "      <td>42.348849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             4        150         0.0001           2  0.032611  0.032611   \n",
       "1             4        200         0.0001           4  0.032929  0.032929   \n",
       "2             4        150         0.0001           2  0.033355  0.033355   \n",
       "3             4        150         0.0001           2  0.033507  0.033507   \n",
       "4             4        200         0.0001           4  0.033559  0.033559   \n",
       "5             4        150         0.0001           2  0.033706  0.033706   \n",
       "6             4        150         0.0001           2  0.033747  0.033747   \n",
       "7             4        200         0.0001           4  0.034040  0.034040   \n",
       "8             4        200         0.0001           4  0.034466  0.034466   \n",
       "9             4        200         0.0010           4  0.034502  0.034502   \n",
       "10            4        150         0.0001           2  0.034973  0.034973   \n",
       "11            4        150         0.0001           2  0.035032  0.035032   \n",
       "12            4        200         0.0010           4  0.035145  0.035145   \n",
       "13            4        150         0.0001           2  0.035158  0.035158   \n",
       "14            2        200         0.0001           4  0.035455  0.035455   \n",
       "15            2        200         0.0001           4  0.035617  0.035617   \n",
       "16            2        200         0.0001           4  0.035630  0.035630   \n",
       "17            3         50         0.0001           4  0.035635  0.035635   \n",
       "18            2        100         0.0001           4  0.035940  0.035940   \n",
       "19            4        150         0.0001           2  0.036084  0.036084   \n",
       "20            4        100         0.0010           4  0.036131  0.036131   \n",
       "21            2        100         0.0001           4  0.036182  0.036182   \n",
       "22            2        100         0.0001           4  0.036363  0.036363   \n",
       "23            4        200         0.0010           4  0.037369  0.037369   \n",
       "24            4        200         0.0010           4  0.039461  0.039461   \n",
       "25            4        200         0.0010           4  0.041069  0.041069   \n",
       "26            4        200         0.0010           4  0.047045  0.047045   \n",
       "27            2        200         0.0010           4  0.047098  0.047098   \n",
       "28            2        200         0.0001          16  0.051968  0.051968   \n",
       "29            2        100         0.0001          16  0.063693  0.063693   \n",
       "30            4        200         0.0010          16  0.070093  0.070093   \n",
       "31            4        200         0.0010          16  0.070681  0.070681   \n",
       "32            3        100         0.0010           2  0.072502  0.072502   \n",
       "33            1        100         0.0001           4  0.092259  0.092259   \n",
       "34            2        100         0.0001          16  0.225542  0.225542   \n",
       "\n",
       "    Elapsed time  \n",
       "0     167.099050  \n",
       "1     144.454789  \n",
       "2     138.013875  \n",
       "3     264.786963  \n",
       "4     133.880606  \n",
       "5     161.942878  \n",
       "6     122.304431  \n",
       "7     123.077909  \n",
       "8     144.161996  \n",
       "9     155.336411  \n",
       "10    141.867110  \n",
       "11    116.105876  \n",
       "12    112.716312  \n",
       "13    143.875808  \n",
       "14    143.403856  \n",
       "15     89.530560  \n",
       "16    143.444309  \n",
       "17     84.565545  \n",
       "18    143.494776  \n",
       "19    157.249885  \n",
       "20     98.885346  \n",
       "21    143.309271  \n",
       "22     92.669136  \n",
       "23    143.716630  \n",
       "24    203.975269  \n",
       "25    205.399505  \n",
       "26    160.603505  \n",
       "27    100.331205  \n",
       "28     37.611956  \n",
       "29     32.977866  \n",
       "30     58.111993  \n",
       "31     42.599856  \n",
       "32    203.669907  \n",
       "33     71.274787  \n",
       "34     42.348849  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla2.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 74.480 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
