{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 20:40:30.348669: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-16 20:40:30.436670: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:40:30.436687: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-16 20:40:31.051577: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:40:31.051640: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:40:31.051647: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,5e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.8         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 20:40:32.071935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-16 20:40:32.072120: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:40:32.072175: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:40:32.072222: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:40:32.072268: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:40:32.072313: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:40:32.072359: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:40:32.072404: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:40:32.072449: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:40:32.072455: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-16 20:40:32.072699: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1066 - mean_squared_error: 0.1066\n",
      "Loss: 0.10661908984184265 , Elapsed time: 135.26982045173645\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0523 - mean_squared_error: 0.0523\n",
      "Loss: 0.0523441806435585 , Elapsed time: 30.475261211395264\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0582 - mean_squared_error: 0.0582\n",
      "Loss: 0.05817752331495285 , Elapsed time: 42.48490595817566\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0780 - mean_squared_error: 0.0780\n",
      "Loss: 0.07801317423582077 , Elapsed time: 83.03803324699402\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Loss: 0.03591706231236458 , Elapsed time: 73.18982815742493\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax     \n",
      "0  \t5     \t0.0359171\t0.0662142\t0.106619\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.2181 - mean_squared_error: 2.2181\n",
      "Loss: 2.2180871963500977 , Elapsed time: 24.89381742477417\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 1 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0982 - mean_squared_error: 0.0982\n",
      "Loss: 0.09817862510681152 , Elapsed time: 74.19393539428711\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Loss: 0.03977292776107788 , Elapsed time: 143.403178691864\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Loss: 0.031103840097784996 , Elapsed time: 203.2203631401062\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.0311038\t0.484612 \t2.21809 \n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Loss: 0.031130023300647736 , Elapsed time: 203.2706379890442\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0873 - mean_squared_error: 0.0873\n",
      "Loss: 0.08733964711427689 , Elapsed time: 73.88216185569763\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0605 - mean_squared_error: 0.0605\n",
      "Loss: 0.06045744940638542 , Elapsed time: 140.99322056770325\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t3     \t0.0311038\t0.0491896\t0.0873396\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Loss: 0.03219844028353691 , Elapsed time: 137.70289659500122\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0611 - mean_squared_error: 0.0611\n",
      "Loss: 0.0611499585211277 , Elapsed time: 136.90720748901367\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Loss: 0.03206653147935867 , Elapsed time: 141.85797810554504\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.2863 - mean_squared_error: 1.2863\n",
      "Loss: 1.2862653732299805 , Elapsed time: 42.0225625038147\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t4     \t0.0311038\t0.288557 \t1.28627  \n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0754 - mean_squared_error: 0.0754\n",
      "Loss: 0.07543320953845978 , Elapsed time: 203.81817889213562\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Loss: 0.032098155468702316 , Elapsed time: 203.43689846992493\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0403 - mean_squared_error: 0.0403\n",
      "Loss: 0.04027803614735603 , Elapsed time: 43.203288078308105\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Loss: 0.031316209584474564 , Elapsed time: 161.87655639648438\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t4     \t0.0311038\t0.0420459\t0.0754332\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Loss: 0.030907604843378067 , Elapsed time: 203.68458223342896\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Loss: 0.03106035478413105 , Elapsed time: 158.74502849578857\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Loss: 0.034901585429906845 , Elapsed time: 203.7105312347412\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Loss: 0.03289905935525894 , Elapsed time: 150.7766809463501\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.0309076\t0.0321745\t0.0349016\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Loss: 0.03148604556918144 , Elapsed time: 143.53579902648926\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Loss: 0.031197531148791313 , Elapsed time: 203.21327829360962\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Loss: 0.03569530323147774 , Elapsed time: 70.09504199028015\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Loss: 0.035897016525268555 , Elapsed time: 143.5843222141266\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t4     \t0.0309076\t0.0330367\t0.035897 \n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "Loss: 0.03710316866636276 , Elapsed time: 65.34783124923706\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Loss: 0.031129412353038788 , Elapsed time: 106.79453873634338\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Loss: 0.03638654202222824 , Elapsed time: 56.19607591629028\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Loss: 0.031444232910871506 , Elapsed time: 148.04670810699463\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.0309076\t0.0333942\t0.0371032\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Loss: 0.03603259101510048 , Elapsed time: 80.10252642631531\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 1s 2ms/step - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Loss: 0.030337007716298103 , Elapsed time: 145.27002358436584\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0455 - mean_squared_error: 0.0455\n",
      "Loss: 0.04549738019704819 , Elapsed time: 63.721784830093384\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Loss: 0.03545187786221504 , Elapsed time: 180.06966638565063\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t4     \t0.030337 \t0.0356453\t0.0454974\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1243 - mean_squared_error: 0.1243\n",
      "Loss: 0.12425774335861206 , Elapsed time: 120.44880795478821\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0915 - mean_squared_error: 0.0915\n",
      "Loss: 0.0915336087346077 , Elapsed time: 104.03026580810547\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Loss: 0.030649643391370773 , Elapsed time: 103.35592222213745\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Loss: 0.0376989021897316 , Elapsed time: 67.04635000228882\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t4     \t0.0306496\t0.0630095\t0.124258 \n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1065 - mean_squared_error: 0.1065\n",
      "Loss: 0.10653684288263321 , Elapsed time: 95.48811602592468\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 42 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0526 - mean_squared_error: 0.0526\n",
      "Loss: 0.05260074511170387 , Elapsed time: 52.29423785209656\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 43 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1371 - mean_squared_error: 0.1371\n",
      "Loss: 0.13708467781543732 , Elapsed time: 89.99273157119751\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 44 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0949 - mean_squared_error: 0.0949\n",
      "Loss: 0.09491761028766632 , Elapsed time: 72.33954954147339\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t4     \t0.0309076\t0.0844095\t0.137085 \n",
      "-- Best Individual =  [0, 1, 1, 1, 0, 0, 0]\n",
      "-- Best Fitness =  0.030907604843378067\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 85.45385546286902 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 1 , Number of neurons: 100\n",
      "Batch size 4 , Learning rate: 0.005\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030337</td>\n",
       "      <td>0.030337</td>\n",
       "      <td>145.270024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030650</td>\n",
       "      <td>0.030650</td>\n",
       "      <td>103.355922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030908</td>\n",
       "      <td>0.030908</td>\n",
       "      <td>203.684582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031060</td>\n",
       "      <td>0.031060</td>\n",
       "      <td>158.745028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031104</td>\n",
       "      <td>0.031104</td>\n",
       "      <td>203.220363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031129</td>\n",
       "      <td>0.031129</td>\n",
       "      <td>106.794539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031130</td>\n",
       "      <td>0.031130</td>\n",
       "      <td>203.270638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031198</td>\n",
       "      <td>0.031198</td>\n",
       "      <td>203.213278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031316</td>\n",
       "      <td>0.031316</td>\n",
       "      <td>161.876556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031444</td>\n",
       "      <td>0.031444</td>\n",
       "      <td>148.046708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031486</td>\n",
       "      <td>0.031486</td>\n",
       "      <td>143.535799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032067</td>\n",
       "      <td>0.032067</td>\n",
       "      <td>141.857978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032098</td>\n",
       "      <td>0.032098</td>\n",
       "      <td>203.436898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032198</td>\n",
       "      <td>0.032198</td>\n",
       "      <td>137.702897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032899</td>\n",
       "      <td>0.032899</td>\n",
       "      <td>150.776681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034902</td>\n",
       "      <td>0.034902</td>\n",
       "      <td>203.710531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035452</td>\n",
       "      <td>0.035452</td>\n",
       "      <td>180.069666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035695</td>\n",
       "      <td>0.035695</td>\n",
       "      <td>70.095042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035897</td>\n",
       "      <td>0.035897</td>\n",
       "      <td>143.584322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035917</td>\n",
       "      <td>0.035917</td>\n",
       "      <td>73.189828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036033</td>\n",
       "      <td>0.036033</td>\n",
       "      <td>80.102526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036387</td>\n",
       "      <td>0.036387</td>\n",
       "      <td>56.196076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037103</td>\n",
       "      <td>0.037103</td>\n",
       "      <td>65.347831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037699</td>\n",
       "      <td>0.037699</td>\n",
       "      <td>67.046350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>143.403179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.040278</td>\n",
       "      <td>0.040278</td>\n",
       "      <td>43.203288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.045497</td>\n",
       "      <td>0.045497</td>\n",
       "      <td>63.721785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.052344</td>\n",
       "      <td>0.052344</td>\n",
       "      <td>30.475261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.052601</td>\n",
       "      <td>0.052601</td>\n",
       "      <td>52.294238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.058178</td>\n",
       "      <td>0.058178</td>\n",
       "      <td>42.484906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.060457</td>\n",
       "      <td>0.060457</td>\n",
       "      <td>140.993221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.061150</td>\n",
       "      <td>0.061150</td>\n",
       "      <td>136.907207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.075433</td>\n",
       "      <td>0.075433</td>\n",
       "      <td>203.818179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.078013</td>\n",
       "      <td>0.078013</td>\n",
       "      <td>83.038033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.087340</td>\n",
       "      <td>0.087340</td>\n",
       "      <td>73.882162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.091534</td>\n",
       "      <td>0.091534</td>\n",
       "      <td>104.030266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.094918</td>\n",
       "      <td>0.094918</td>\n",
       "      <td>72.339550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.098179</td>\n",
       "      <td>0.098179</td>\n",
       "      <td>74.193935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.106537</td>\n",
       "      <td>0.106537</td>\n",
       "      <td>95.488116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.106619</td>\n",
       "      <td>0.106619</td>\n",
       "      <td>135.269820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.124258</td>\n",
       "      <td>0.124258</td>\n",
       "      <td>120.448808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.137085</td>\n",
       "      <td>0.137085</td>\n",
       "      <td>89.992732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>1.286265</td>\n",
       "      <td>1.286265</td>\n",
       "      <td>42.022563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>2.218087</td>\n",
       "      <td>2.218087</td>\n",
       "      <td>24.893817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             2        200         0.0001           2  0.030337  0.030337   \n",
       "1             2        200         0.0001           2  0.030650  0.030650   \n",
       "2             2        200         0.0001           2  0.030908  0.030908   \n",
       "3             2        100         0.0001           2  0.031060  0.031060   \n",
       "4             2        100         0.0001           2  0.031104  0.031104   \n",
       "5             2        100         0.0001           2  0.031129  0.031129   \n",
       "6             2        100         0.0001           2  0.031130  0.031130   \n",
       "7             2        200         0.0001           2  0.031198  0.031198   \n",
       "8             2        100         0.0001           2  0.031316  0.031316   \n",
       "9             2        200         0.0001           2  0.031444  0.031444   \n",
       "10            2        100         0.0001           2  0.031486  0.031486   \n",
       "11            2        100         0.0001           2  0.032067  0.032067   \n",
       "12            2        100         0.0001           2  0.032098  0.032098   \n",
       "13            2        100         0.0001           2  0.032198  0.032198   \n",
       "14            2        100         0.0001           2  0.032899  0.032899   \n",
       "15            2        100         0.0001           2  0.034902  0.034902   \n",
       "16            4        200         0.0001           2  0.035452  0.035452   \n",
       "17            2        100         0.0001           4  0.035695  0.035695   \n",
       "18            2        100         0.0001           2  0.035897  0.035897   \n",
       "19            2        100         0.0001           4  0.035917  0.035917   \n",
       "20            2        200         0.0001           4  0.036033  0.036033   \n",
       "21            2        100         0.0001           4  0.036387  0.036387   \n",
       "22            2        100         0.0001           4  0.037103  0.037103   \n",
       "23            4        200         0.0050           4  0.037699  0.037699   \n",
       "24            3        200         0.0050           4  0.039773  0.039773   \n",
       "25            2        100         0.0001           8  0.040278  0.040278   \n",
       "26            2        100         0.0050           4  0.045497  0.045497   \n",
       "27            2        200         0.0001          16  0.052344  0.052344   \n",
       "28            2        200         0.0050           4  0.052601  0.052601   \n",
       "29            4        200         0.0050          16  0.058178  0.058178   \n",
       "30            1        100         0.0001           2  0.060457  0.060457   \n",
       "31            1        100         0.0050           2  0.061150  0.061150   \n",
       "32            4        100         0.0050           2  0.075433  0.075433   \n",
       "33            1        100         0.0001           4  0.078013  0.078013   \n",
       "34            2        100         0.0050           4  0.087340  0.087340   \n",
       "35            2        100         0.0050           2  0.091534  0.091534   \n",
       "36            1        150         0.0050           2  0.094918  0.094918   \n",
       "37            1         50         0.0001           4  0.098179  0.098179   \n",
       "38            4        100         0.0050           2  0.106537  0.106537   \n",
       "39            3        100         0.0050           2  0.106619  0.106619   \n",
       "40            2        150         0.0050           2  0.124258  0.124258   \n",
       "41            2        150         0.0050           2  0.137085  0.137085   \n",
       "42            1        100         0.0001          16  1.286265  1.286265   \n",
       "43            1        100         0.0001          16  2.218087  2.218087   \n",
       "\n",
       "    Elapsed time  \n",
       "0     145.270024  \n",
       "1     103.355922  \n",
       "2     203.684582  \n",
       "3     158.745028  \n",
       "4     203.220363  \n",
       "5     106.794539  \n",
       "6     203.270638  \n",
       "7     203.213278  \n",
       "8     161.876556  \n",
       "9     148.046708  \n",
       "10    143.535799  \n",
       "11    141.857978  \n",
       "12    203.436898  \n",
       "13    137.702897  \n",
       "14    150.776681  \n",
       "15    203.710531  \n",
       "16    180.069666  \n",
       "17     70.095042  \n",
       "18    143.584322  \n",
       "19     73.189828  \n",
       "20     80.102526  \n",
       "21     56.196076  \n",
       "22     65.347831  \n",
       "23     67.046350  \n",
       "24    143.403179  \n",
       "25     43.203288  \n",
       "26     63.721785  \n",
       "27     30.475261  \n",
       "28     52.294238  \n",
       "29     42.484906  \n",
       "30    140.993221  \n",
       "31    136.907207  \n",
       "32    203.818179  \n",
       "33     83.038033  \n",
       "34     73.882162  \n",
       "35    104.030266  \n",
       "36     72.339550  \n",
       "37     74.193935  \n",
       "38     95.488116  \n",
       "39    135.269820  \n",
       "40    120.448808  \n",
       "41     89.992732  \n",
       "42     42.022563  \n",
       "43     24.893817  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla2.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 85.451 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
