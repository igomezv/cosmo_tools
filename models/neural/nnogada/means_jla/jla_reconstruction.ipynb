{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 00:15:46.631454: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-20 00:15:46.721235: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-20 00:15:46.721253: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-20 00:15:47.283673: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-20 00:15:47.283733: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-20 00:15:47.283740: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from sklearn.metrics import r2_score, explained_variance_score, mean_absolute_error\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zcmb</th>\n",
       "      <th>mb</th>\n",
       "      <th>dmb</th>\n",
       "      <th>errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.503084</td>\n",
       "      <td>23.001698</td>\n",
       "      <td>0.088031</td>\n",
       "      <td>0.120219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.580724</td>\n",
       "      <td>23.573937</td>\n",
       "      <td>0.090132</td>\n",
       "      <td>0.121435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.494795</td>\n",
       "      <td>22.960139</td>\n",
       "      <td>0.088110</td>\n",
       "      <td>0.089552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.345928</td>\n",
       "      <td>22.398137</td>\n",
       "      <td>0.087263</td>\n",
       "      <td>0.119729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.677662</td>\n",
       "      <td>24.078115</td>\n",
       "      <td>0.098356</td>\n",
       "      <td>0.100088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>0.027064</td>\n",
       "      <td>16.504006</td>\n",
       "      <td>0.141685</td>\n",
       "      <td>0.185894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>0.025468</td>\n",
       "      <td>15.797848</td>\n",
       "      <td>0.143429</td>\n",
       "      <td>0.193666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>0.023810</td>\n",
       "      <td>15.895501</td>\n",
       "      <td>0.144315</td>\n",
       "      <td>0.184957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>0.023867</td>\n",
       "      <td>16.068268</td>\n",
       "      <td>0.144350</td>\n",
       "      <td>0.175960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>0.022068</td>\n",
       "      <td>15.718540</td>\n",
       "      <td>0.144685</td>\n",
       "      <td>0.160803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>740 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         zcmb         mb       dmb    errors\n",
       "0    0.503084  23.001698  0.088031  0.120219\n",
       "1    0.580724  23.573937  0.090132  0.121435\n",
       "2    0.494795  22.960139  0.088110  0.089552\n",
       "3    0.345928  22.398137  0.087263  0.119729\n",
       "4    0.677662  24.078115  0.098356  0.100088\n",
       "..        ...        ...       ...       ...\n",
       "735  0.027064  16.504006  0.141685  0.185894\n",
       "736  0.025468  15.797848  0.143429  0.193666\n",
       "737  0.023810  15.895501  0.144315  0.184957\n",
       "738  0.023867  16.068268  0.144350  0.175960\n",
       "739  0.022068  15.718540  0.144685  0.160803\n",
       "\n",
       "[740 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)\n",
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]\n",
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmag =df[\"dmb\"]\n",
    "\n",
    "df2=df['errors']+df['dmb']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((555, 1), (185, 1), (555, 2), (185, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.75\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_test = np.split(z, indx)\n",
    "Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_test), Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss -> val_loss\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n",
    "                                   min_delta=0.0,\n",
    "                                   patience=200,\n",
    "                                   restore_best_weights=True, verbose=False)\n",
    "                                   ]\n",
    "\n",
    "n_cols = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):    \n",
    "    \n",
    "    # Train LSTM model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(hparams['HP_LAYERS']):        \n",
    "        model.add(Dense(hparams['HP_NUM_UNITS'], activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "     \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hparams['HP_LEARNING'], beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse', \n",
    "            metrics=['mean_squared_error'])\n",
    "    \n",
    "    # Run with 1 epoch to speed things up for demo purposes\n",
    "\n",
    "    train = model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_test, Y_test),\n",
    "              callbacks=callbacks, batch_size=hparams['HP_BATCHSIZE'], shuffle=False, verbose=False)\n",
    "\n",
    "    _, loss = model.evaluate(X_test, Y_test)\n",
    "    \n",
    "    return model, loss, train.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2\t50\t0.00100\t16\n",
    "hparams1 = {'HP_LAYERS': 2, 'HP_NUM_UNITS': 50, 'HP_BATCHSIZE': 16, 'HP_LEARNING':0.001}\n",
    "# 4\t150\t0.00010\t16\n",
    "hparams2 = {'HP_LAYERS': 4, 'HP_NUM_UNITS': 150, 'HP_BATCHSIZE': 16, 'HP_LEARNING':0.0001}\n",
    "# 3\t200\t0.00100\t16\n",
    "hparams3 = {'HP_LAYERS': 3, 'HP_NUM_UNITS': 200, 'HP_BATCHSIZE': 16, 'HP_LEARNING':0.001}\n",
    "# 4\t100\t0.00100\t16\n",
    "hparams4 = {'HP_LAYERS': 4, 'HP_NUM_UNITS': 100, 'HP_BATCHSIZE': 16, 'HP_LEARNING':0.001}\n",
    "# 3\t200\t0.00100\t16\n",
    "hparams5 = {'HP_LAYERS': 3, 'HP_NUM_UNITS': 200, 'HP_BATCHSIZE': 16, 'HP_LEARNING':0.001}\n",
    "# 4\t150\t0.00010\t16\n",
    "hparams6 = {'HP_LAYERS': 4, 'HP_NUM_UNITS': 150, 'HP_BATCHSIZE': 16, 'HP_LEARNING':0.0001}\n",
    "# 3\t200\t0.00100\t16\n",
    "hparams7 = {'HP_LAYERS': 3, 'HP_NUM_UNITS': 200, 'HP_BATCHSIZE': 16, 'HP_LEARNING':0.001}\n",
    "# 3\t200\t0.00100\t16\n",
    "hparams8 = {'HP_LAYERS': 3, 'HP_NUM_UNITS': 200, 'HP_BATCHSIZE': 16, 'HP_LEARNING':0.001}\n",
    "# 4\t100\t0.00100\t16\n",
    "hparams9 = {'HP_LAYERS': 4, 'HP_NUM_UNITS': 100, 'HP_BATCHSIZE': 16, 'HP_LEARNING':0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0448 - mean_squared_error: 0.0448\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0807 - mean_squared_error: 0.0807\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0409 - mean_squared_error: 0.0409\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0678 - mean_squared_error: 0.0678\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "6/6 [==============================] - 0s 5ms/step\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "6/6 [==============================] - 0s 5ms/step\n",
      "Run 0: 0.0370\t0.1141\t0.6914\n",
      "Run 1: 0.0361\t0.1133\t0.5394\n",
      "Run 2: 0.0350\t0.1093\t0.4285\n",
      "Run 3: 0.0448\t0.1222\t0.4994\n",
      "Run 4: 0.0807\t0.1714\t0.4531\n",
      "Run 5: 0.0409\t0.1189\t0.6572\n",
      "Run 6: 0.0678\t0.1478\t0.6657\n",
      "Run 7: 0.0369\t0.1101\t0.4174\n",
      "Run 8: 0.0340\t0.1035\t0.6679\n"
     ]
    }
   ],
   "source": [
    "models_grid = []\n",
    "mse_grid = []\n",
    "mae_grid = []\n",
    "r2_grid = []\n",
    "histories_grid = []\n",
    "hparams_grid = [hparams1, hparams2, hparams3,hparams4,hparams5,hparams6,hparams7,\n",
    "                hparams8,hparams9]\n",
    "\n",
    "for i in range(9):\n",
    "    model, loss_test, history = train_test_model(hparams_grid[i])\n",
    "    models_grid.append(model)\n",
    "    mse_grid.append(loss_test)\n",
    "    histories_grid.append(history)\n",
    "    y = model.predict(X_test)\n",
    "    r2_grid.append(r2_score(y, Y_test))\n",
    "    mae_grid.append(mean_absolute_error(y, Y_test))\n",
    "\n",
    "\n",
    "for i in range(9):\n",
    "    print(\"Run {}: {:.4f}\\t{:.4f}\\t{:.4f}\".format(i, mse_grid[i], mae_grid[i], r2_grid[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic results\n",
    "\n",
    "## A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\t50\t0.00010\t2\n",
    "hparams_gena1 = {'HP_LAYERS': 4, 'HP_NUM_UNITS': 50, 'HP_BATCHSIZE': 2, 'HP_LEARNING':0.0001}\n",
    "# 2\t200\t0.00010\t2\n",
    "hparams_gena2 = {'HP_LAYERS': 2, 'HP_NUM_UNITS': 200, 'HP_BATCHSIZE': 2, 'HP_LEARNING':0.0001}\n",
    "# 2\t100\t0.00010\t2\n",
    "hparams_gena3 = {'HP_LAYERS': 2, 'HP_NUM_UNITS': 100, 'HP_BATCHSIZE': 2, 'HP_LEARNING':0.0001}\n",
    "# 4\t150\t0.00010\t4\n",
    "hparams_gena4 = {'HP_LAYERS': 4, 'HP_NUM_UNITS': 150, 'HP_BATCHSIZE': 4, 'HP_LEARNING':0.0001}\n",
    "# 3\t100\t0.00010\t2\n",
    "hparams_gena5 = {'HP_LAYERS': 3, 'HP_NUM_UNITS': 100, 'HP_BATCHSIZE': 2, 'HP_LEARNING':0.0001}\n",
    "# 2\t100\t0.00010\t2\n",
    "hparams_gena6 = {'HP_LAYERS': 2, 'HP_NUM_UNITS': 100, 'HP_BATCHSIZE': 2, 'HP_LEARNING':0.0001}\n",
    "# 1\t200\t0.00100\t4\n",
    "# 1 \t50 \t0.0010 \t2\n",
    "hparams_gena7 = {'HP_LAYERS': 1, 'HP_NUM_UNITS': 50, 'HP_BATCHSIZE': 2, 'HP_LEARNING':0.001}\n",
    "# 2\t200\t0.00010\t2\n",
    "hparams_gena8 = {'HP_LAYERS': 2, 'HP_NUM_UNITS': 200, 'HP_BATCHSIZE': 2, 'HP_LEARNING':0.0001}\n",
    "# 2\t200\t0.00100\t8\n",
    "hparams_gena9 = {'HP_LAYERS': 2, 'HP_NUM_UNITS': 200, 'HP_BATCHSIZE': 8, 'HP_LEARNING':0.0001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 00:16:37.861435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-20 00:16:37.861697: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-20 00:16:37.861783: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-20 00:16:37.861855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-20 00:16:37.861926: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-20 00:16:37.861996: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-20 00:16:37.862065: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-20 00:16:37.862134: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-20 00:16:37.862204: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-20 00:16:37.862215: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-20 00:16:37.862460: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "6/6 [==============================] - 0s 737us/step\n",
      "6/6 [==============================] - 0s 902us/step - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "6/6 [==============================] - 0s 710us/step\n",
      "6/6 [==============================] - 0s 859us/step - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "6/6 [==============================] - 0s 796us/step\n",
      "6/6 [==============================] - 0s 908us/step - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "6/6 [==============================] - 0s 778us/step\n",
      "6/6 [==============================] - 0s 908us/step - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "6/6 [==============================] - 0s 761us/step\n",
      "6/6 [==============================] - 0s 900us/step - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "6/6 [==============================] - 0s 671us/step\n",
      "6/6 [==============================] - 0s 920us/step - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "6/6 [==============================] - 0s 651us/step\n",
      "6/6 [==============================] - 0s 990us/step - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "6/6 [==============================] - 0s 748us/step\n",
      "6/6 [==============================] - 0s 807us/step - loss: 0.0411 - mean_squared_error: 0.0411\n",
      "6/6 [==============================] - 0s 737us/step\n",
      "Run 0: 0.0363\t0.1069\t0.5894\n",
      "Run 1: 0.0315\t0.0996\t0.6893\n",
      "Run 2: 0.0313\t0.0997\t0.6866\n",
      "Run 3: 0.0340\t0.1049\t0.7091\n",
      "Run 4: 0.0363\t0.1070\t0.6357\n",
      "Run 5: 0.0316\t0.0998\t0.6515\n",
      "Run 6: 0.0323\t0.1023\t0.4249\n",
      "Run 7: 0.0316\t0.0997\t0.6787\n",
      "Run 8: 0.0411\t0.1157\t0.6956\n"
     ]
    }
   ],
   "source": [
    "models_gena = []\n",
    "mse_gena = []\n",
    "mae_gena = []\n",
    "r2_gena = []\n",
    "histories_gena = []\n",
    "hparams_gena = [hparams_gena1, hparams_gena2, hparams_gena3,hparams_gena4,hparams_gena5,hparams_gena6,hparams_gena7,\n",
    "                hparams_gena8,hparams_gena9]\n",
    "\n",
    "for i in range(9):\n",
    "    model, loss_test, history = train_test_model(hparams_gena[i])\n",
    "    models_gena.append(model)\n",
    "    mse_gena.append(loss_test)\n",
    "    histories_gena.append(history)\n",
    "    y = model.predict(X_test)\n",
    "    r2_gena.append(r2_score(y, Y_test))\n",
    "    mae_gena.append(mean_absolute_error(y, Y_test))\n",
    "    \n",
    "for i in range(9):\n",
    "    print(\"Run {}: {:.4f}\\t{:.4f}\\t{:.4f}\".format(i, mse_gena[i], mae_gena[i], r2_gena[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\t200\t0.00010\t2\n",
    "hparams_genb1 = {'HP_LAYERS': 4, 'HP_NUM_UNITS': 200, 'HP_BATCHSIZE': 2, 'HP_LEARNING':0.0001}\n",
    "# 4\t150\t0.00010\t2\n",
    "hparams_genb2 = {'HP_LAYERS': 4, 'HP_NUM_UNITS': 150, 'HP_BATCHSIZE': 2, 'HP_LEARNING':0.0001}\n",
    "# 3\t100\t0.00010\t4\n",
    "hparams_genb3 = {'HP_LAYERS': 3, 'HP_NUM_UNITS': 100, 'HP_BATCHSIZE': 4, 'HP_LEARNING':0.0001}\n",
    "# 4\t100\t0.00100\t4\n",
    "hparams_genb4 = {'HP_LAYERS': 4, 'HP_NUM_UNITS': 100, 'HP_BATCHSIZE': 4, 'HP_LEARNING':0.001}\n",
    "# 4\t100\t0.00010\t4\n",
    "hparams_genb5 = {'HP_LAYERS': 4, 'HP_NUM_UNITS': 100, 'HP_BATCHSIZE': 4, 'HP_LEARNING':0.0001}\n",
    "# 3\t100\t0.00010\t4\n",
    "hparams_genb6 = {'HP_LAYERS': 3, 'HP_NUM_UNITS': 100, 'HP_BATCHSIZE': 4, 'HP_LEARNING':0.0001}\n",
    "# 3\t200\t0.00100\t16\n",
    "hparams_genb7 = {'HP_LAYERS': 3, 'HP_NUM_UNITS': 200, 'HP_BATCHSIZE': 16, 'HP_LEARNING':0.0001}\n",
    "# 4\t50\t0.00010\t4\n",
    "hparams_genb8 = {'HP_LAYERS': 4, 'HP_NUM_UNITS': 50, 'HP_BATCHSIZE': 4, 'HP_LEARNING':0.0001}\n",
    "# 2\t100\t0.00010\t2\n",
    "hparams_genb9 = {'HP_LAYERS': 2, 'HP_NUM_UNITS': 100, 'HP_BATCHSIZE': 2, 'HP_LEARNING':0.0001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0470 - mean_squared_error: 0.0470\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "6/6 [==============================] - 0s 816us/step\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "6/6 [==============================] - 0s 774us/step\n",
      "6/6 [==============================] - 0s 924us/step - loss: 0.0515 - mean_squared_error: 0.0515\n",
      "6/6 [==============================] - 0s 820us/step\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "6/6 [==============================] - 0s 775us/step\n",
      "6/6 [==============================] - 0s 941us/step - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "6/6 [==============================] - 0s 819us/step\n",
      "6/6 [==============================] - 0s 943us/step - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "6/6 [==============================] - 0s 783us/step\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "6/6 [==============================] - 0s 796us/step\n",
      "6/6 [==============================] - 0s 929us/step - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "6/6 [==============================] - 0s 699us/step\n",
      "Run 0: 0.0470\t0.1285\t0.7054\n",
      "Run 1: 0.0407\t0.1177\t0.6659\n",
      "Run 2: 0.0333\t0.1009\t0.6912\n",
      "Run 3: 0.0515\t0.1449\t0.0920\n",
      "Run 4: 0.0342\t0.1075\t0.6933\n",
      "Run 5: 0.0343\t0.1029\t0.6792\n",
      "Run 6: 0.0386\t0.1102\t0.6795\n",
      "Run 7: 0.0333\t0.1021\t0.7204\n",
      "Run 8: 0.0305\t0.0982\t0.6779\n"
     ]
    }
   ],
   "source": [
    "models_genb = []\n",
    "mse_genb = []\n",
    "mae_genb = []\n",
    "r2_genb = []\n",
    "histories_genb = []\n",
    "hparams_genb = [hparams_genb1, hparams_genb2, hparams_genb3,hparams_genb4,hparams_genb5,hparams_genb6,hparams_genb7,\n",
    "                hparams_genb8,hparams_genb9]\n",
    "\n",
    "for i in range(9):\n",
    "    model, loss_test, history = train_test_model(hparams_genb[i])\n",
    "    models_genb.append(model)\n",
    "    mse_genb.append(loss_test)\n",
    "    histories_genb.append(history)\n",
    "    y = model.predict(X_test)\n",
    "    r2_genb.append(r2_score(y, Y_test))\n",
    "    mae_genb.append(mean_absolute_error(y, Y_test))\n",
    "    \n",
    "for i in range(9):\n",
    "    print(\"Run {}: {:.4f}\\t{:.4f}\\t{:.4f}\".format(i, mse_genb[i], mae_genb[i], r2_genb[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\t200\t0.00100\t4\n",
    "hparams_genc1 = {'HP_LAYERS': 1, 'HP_NUM_UNITS': 200, 'HP_BATCHSIZE': 4, 'HP_LEARNING':0.001}\n",
    "# 4\t50\t0.00010\t4\n",
    "hparams_genc2 = {'HP_LAYERS': 4, 'HP_NUM_UNITS': 50, 'HP_BATCHSIZE': 4, 'HP_LEARNING':0.0001}\n",
    "# 2\t100\t0.00010\t2\n",
    "hparams_genc3 = {'HP_LAYERS': 2, 'HP_NUM_UNITS': 100, 'HP_BATCHSIZE': 2, 'HP_LEARNING':0.0001}\n",
    "# 2\t200\t0.00100\t8\n",
    "hparams_genc4 = {'HP_LAYERS': 2, 'HP_NUM_UNITS': 200, 'HP_BATCHSIZE': 8, 'HP_LEARNING':0.001}\n",
    "# 1\t200\t0.00100\t4\n",
    "hparams_genc5 = {'HP_LAYERS': 1, 'HP_NUM_UNITS': 200, 'HP_BATCHSIZE': 4, 'HP_LEARNING':0.001}\n",
    "# 4\t100\t0.00010\t4\n",
    "hparams_genc6 = {'HP_LAYERS': 4, 'HP_NUM_UNITS': 100, 'HP_BATCHSIZE': 4, 'HP_LEARNING':0.0001}\n",
    "# 2\t150\t0.00010\t2\n",
    "hparams_genc7 = {'HP_LAYERS': 2, 'HP_NUM_UNITS': 150, 'HP_BATCHSIZE': 2, 'HP_LEARNING':0.0001}\n",
    "# 3\t100\t0.00010\t4\n",
    "hparams_genc8 = {'HP_LAYERS': 3, 'HP_NUM_UNITS': 100, 'HP_BATCHSIZE': 4, 'HP_LEARNING':0.0001}\n",
    "# 2\t100\t0.00010\t2\n",
    "hparams_genc9 = {'HP_LAYERS': 2, 'HP_NUM_UNITS': 100, 'HP_BATCHSIZE': 2, 'HP_LEARNING':0.0001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 893us/step - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "6/6 [==============================] - 0s 751us/step\n",
      "6/6 [==============================] - 0s 951us/step - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "6/6 [==============================] - 0s 770us/step\n",
      "6/6 [==============================] - 0s 903us/step - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "6/6 [==============================] - 0s 747us/step\n",
      "6/6 [==============================] - 0s 996us/step - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "6/6 [==============================] - 0s 711us/step\n",
      "6/6 [==============================] - 0s 855us/step - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "6/6 [==============================] - 0s 725us/step\n",
      "6/6 [==============================] - 0s 992us/step - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "6/6 [==============================] - 0s 772us/step\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "6/6 [==============================] - 0s 894us/step\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "6/6 [==============================] - 0s 772us/step\n",
      "6/6 [==============================] - 0s 892us/step - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "6/6 [==============================] - 0s 704us/step\n",
      "Run 0: 0.0339\t0.1043\t0.3435\n",
      "Run 1: 0.0330\t0.1021\t0.4566\n",
      "Run 2: 0.0314\t0.0980\t0.6970\n",
      "Run 3: 0.0357\t0.1120\t0.5074\n",
      "Run 4: 0.0319\t0.1015\t0.3628\n",
      "Run 5: 0.0328\t0.1039\t0.6673\n",
      "Run 6: 0.0317\t0.0989\t0.6083\n",
      "Run 7: 0.0329\t0.1022\t0.6479\n",
      "Run 8: 0.0338\t0.1006\t0.6747\n"
     ]
    }
   ],
   "source": [
    "models_genc = []\n",
    "mse_genc = []\n",
    "mae_genc = []\n",
    "r2_genc = []\n",
    "histories_genc = []\n",
    "hparams_genc = [hparams_genc1, hparams_genc2, hparams_genc3,hparams_genc4,hparams_genc5,hparams_genc6,hparams_genc7,\n",
    "                hparams_genc8,hparams_genc9]\n",
    "\n",
    "for i in range(9):\n",
    "    model, loss_test, history = train_test_model(hparams_genc[i])\n",
    "    models_genc.append(model)\n",
    "    mse_genc.append(loss_test)\n",
    "    histories_genc.append(history)\n",
    "    y = model.predict(X_test)\n",
    "    r2_genc.append(r2_score(y, Y_test))\n",
    "    mae_genc.append(mean_absolute_error(y, Y_test))\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Run {}: {:.4f}\\t{:.4f}\\t{:.4f}\".format(i, mse_genc[i], mae_genc[i], r2_genc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
