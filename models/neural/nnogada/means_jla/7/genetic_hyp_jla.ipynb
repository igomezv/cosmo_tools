{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 19:23:15.476726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-18 19:23:15.596802: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-18 19:23:15.596820: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-18 19:23:16.484429: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-18 19:23:16.484516: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-18 19:23:16.484523: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,1e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.5         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 19:23:17.596366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 19:23:17.596630: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-18 19:23:17.596712: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 19:23:17.596786: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 19:23:17.596859: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-18 19:23:17.596931: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-18 19:23:17.597000: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 19:23:17.597072: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 19:23:17.597144: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-18 19:23:17.597156: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-18 19:23:17.597375: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1011 - mean_squared_error: 0.1011\n",
      "Loss: 0.10106991976499557 , Elapsed time: 126.01640152931213\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0500 - mean_squared_error: 0.0500\n",
      "Loss: 0.04995831102132797 , Elapsed time: 26.058431148529053\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0519 - mean_squared_error: 0.0519\n",
      "Loss: 0.05192200839519501 , Elapsed time: 41.354286909103394\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2301 - mean_squared_error: 0.2301\n",
      "Loss: 0.23012633621692657 , Elapsed time: 63.23668169975281\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Loss: 0.03661171719431877 , Elapsed time: 83.06876134872437\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax     \n",
      "0  \t5     \t0.0366117\t0.0939377\t0.230126\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0798 - mean_squared_error: 0.0798\n",
      "Loss: 0.07981199771165848 , Elapsed time: 203.41753244400024\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0568 - mean_squared_error: 0.0568\n",
      "Loss: 0.05679528787732124 , Elapsed time: 42.01056718826294\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1893 - mean_squared_error: 0.1893\n",
      "Loss: 0.1893199384212494 , Elapsed time: 21.33950448036194\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t3     \t0.0366117\t0.0927218\t0.18932 \n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0422 - mean_squared_error: 0.0422\n",
      "Loss: 0.04223805293440819 , Elapsed time: 47.41157007217407\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0679 - mean_squared_error: 0.0679\n",
      "Loss: 0.06791171431541443 , Elapsed time: 203.4241805076599\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t2     \t0.0366117\t0.052637 \t0.079812\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0960 - mean_squared_error: 0.0960\n",
      "Loss: 0.09596838802099228 , Elapsed time: 204.6138300895691\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0391 - mean_squared_error: 0.0391\n",
      "Loss: 0.03908824920654297 , Elapsed time: 143.35365462303162\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0607 - mean_squared_error: 0.0607\n",
      "Loss: 0.06074228137731552 , Elapsed time: 78.96120738983154\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t3     \t0.0366117\t0.0538045\t0.0959684\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Loss: 0.036378443241119385 , Elapsed time: 83.3900773525238\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Loss: 0.03056924417614937 , Elapsed time: 66.45128726959229\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Loss: 0.03858696296811104 , Elapsed time: 64.6094753742218\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0352 - mean_squared_error: 0.0352\n",
      "Loss: 0.035165026783943176 , Elapsed time: 68.4864764213562\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t4     \t0.0305692\t0.0354623\t0.038587 \n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Loss: 0.03596372902393341 , Elapsed time: 64.59961867332458\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0788 - mean_squared_error: 0.0788\n",
      "Loss: 0.07884185761213303 , Elapsed time: 126.64475059509277\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Loss: 0.03653509169816971 , Elapsed time: 83.43603277206421\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0887 - mean_squared_error: 0.0887\n",
      "Loss: 0.08874109387397766 , Elapsed time: 53.7267906665802\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.0305692\t0.0541302\t0.0887411\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Loss: 0.03267800807952881 , Elapsed time: 104.08072519302368\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t1     \t0.0305692\t0.0417244\t0.0788419\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Loss: 0.035757843405008316 , Elapsed time: 70.07665252685547\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0375 - mean_squared_error: 0.0375\n",
      "Loss: 0.037456028163433075 , Elapsed time: 56.86623454093933\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Loss: 0.03557601943612099 , Elapsed time: 61.230748891830444\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0540 - mean_squared_error: 0.0540\n",
      "Loss: 0.053997695446014404 , Elapsed time: 142.93108534812927\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.0305692\t0.0386714\t0.0539977\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Loss: 0.036994460970163345 , Elapsed time: 52.59853148460388\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0403 - mean_squared_error: 0.0403\n",
      "Loss: 0.04033845663070679 , Elapsed time: 38.194976568222046\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Loss: 0.031721342355012894 , Elapsed time: 42.30182242393494\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Loss: 0.030899865552783012 , Elapsed time: 86.3870587348938\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t4     \t0.0305692\t0.0341047\t0.0403385\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0375 - mean_squared_error: 0.0375\n",
      "Loss: 0.03748289495706558 , Elapsed time: 15.02422833442688\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Loss: 0.03872212767601013 , Elapsed time: 21.340798139572144\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Loss: 0.029963379725813866 , Elapsed time: 36.807384967803955\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t3     \t0.0299634\t0.0336918\t0.0387221\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Loss: 0.0368259996175766 , Elapsed time: 36.398876905441284\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Loss: 0.03168635815382004 , Elapsed time: 33.83542776107788\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0375 - mean_squared_error: 0.0375\n",
      "Loss: 0.03746693208813667 , Elapsed time: 13.513292074203491\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Loss: 0.030514085665345192 , Elapsed time: 32.30124545097351\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t4     \t0.0305141\t0.0334125\t0.0374669\n",
      "-- Best Individual =  [0, 0, 1, 1, 1, 0, 1]\n",
      "-- Best Fitness =  0.03056924417614937\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABkOUlEQVR4nO3dd1gUx//A8fdRRcGGAorERMESbKio2IgggkEiikbxq8YWWyyxJDH2hiWWxJqEn8ZoLIkVoygWjJLYazARjY0IKKBio8Mxvz8uXiSUA+Q4yrye5x7vdndmP3OH97md3Z1RCCEEkiRJkpRHeroOQJIkSSpZZOKQJEmS8kUmDkmSJClfZOKQJEmS8kUmDkmSJClfZOKQJEmS8kUmjjLm/v37ODg4oFQqdR0KLi4unDp1StdhFKmtW7fStm1bHBwcePLkCQ4ODkREROg6LEkLhg0bxp49e3QdhlbIxFFIXFxcaNSoEXFxcZmWd+/enfr16xMZGanV/e/evZv69euzcOHCTMuPHj1K/fr1mTJlCgA1a9bk8uXL6OvrazWewrJq1Srq169PaGiorkN5bWlpaSxatIjvvvuOy5cvU6VKFS5fvoyNjQ0AU6ZM4csvv9RxlMXH1atXGTFiBI6OjrRs2ZJ3332XL7/8kmfPnuk6tCxWrVrF5MmTMy1bt24dPXr00FFE2iUTRyGytrYmMDBQ/frGjRskJycX2f7feOMNDhw4QHp6unpZQEAAb775ZpHFUJiEEOzdu5fKlStr7ZdbUR55PX78mJSUFGxtbYtsnyXBq3+vL126dImBAwfSvHlzDh48yIULF1i3bh36+vpcv35d5/GVdTJxFKLu3bsTEBCgfh0QEIC3t3embY4fP463tzfNmzfH2dmZVatWqdcdOHAAV1dX4uPjAThx4gTt2rXLchSTk2rVqlGvXj1+++03AJ4+fcrly5dxcXFRbxMZGUn9+vXV/xkGDBjAV199Rd++fXFwcGDIkCE57u/Zs2eMGDGCNm3a4OjoyIgRI4iOjlav11RXQEAAnTp1onXr1nz99dca23PhwgViY2OZOnUqBw4cIDU1FYChQ4eyefPmTNu+9957HD58GIDbt28zePBgWrVqhbu7OwcOHFBvN2XKFGbNmsWHH35Is2bNOHv2bK6fyX/jXrNmTaYutoyMDPz9/encuTOtW7dm/PjxPH36NEtb7t69i4eHBwCOjo4MHDgQgPr16/P333/z008/sW/fPtavX4+DgwMjR44EVEey69evx8vLixYtWvDxxx+TkpKirveXX36he/futGzZkr59+2b6UvX396dDhw44ODjg7u7O6dOnAQgNDaVnz540b96ctm3bZjlKfdX27dtxc3OjVatWjBw5kpiYGABmzpzJ4sWLM207atQoNmzYAEBMTAxjx46lTZs2uLi4sGnTJvV2q1atYty4cUyePJnmzZtn+6NgyZIl9OzZkxEjRlCtWjVAdbQ8btw4Wrdurd5u586ddO3aFUdHR4YOHUpUVJR6Xf369dm2bRtdunTB0dGROXPm8OpAGZrKbtmyhS5dutClSxcA5s+fj7OzM82bN6dnz55cuHABgJCQEL799lsOHjyIg4MD7733HqD6/7Bjxw5A9Xeydu1aOnXqhJOTE59++ikvXrwA/v0/uWfPHt55550s/z/y83kVGSEVik6dOomTJ0+KLl26iFu3bon09HTRsWNHERkZKerVqyciIiKEEEKcOXNGXL9+XSiVShEWFiacnJzEkSNH1PVMnDhRfPbZZyIuLk60a9dOHDt2LE/737Vrl+jbt6/4+eefxfjx44UQQmzevFnMmDFDLF++XHz22WdCCCEiIiJEvXr1RFpamhBCiP79+wtXV1dx584dkZSUJPr37y+WLFmS7T7i4uJEUFCQSExMFC9evBBjx44Vo0aNUq/Pra6bN2+KZs2aiXPnzomUlBSxYMEC0bBhQ3Hy5Mkc2/T555+LcePGidTUVNGqVStx6NAhIYQQe/bsEX369FFvd/PmTdGiRQuRkpIiEhISRMeOHcXOnTtFWlqa+OOPP0SrVq3EX3/9JYQQ4rPPPhPNmzcXFy5cEEqlUiQnJ+f6mbyM+/z58yIlJUUsWrRIvP322+q4N2zYIHr37i0ePHggUlJSxIwZM8SECROybc9/33shhKhXr54IDw9Xx7Z8+fJMZTp16iR8fHxEdHS0ePLkifDw8BBbt24VQgjxxx9/iDZt2ogrV66I9PR0sXv3btGpUyeRkpIibt++LTp27Ciio6PV+/7777+FEEK8//77Ys+ePUIIIeLj48Xly5ezjffUqVOiVatW4o8//hApKSli7ty5ol+/fkIIIc6dOyc6duwoMjIyhBBCPH36VDRu3FhER0cLpVIpevToIVatWiVSUlLEvXv3hIuLiwgJCRFCCLFy5Urx9ttviyNHjgilUimSkpIy7TchIUE0aNBAnDlzJtu4Xjpy5Ijo3LmzuHXrlkhLSxNr1qzJ9HdRr149MXz4cPHs2TMRFRUlWrduLU6cOJHnsoMGDRJPnjxRxxcQECDi4uJEWlqaWL9+vWjbtq1ITk5Wt2nSpEmZ4uvfv7/Yvn27EEKIHTt2iM6dO4t79+6J+Ph48dFHH4nJkyerP5t69eqJadOmiaSkJBEWFibs7e3FrVu38vV5FSV5xFHIXh51nDx5kjp16mBpaZlpfevWralfvz56eno0aNAAT09Pzp07p14/a9Yszpw5w8CBA3FxcaFTp0752r+bmxvnzp3jxYsX7N27l+7du2ss07NnT9566y3KlSuHh4cHYWFh2W5XpUoV3N3dMTExwdTUlFGjRnH+/Pk81RUUFMQ777yDo6MjRkZGjB8/Hj29nP/8kpKSCAoKwsvLC0NDQ9zd3dW/TDt37sz169fVvxD37duHm5sbRkZGHD9+HGtra3x8fDAwMMDe3h53d3cOHTqkrtvV1ZUWLVqgp6eHsbFxrp9JUFAQnTp1omXLlhgZGTFu3DgUCoW6rp9++okJEyZgZWWFkZERY8aM4dChQ4XavTFgwAAsLS2pXLkynTp1Ur+n27dvp0+fPjRt2hR9fX169OiBoaEhV65cQV9fn9TUVG7fvk1aWhq1atXijTfeAMDAwIB79+4RFxdHhQoVaNasWbb73bdvHz4+Ptjb22NkZMTEiRO5cuUKkZGRtGzZEoVCof7VfejQIZo1a4alpSVXr14lLi6OMWPGYGRkhI2NDe+//36mI79mzZrRuXNn9PT0KFeuXKb9Pn/+nIyMDPWRBsAXX3xBy5YtadasGWvXrgXgxx9/ZPjw4dStWxcDAwNGjhxJWFhYpiOHDz/8kIoVK1KzZk1at26tPiLLS9nhw4dTuXJldXzdu3enSpUqGBgYMGTIEFJTU7l7926ePsN9+/YxaNAgbGxsqFChAhMnTszSrTxmzBjKlStHgwYNaNCggTrWvH5eRclA1wGUNt27d6d///5ERkZm+6X9+++/s3TpUm7evElaWhqpqanqLgyAihUr4uHhwYYNG1i5cmW+91+uXDmcnZ1Zu3YtT548oUWLFoSEhORapnr16urnJiYmJCYmZrtdUlISCxcu5Ndff1WfoExISECpVKpPtudUV2xsLFZWVup15cuXp3LlyjnGdOTIEQwMDOjYsSMAXl5eDB48mLi4OKpWrYqzszOBgYEMHz6cwMBA5s2bB0BUVBShoaG0bNlSXZdSqVR3HwDUqFEj075y+0z+G7eJiUmmuO/fv89HH32UKQnq6enx+PHjLD8aCuq/72lsbKx63wEBAZm67dLS0oiNjaVVq1ZMnTqVVatWcevWLdq3b8+UKVOwtLTEz8+PlStX0rVrV2rVqsWYMWOy/YESGxuLvb29+nWFChWoXLkyMTEx1KpVi3fffZf9+/fj6OjIvn371O9xVFQUsbGxWT6DV1+/+p7+V8WKFdHT0+Phw4fUrVsXgE8//ZRPP/2UyZMnq89L3b9/nwULFmTqMhNCEBMTg7W1dbbvXUJCQp7L/vfv5LvvvmPHjh3ExsaiUCiIj4/nyZMnObbjVbGxsep6QXU+ND09ncePH6uXvZooX/2/k9fPqyjJxFHIrK2tqVWrFidOnMDPzy/L+kmTJtG/f3/WrVuHsbExfn5+mf74wsLC2LVrF926dWP+/PmsX78+3zF4e3vzwQcfMGbMmNdqy39999133L17l+3bt1O9enXCwsLw9vbO1G+cEwsLC27fvq1+nZSUlO25gJcCAgJITExU/wcRQpCWlsb+/fsZOHAg3bp1Y/Xq1Tg6OpKcnKzu965RowaOjo7qvva8yO0zsbCwyPSrMjk5OVPcVlZWLFiwgBYtWuR5fzl59UgmL2rUqMHIkSMZNWpUtuu9vLzw8vIiPj6emTNnsnTpUpYsWcKbb77J8uXLycjI4PDhw4wbN46zZ89Svnz5TOUtLCwy/QJPTEzk6dOn6oTYrVs3hgwZwvDhwwkNDWXNmjXquGrVqqU+55TftpYvX56mTZty5MgR2rRpo7H9r/4oyKu8lH01xgsXLvB///d/fP/999jZ2aGnp4ejo6P6b1/TZ/ff9/L+/fsYGBhgbm6e6TxhdvL6eRUl2VWlBX5+fmzcuDHbDzYhIYFKlSphbGxMaGgo+/fvV69LSUnhk08+YcKECSxcuJDY2Fi2bNmiXj9gwIAsJ26z06pVKzZs2ED//v0Lp0GvxG5sbEzFihV5+vQpq1evznNZd3d3jh8/zoULF0hNTWXlypVkZGRku21MTAynT5/mm2++ISAggICAAPbu3cuHH36ovvjA2dmZ+/fvs3LlSt599131L/533nmH8PBwAgICSEtLIy0tjdDQ0ExJK7t25fSZuLu7c+zYMS5duqSO+9VE6evry1dffaX+UoiLi+Po0aN5fl9eZW5unq/Ltnv37s2PP/7I77//jhCCxMREjh8/Tnx8PHfu3OH06dOkpqZiZGSEsbGx+qhw7969xMXFoaenR8WKFQGyvTzby8uL3bt3ExYWRmpqKsuXL6dJkybUqlULgLfffpuqVasyffp02rdvr66rSZMmmJqa4u/vT3JyMkqlkr/++itfl1RPnjyZXbt24e/vr/5VHh0dnen96du3L/7+/ty8eROAFy9ecPDgwTzVn9+yCQkJ6OvrU7VqVdLT01m9erX6IhZQfXZRUVE5/k1369aNjRs3EhERQUJCAl9++SVdu3bFwEDzb/e8fl5FSSYOLXjjjTdo3LhxtutmzZrFypUrcXBwYM2aNXTt2lW9btmyZVhaWtKvXz+MjIxYsmQJK1asIDw8HIAHDx7QvHlzjftXKBQ4OTnl2hVUEB988AEpKSm0adOGPn360KFDhzyXtbOzY+bMmUyePJkOHTpQsWLFHLsr9u7dS8OGDWnfvj3Vq1dXPwYMGMCNGzf466+/MDIyws3NjVOnTtGtWzd1WVNTU9avX8+BAwfo0KED7du3Z+nSpeorsrKT22diZ2fHjBkzmDhxIh06dKBChQpUrVoVIyMjAPW5qCFDhuDg4MD7779f4HtOevXqxa1bt2jZsiWjR4/WuH3jxo2ZN28ec+fOxdHRkS5durB7924AUlNTWbZsGa1bt6Z9+/bExcUxYcIEAH799Vc8PT1xcHDAz8+PL7/8EmNj4yz1Ozk5MX78eMaOHUv79u2JiIjIcp+Jp6dnls9AX1+fr7/+muvXr+Pq6kqbNm2YPn16pi9aTVq2bMnGjRs5f/487u7utGzZkmHDhtG6dWv1DyI3NzeGDRvGxIkTad68Od26ddPYLftSfsu2b9+ejh074u7ujouLC8bGxpm6sl52bbZu3Trbezd8fHx477336N+/P66urhgZGTFjxow8xZrXz6soKURe+hkknYuOjmb8+PH89NNPug6lTEtISMDR0ZFDhw6pb9yTpLJGJg5J0uDYsWM4OTkhhGDRokWEhoayZ8+efJ+TkKTSQnZVSZIGwcHBdOjQgQ4dOvD333+zfPlymTSkMk0ecUiSJEn5Io84JEmSpHwpE/dxXLlypcBXIaSkpOj8CoaiJttcNsg2lw2v0+aUlJRs71QvE4nD2NiYhg0bFqhsWFhYgcuWVLLNZYNsc9nwOm3Oafgh2VUlSZIk5YtMHJIkSVK+yMQhSZIk5UuZOMchSZKUF2lpaURGRhbpzJ3alpaWluO5ipfKlStHrVq1MDQ0zFOdMnFIkiT9IzIyEjMzM958881Sc5NnUlISJiYmOa4XQvD48WMiIyN566238lSn7KqSJEn6R3JyMubm5qUmaeSFQqHA3Nw8X0dZMnFIkiS9oiwljZfy22aZOHJx98ldjt8/ruswJEmSihWZOHKx7699jP5tND/f+FnXoUiSVAbUr1+fTz75RP06PT2dNm3aMGLECEA14Ka/v7+uwlOTiSMXI1qMwL6KPR8EfED403BdhyNJUilXvnx5bt68qT7fcPLkyUxz17u6ujJ8+HBdhaem1cQREhKCu7s7bm5u2WbJn3/+WT0vct++fbl+/TqgmuluwIABdO3aFU9PTzZu3Kgus2rVKjp06ED37t3p3r07J06c0Fr8xgbGLHdajhCC93e8T6oy51nkJEmSCkPHjh05fvw4AIGBgXh6eqrX7d69m7lz5wIwZcoU5s+fT9++fXF1dSUoKKjIYtTa5bhKpZK5c+eyYcMGLC0t6dWrFy4uLtja2qq3qVWrFps3b6ZSpUqcOHGCGTNmsGPHDvT19ZkyZQr29vbEx8fj4+NDu3bt1GUHDRrE0KFDtRV6JjamNmzovoGe23vyyeFPWNF1RZHsV5Ik3dq0Cb77rnDrHDIEBg7MfZt3332XtWvX0qlTJ27cuIGPjw8XL17MdtvY2Fi2bt3KnTt3GDVqlHoKW23T2hFHaGgotWvXxsbGBiMjIzw9PQkODs60TfPmzalUqRIAzZo1Izo6GgALCwvs7e0B1RzSderUISYmRluhatSjYQ8+bv0xK8+tZNe1XTqLQ5Kk0q9BgwZERkayf/9+nJ2dc922c+fO6OnpYWtry6NHj4ooQi0eccTExGBlZaV+bWlpSWhoaI7b79y5k44dO2ZZHhkZSVhYGE2bNlUv27JlCwEBATRq1IgpU6aok09OUlJSNN45mZPk5GTCwsIYVGsQwTeDGRQwCNMEU94wfaNA9ZUEL9tclsg2lw2a2pyWlkZSUhIAvXurHoXtn+qzJYQgKSmJjh07snjxYtatW8fTp09RKpUkJSWRmppKeno6SUlJpKen/1NfUqayOdWpSV7uMH9Ja4kju4kFc7pW+MyZM+zcuZOtW7dmWp6QkMC4ceOYOnUqpqamAPj6+jJ69GgUCgUrVqxg0aJFLFy4MNdYCmtY9X219uHwrQOfX/qcU0NPUc6gXIHqLO7k0NNlg2xz9utzu8ta2xQKBSYmJvTt25cqVarQpEkTzp49i76+PiYmJhgZGWFgYICJiQkGBgYYGRmp431Z9r803Tn+kqGhYZb3psiHVbeyslJ3PYHqCMTCwiLLdtevX2f69OmsXbuWKlWqqJenpaUxbtw4vLy86NKli3p5tWrV0NfXR09Pj969e3P16lVtNSGL2pVrs6nHJi5HX2bioYlFtl9JksoWKysrPvjgA12HkSOtHXE0btyY8PBwIiIisLS0JDAwkGXLlmXa5v79+4wdO5Yvvvgi0xgpQgimTZtGnTp1GDx4cKYysbGx6gR09OhR7OzstNWEbHWr141P2n7CklNL6Fi7I30b9S3S/UuSVHpdvnw5y7LWrVvTunVrAHr27EnPnj0BWLRokcay2qK1xGFgYMDMmTMZNmwYSqUSHx8f7Ozs2LZtG6DqclqzZg1Pnz5lzpw5AOjr67N7924uXrzI3r17qVevHt27dwdg4sSJODs7s2TJEvVlu9bW1upL04qSn4sfpyJO8eG+D3GwcqB+tfpFHoMkSZKuKER2JyNKmdedOjG7spHPI3H41oEapjU4O+wsJoa66xctbLLvu2yQbc7/+pIor+c4smt7Tu+HvHO8gGpVrMUPPX7gauxVxh4cq+twJEmSioxMHK/Bw9aDaR2msf7yen74/QddhyNJklQkZOJ4TbPfmY1zbWdGBo7k2sNrug5HkiRJ62TieE0GegZs89mGqZEpvXf0JiE1QdchSZIkaZVMHIWghlkNtvbcStjDMEYfGJ3tzY+SJEmaaBpWvbiQiaOQuNZxZZbzLDb9vokNVzboOhxJkkogTcOqFxcycRSi6R2n07lOZz468BGhMTmPyyVJkpST3IZVT0xM5PPPP8fHxwdvb2+OHj0KqMb069evHz169KBHjx5cunQJgLNnzzJ06FDGjRuHh4cHkyZNKpQeEa3dAFgW6evps7nHZhy+daD3jt5c+PACZsZmug5LkqQC2PT7Jr67XLjjqg9xGMLAprmPq57bsOrffPMNbdq0YeHChTx//pzevXvTtm1bzM3N2bBhA8bGxoSHhzNx4kR2794NwI0bN1i+fDkWFhb4+vpy8eJFWrZs+VrtkImjkFmaWrLNZxsum1wYvn84W3tuzfdE8JIklV25Dav+22+/cezYMb77Z6KQlJQUHjx4gIWFBXPnzuX69evo6ekRHh6uLmNvb68eqbxBgwZERUXJxFEcOb/pzLxO85h2bJrqUt2WI3UdkiRJ+TSw6UCNRwfa4uLiwhdffMGmTZt4+vRppnUrV66kTp06mZatWrWKatWqsXfvXjIyMmjSpIl6nZGRkfq5vr4+SqXyteOT5zi0ZEr7KXjYejA+aDyXHlzSdTiSJJUgvXr1YvTo0dSvn3kcvPbt27N582b1eYpr11T3jr148YLq1aujp6fH3r17CyU55EYmDi3RU+jxQ48fsKhgQe8dvXmW/EzXIUmSVELkNKz66NGjSU9P57333qNbt26sWKGayrpfv37s2bOH999/n/DwcMqXL6/V+OQgh1osC3Dy3kmcv3fGu4E3O3rvKBHnO0rjQG+ayDaXDXKQw5zJQQ6LkXZvtGNR50XsCtvF6nOrdR2OJEnSa5OJowhMcpqEVz0vJh2exLmoc7oOR5Ik6bXIxFEEFAoF33t/T02zmry/432eJD3RdUiSJEkFptXEERISgru7O25ubvj7+2dZ//PPP+Pl5YWXlxd9+/ZVz+yXW9mnT58yePBgunTpwuDBg3n2rGScdK5qUpWfev3E/Rf3GbR3kBzPSpKkEktriUOpVDJ37lzWrVtHYGAg+/fv59atW5m2qVWrFps3b2bfvn2MGjWKGTNmaCzr7++Pk5MThw8fxsnJKduEVFy1rtWaJW5L+PnGzyw/vVzX4UiSJBWI1hJHaGgotWvXxsbGBiMjIzw9PQkODs60TfPmzalUqRIAzZo1Izo6WmPZ4OBgvL29ATKN1VJSjGs9jp4NezIleAqnI07rOhxJkqR809qd4zExMerb3AEsLS0JDc154L+dO3fSsWNHjWUfP36MhYUFABYWFsTFxWmMJSUlhbCwsAK1Izk5ucBlc/Jp/U85f+88Pbf1ZFeXXVQxrlKo9b8ubbS5uJNtLhs0tTktLY2kpKQijCizZs2a4enpiZ+fH6AaVt3NzY1GjRqxatWqAtUphMhTm9LS0vL896C1xJFdH35O9zCcOXOGnTt3snXr1nyXzQtjY2Od3ceRkwCLAJzWOzH/z/ns77cfPUXxuU6hNF7Lrolsc9mQl/s48nLPg7aUL1+eO3fuoFAoKFeuHCdOnMDS0hJ9ff0Cx5XX+zgMDQ2zvY8jO/n6tsrIyCA+Pj5P21pZWam7nkB1FPHySOFV169fZ/r06axdu5YqVapoLGtubk5sbCwAsbGxVK1aNT9NKDaa12jOl+5fcvDWQb44+YWuw5EkqZjIbVj10NBQ+vbti7e3N3379uXOnTsAbNiwgc8//xxQjYbbrVs3rR45aTzimDRpEnPmzEFPT4+ePXsSHx/PoEGDGDZsWK7lGjduTHh4OBEREVhaWhIYGMiyZcsybXP//n3Gjh3LF198wVtvvZWnsi4uLgQEBDB8+HACAgJwdXUtSLuLhVEtRxHydwjTjk2jrU1bOtbuqOuQJEl6adMm+K5wh1VnyBAYWPBh1evUqcPmzZsxMDDg1KlTfPnll6xatYoPPviAAQMGcOTIEb7++mvmzJmj1SMnjYnj1q1bmJqa8vPPP+Ps7MzkyZPp2bOnxsRhYGDAzJkzGTZsGEqlEh8fH+zs7Ni2bRsAvr6+rFmzhqdPnzJnzhxANXLj7t27cywLMHz4cD7++GN27txJjRo11GO1lEQKhQJ/L38uPbhE3519uTLyChYVsh6VSZJUduQ2rPqLFy/47LPP+Pvvv1EoFKSlpQGgp6fHokWLeO+99+jTpw8tWrTQaowaE0d6ejppaWkcPXqU/v37Y2homOfzDc7Ozlka7uvrq37u5+enPgmUl7IAVapUYePGjXnaf0lQ0bgiO3rvoM36Nvxv9/8I+l8Q+nr6ug5LkqSBAzUeHWhLTsOqr1ixgtatW7NmzRoiIyMZ+Ep8Lwc3fNmVr00az3H06dMHFxcXkpKScHR0JCoqClNTU60HVpY0tWrKqq6rOHrnKH6/Zp9IJUkqO3IaVv3FixfqOcj37NmTabmfnx+bN2/m6dOnBAUFaTU+jYlj4MCB/Prrr/zf//0fCoUCa2trNm3apNWgyqKhDkPp36Q/s4/P5tjdY7oOR5IkHcppWPVhw4axfPly+vbtm2nOjQULFtCvXz/eeust/Pz8WLZsGY8fP9ZegEKD77//Xrx48UJkZGSIzz//XHh7e4tff/1VU7Fi5dq1azopm18vUl6IBqsbCMslluL+8/tFtt//Kso2FxeyzWWDpjaXxvckMTExT9tl1/ac3g+NRxy7du3C1NSU3377jbi4OBYuXJjl6iipcJgambKz906epzyn3+5+pGek6zokSZKkLDQmDvHPzXgnTpzAx8eHBg0ayAH6tMjewp61nms5Hn6cH//4UdfhSJIkZaExcTRq1IghQ4YQEhJC+/btiY+PR0+v+NzlXBp90PQDrEytCLwZqOtQJKnMKYs/jPPbZo2X4/r5+REWFoaNjQ0mJiY8efKEBQsWFDhASTOFQoF7XXf2/bUPZYZSXp4rSUWkXLlyPH78GHNz8xIxzXNhEELw+PFjypUrl+cyGhOHQqHg1q1b/PLLL4wZM4akpCRSU1NfK1BJMw9bDzb+vpEL9y/QulZrXYcjSWVCrVq1iIyM5OHDh7oOpdCkpaVhaGiY6zblypWjVq1aea5TY+KYPXs2enp6nDlzhjFjxlChQgXGjh3Lrl278rwTKf/c6rihQEHQrSCZOCSpiBgaGmYa/qg00MZglhpPVoSGhjJr1iyMjY0BqFSpkvo2d0l7zMub08q6FUG3tXsjjyRJUn5pTBwGBgYolUp1f19cXJw8OV5EPGw9OBd1jseJWryRR5IkKZ80ZoABAwbw0Ucf8fjxY7788kt8fX0ZMWJEUcRW5nnYepAhMjh6p2TNcihJUumm8RzHe++9h729PWfOnEEIwdq1a6lbt25RxFbmOdZ0pEq5KgTdDqJPoz66DkeSJAnI4wyAb775JqampuqxUe7fv0/NmjW1GpgE+nr6dKnbhaBbQQghyszlgZIkFW8aE8cPP/zA6tWrqVatWqZzG/v27dNqYJKKh60HP/35E7/H/E4zq2a6DkeSJElz4ti0aRNBQUHqaV2louVe1x2AoFtBMnFIklQsaDw5bmVlhZmZWYEqDwkJwd3dHTc3N/z9/bOsv337Nn369KFRo0asX79evfzOnTt0795d/WjevDnff/89AKtWraJDhw7qdSdOnChQbCVFDbMaNLVsStAteVmuJEnFg8YjDhsbGwYMGMA777yDkZGRevngwYNzLadUKpk7dy4bNmzA0tKSXr164eLigq2trXqbypUrM23aNIKDgzOVrVOnDnv37lXX07FjR9zc3NTrBw0axNChQ/PWwlKgq21Xlp5eyvOU51Q0rqjrcCRJKuM0HnHUrFmTdu3akZaWRkJCgvqhSWhoKLVr18bGxgYjIyM8PT2zJAhzc3OaNGmCgUHO+ev06dPY2NhgbW2dh+aUTh62HqRnpMsJniRJKhY0HnHUrVuXrl27Zlp28OBBjRXHxMRgZWWlfm1paUloaGi+AwwMDKRbt26Zlm3ZsoWAgAAaNWrElClTqFSpUq51pKSkEBYWlu99AyQnJxe4bGGprKxMBYMKbDu/jfqivuYCr6k4tLmoyTaXDbLNhUNj4vD398+SOLJb9l/ZDdOb38tJU1NTOXbsGJMmTVIv8/X1ZfTo0SgUClasWMGiRYtYuHBhrvUYGxsXeKwWbYzzUhBuf7px9sFZGjRooPXLcotLm4uSbHPZINuc/7LZyTFxnDhxgpCQEGJiYpg/f756eXx8PPr6mof5trKyIjo6Wv06JiYGCwuL/MRMSEgI9vb2VKtWTb3s1ee9e/dm5MiR+aqzpPKo60HA9QBuPL5Bg2oNdB2OJEllWI7nOCwtLWnUqBHGxsbY29urHy4uLpmugMpJ48aNCQ8PJyIigtTUVAIDA3FxcclXcIGBgXh6emZaFhsbq35+9OhR7Ozs8lVnSeVu++9luZIkSbqU4xFHgwYNaNCgAV5eXrmevM6xYgMDZs6cybBhw1Aqlfj4+GBnZ8e2bdsAVZfTw4cP8fHxUc8quHHjRg4cOICpqSlJSUmcOnWKuXPnZqp3yZIlXL9+HQBra+ss60urNyu/SYNqDQi6FcTHbT7WdTiSJJVhOWaE8ePHs2LFCnr06JHt+rzcOe7s7Iyzs3OmZb6+vurn1atXJyQkJNuyJiYmnD17NsvyJUuWaNxvaeVR14NvLn5DUloSJoYmug5HkqQyKsfEMWXKFAC++eabIgtGyp2HrQdfnf2KE3+fwMPWQ9fhSJJURuV4jmP06NGAqjvou+++w9raOtNDKnoda3eknEE5eZ5DkiSdyjFxvHo57aVLl4okGCl3JoYmvPPmOzJxSJKkUzkmDjmEd/HkUdeDG49vcPfJXV2HIklSGZXjOY47d+7g5eUFwL1799TPX5LDquuGh60HHIJDtw8xsmXZuIdFkqTiJcfEceDAgaKMQ8qjeub1eLPymwTdCpKJQ5IkncgxccgT4MWTQqHAo64Hm69uJlWZipG+keZCkiRJhUjj6LhS8eNh60F8ajynIk7pOhRJksogmThKIJe3XDDQM5BXV0mSpBN5ShzJycncuXNH27FIeWRmbEb7N9rLxCFJkk5oTBzHjh2je/fuDBs2DFANs1tWRqQtzjzqevB7zO/cf3Ff16FIklTGaEwcq1evZufOnVSsqJqytGHDhkRFRWk9MCl3Xe1U86Ecvn1Yx5FIklTWaEwc+vr6mJmZFUUsUj40tmhMDdMaHLyleTZGSZKkwqRxvHQ7Ozv27duHUqkkPDycH374AQcHh6KITcqFQqHAw1Y1uVN6RjoGevkf+l6SJKkgNB5xzJgxg1u3bmFkZMTEiRMxNTVl2rRpRRGbpIGHrQdPkp9wPuq8rkORJKkM0fgz1cTEhAkTJjBhwoR8Vx4SEoKfnx8ZGRn07t2b4cOHZ1p/+/Ztpk6dyp9//smECRMYOnSoep2LiwsVKlRAT08PfX19du/eDcDTp0+ZMGECUVFRWFtb89VXX1GpUqV8x1YadK7TGT2FHkG3gnCycdJ1OJIklREaE0d2V1CZmZnRqFEj+vbti7GxcbbllEolc+fOZcOGDVhaWtKrVy9cXFywtbVVb1O5cmWmTZtGcHBwtnVs3LiRqlWrZlrm7++Pk5MTw4cPx9/fH39/fz755BNNzSiVqppUpbV1a4JuBzGn0xxdhyNJUhmhsauqVq1aVKhQgffff5/3338fU1NTqlWrRnh4ONOnT8+xXGhoKLVr18bGxgYjIyM8PT2zJAhzc3OaNGmSr6lpg4OD8fb2BsDb25ujR4/muWxp5GHrwfmo8zxKfKTrUCRJKiM0fmOHhYWxZcsW9WsXFxf+97//sWXLFjw9PXMsFxMTg5WVlfq1paUloaGh+Qpu6NChKBQK+vTpQ58+fQB4/PgxFhYWAFhYWBAXF6exnpSUFMLCwvK175eSk5MLXLYo1Nevj0Dw/a/f4/lGzp9HfhT3NmuDbHPZINtcODQmjri4OO7fv0/NmjUBuH//Pk+ePAHA0NAwx3KvTgT1Un7m+Ni2bRuWlpY8fvyYwYMHU6dOHRwdHfNc/lXGxsY0bNiwQGXDwsIKXLYo1Muoh/kpc64mXWVyw8mFUmdxb7M2yDaXDbLN+S+bHY2JY8qUKfTr1w8bGxsAIiMjmTVrFomJieouo+xYWVkRHR2tfh0TE6M+UsgLS0tLQNWd5ebmRmhoKI6OjpibmxMbG4uFhQWxsbFZzoGUNfp6+nSp24VDtw6RITLQU8jhxyRJ0i6N3zLOzs4cPnyYadOmMXXqVIKCgnjnnXcoX748gwYNyrFc48aNCQ8PJyIigtTUVAIDA3FxcclTUImJicTHx6ufnzx5Ejs7O0DVVRYQEABAQEAArq6ueaqzNPOw9SAmIYbfo3/XdSiSJJUBeTorHR4ezp07d0hNTeXGjRsAuR5tABgYGDBz5kyGDRuGUqnEx8cHOzs7tm3bBoCvry8PHz7Ex8eH+Ph49PT02LhxIwcOHODJkyd89NFHgOrqrG7dutGxY0cAhg8fzscff8zOnTupUaMGK1asKGjbS40udbsAEHQrCIca8uZMSZK0S2PiWL16NWfPnuX27ds4OzsTEhJCixYtNCYOUB2tODs7Z1rm6+urfl69enVCQkKylDM1NeXnn3/Ots4qVaqwceNGjfsuS6xMrXCwciDodhCfd/hc1+FIklTKaeyqOnToEBs3bqRatWosXLiQvXv3kpqaWhSxSfngYevBqYhTPEt+putQJEkq5TQmDmNjY/T09DAwMCA+Ph5zc3MiIiKKIjYpHzxsPUjPSOfY3WO6DkWSpFJOY1dVo0aNeP78Ob1796Znz56UL1+eJk2aFEVsUj441XLCzMiMoFtB9GjYQ9fhSJJUiuWaOIQQjBgxgooVK+Lr60uHDh2Ij4+nQYMGRRWflEeG+oZ0rtOZoNtBCCHydc+MJElSfuTaVaVQKNRXN4Fq+BGZNIovD1sP7j27x/VH13UdiiRJpZjGcxxNmzbN91Ahkm6413UHkHORS5KkVRrPcZw9e5Yff/wRa2trTExM1Mv37dun1cCk/KtduTYNqzUk6HYQE5zyPwy+JElSXmhMHP/3f/9XFHFIhaSrbVfWnF9DYloi5Q3L6zocSZJKIY1dVdbW1jx48IAzZ86ojzoyMjKKIjapADxsPUhRpnAi/ISuQ5EkqZTSmDhWr17NunXr8Pf3ByAtLa3MTpxUEnSo3QETAxN5nkOSJK3RmDiOHDnC119/rT6/YWlpSUJCgtYDkwqmnEE5Or3ViaDbMnFIkqQdGhOHoaEhCoVCfV9AYmKi1oOSXo9HXQ/+evwXd57c0XUokiSVQhoTR9euXZk5cybPnz9n+/btDB48mPfff78oYpMKyMPWA5CX5UqSpB0ar6oaOnQoJ0+epEKFCty9e5dx48bRrl27oohNKiDbqrbUqVKHoFtBjHYcretwJEkqZTQmju+//x4PDw+ZLEoQhUKBR10PNv6+kZT0FIwNjHUdkiRJpYjGrqr4+HiGDh1Kv3792LJlC48ePSqKuKTX5GHrQUJaAicjTuo6FEmSShmNiWPMmDEEBgYyc+ZMYmNj6d+/f65Txr4qJCQEd3d33Nzc1Jfzvur27dv06dOHRo0asX79evXyBw8eMGDAALp27Yqnp2emiZtWrVpFhw4d6N69O927d+fECXm/QnY6vdUJQz1DeZ5DkqRCl6epYwHMzc2pVq0alStX5vHjxxq3VyqVzJ07lw0bNmBpaUmvXr1wcXHB1tZWvU3lypWZNm0awcHBmcrq6+szZcoU7O3tiY+Px8fHh3bt2qnLDho0iKFDh+Y19DLJ1MiUDrU7EHQriC/cvtB1OJIklSIajzi2bt3KgAEDGDRoEE+ePGH+/Pl5GqcqNDSU2rVrY2Njg5GREZ6enlkShLm5OU2aNMHAIHP+srCwwN7eHlBNI1unTh1iYmLy0y4J1WW5V2OvEvU8StehSJJUimg84rh//z5Tp06lYcOGAKSkpHDw4EG6du2aa7mYmBisrKzUry0tLQs0ym5kZCRhYWE0bdpUvWzLli0EBATQqFEjpkyZQqVKlXKtIyUlhbCwsHzvGyA5ObnAZXWtnl49ADb8ugGfOj55LleS21xQss1lg2xz4dCYOCZPnoxSqeTEiRMEBgby22+/0bJlS42JQwiRZVl+JxdKSEhg3LhxTJ06FVNTUwB8fX0ZPXo0CoWCFStWsGjRIhYuXJhrPcbGxurEl19hYWEFLqtrDUQDap6uSWhiKNMbTs9zuZLc5oKSbS4bZJvzXzY7uSaO8+fPs2/fPk6cOEGTJk24dOkSwcHBmYZXz4mVlRXR0dHq1zExMVhYWOQ54LS0NMaNG4eXlxddunRRL69WrZr6ee/evRk5cmSe6yxrXl6Wu/v6btIz0jHQy/MpLUmSpBzleI6jY8eOLFu2jObNmxMYGMiqVaswNjbOU9IAaNy4MeHh4URERJCamkpgYCAuLi55KiuEYNq0adSpU4fBgwdnWhcbG6t+fvToUezs7PJUZ1nlYevB0+SnnIs6p+tQJEkqJXL8CdqlSxeCg4M5ePAg+vr6uLq65qurycDAgJkzZzJs2DCUSiU+Pj7Y2dmxbds2QNXl9PDhQ3x8fIiPj0dPT4+NGzdy4MABrl+/zt69e6lXrx7du3cHYOLEiTg7O7NkyRKuX1dNjWptbc3cuXNfp/2lXuc6ndFT6BF0K4i2Nm11HY4kSaWAQmR3MuIfQgjOnDlDYGAgJ06cID4+Hj8/P5ydnalQoUJRxvlaXrePr6T3ibb7rh1pyjTOfZi3o47S0Ob8km0uG2SbC6dsrpfjKhQKnJycmD9/PseOHWPZsmUEBwfnuctJKh486npw4f4FHiY81HUokiSVAhrv43jJ0NAQFxcXli1bJu/WLmE8bD0QCI7cOaLrUCRJKgXynDheVa5cucKOQ9KiFjVbUK18NTn8iCRJhaJAiUMqWfQUerjXdefQ7UNkCDlfvCRJryfHxPHtt99y7dq1ooxF0iIPWw9iE2K5En1F16FIklTC5Xg5bq1atdi0aRPXr1+nQYMGdOzYkXbt2mkc3kMqnrrUVd1EGXQriOY1mus4GkmSSrIcE4enpyeenp4AXLt2jV9//ZUxY8aQkZGBk5MTHTt2pEmTJkUWqPR6LCpY0KJGC4JuBTG1w1RdhyNJUgmWpzEo3n77bd5++21GjBhBfHw8J0+eZMeOHTJxlDAeth4s+m0Rz5KfUamcPHKUJKlg8n1y3NTUFHd3d+bNm6eNeCQt8rD1QCmUBN8N1ryxJElSDuRVVWVIm1ptqGRciYM3D+o6FEmSSjCZOMoQAz0DOtfpTNDtoGyHvZckScqLPJ3jiImJISoqCqVSqV7m6OiotaAk7fGw9WBX2C6uPbyGvYW9rsORJKkE0pg4lixZwsGDB6lbty76+vrq5TJxlEzudd0B1WW5MnFIklQQGhPH0aNHCQoKwsjIqCjikbTMppIN9tXtCbodxKS2k3QdjiRJJZDGcxw2NjakpaUVRSxSEfGw9SDk7xASUhN0HYokSSWQxiMOExMTvL29cXJyynTUMX163uewlooXD1sPlp1exvHw43jW89R1OJIklTAajzhcXFwYPXo0Dg4O2Nvbqx95ERISgru7O25ubvj7+2dZf/v2bfr06UOjRo1Yv359nso+ffqUwYMH06VLFwYPHsyzZ8/yFIv0r/ZvtKe8YXk5Wq4kSQWi8YijR48eBapYqVQyd+5cNmzYgKWlJb169cLFxQVbW1v1NpUrV2batGkEBwfnuay/vz9OTk4MHz4cf39//P39+eSTTwoUY1lVzqAcnd7sRNBtmTgkScq/HI84xo8fD4CXl1e2D01CQ0OpXbs2NjY2GBkZ4enpmSVBmJub06RJEwwMDPJcNjg4GG9vbwC8vb05evRovhosqXjYenAr7ha34m7pOhRJkkqYHI84pk2bBsA333xToIpjYmKwsrJSv7a0tCQ0NPS1yz5+/BgLCwsALCwsiIuL01hfSkoKYWFh+QlfLTk5ucBlizNbVEd+G3/bSD+7fpnWldY250a2uWyQbS4cOSaOl1/O1tbWBao4uzuTFQqF1stmx9jYuNAnay/pGtKQumfqciXhCvMaZh53rLS2OTeyzWWDbHP+y2Ynx8Th4OCQ6ctaCIFCoVD/e+nSpVx3aGVlRXR0tPp1TEyMOhlpkltZc3NzYmNjsbCwIDY2lqpVq+apTikrD1sPNlzZQEp6CsYGxroOR5KkEiLHcxxOTk7Y2toyatQo9u/fz+XLl7l06ZL6X00aN25MeHg4ERERpKamEhgYiIuLS56Cyq2si4sLAQEBAAQEBODq6pqnOqWsutp2JTEtkd/u/abTOCKeRfDhzx9S7YtqXLx/UaexSJKkWY5HHGvXruXFixccPnyYGTNmkJKSQteuXfH09KRy5cqaKzYwYObMmQwbNgylUomPjw92dnZs27YNAF9fXx4+fIiPjw/x8fHo6emxceNGDhw4gKmpabZlAYYPH87HH3/Mzp07qVGjBitWrCicdyInpXgwwHfefAcjfSOCbgXhWqfoE3BsQiwLf13I2gtrATDSN+LTo59ydMDR1+qalCRJuxQiD8OkZmRkcODAAebNm8fIkSMZPHhwUcRWaArcx7dhA2LYMBTGxlC+fN4eFSoUbFuDPI03WejcfnAjOj6aq6Ouqpdpux/4WfIzlp5aypdnviQpPYlBTQcx03kme2/sZXzQeA71P6Se6raoyL7vskG2uXDK5vptdenSJQIDA7lw4QItWrRgzZo1tGzZskABlEguLjwaNYrq5ctDYiIkJKj+ffl48QJiYjIvS0iAggzRYmiYOalUrQorV0KbNoXfrld41PVg8pHJRD6PpFbFWlrdV2JaIqvPrWbRb4t4kvyE3m/3Zm6nuTSo1gCAES1G8NWZr/js6Gd0rtMZPYUc9V+SiqMcE4eLiwtmZmZ4enoyb9489ci4f/75J0Ce7x4v0WrX5tFHH1E9v9k6LQ2SkjInlP8+/puE/vsICYGePeHyZbC01E77UJ0gn3xkModuHWJo86Fa2UeqMpV1l9YxL2Qe0fHRdLXtynyX+TSv0TzTdsYGxszrNI/+e/rz0x8/4dvYVyvxSJL0enJMHC8vw/3111/57bffMl0iq1Ao2LRpk/ajK6kMDVWPihULXsfvv6uONnx94cgReGVI+8L0dvW3qVWxFkG3gwo9cSgzlGy9upVZx2dx9+ld2r/Rnu29ttOhdoccy/g29mXJqSVM/2U6Pm/7YKQvR2WWpOImx8Txww8/FGUc0n81bQpr18KQITBzJvj5aWU3CoUCj7oe7Li2g/SMdAz0Xv9cixCCgOsBTP9lOtceXsPByoED/Q7gYeuh8aS3nkKPRZ0X0XVLV/wv+jOm1ZjXjkeSpMIlO5GLs8GDYehQWLAA9u/X2m48bD14lvKMs5FnX6seIQRH7xyl9brW9NzeE2WGku29tnNh+AW62nXN85VS7nXd6fRmJ+aemMuLlBevFVNxdvj2YZp904xJhyZxKuIUGSJD1yFJUp7IxFHcrVoFzZrBgAFw965WduFaxxV9hf5rjZZ7OuI0rptccfvBjZiEGL577zv+GP0Hve175/skt0KhYFHnRTxMfMjy08sLHFNxlpCawIf7PiTieQSrzq2i3XftsF5uzejA0Ry9c5Q0pZwDRyq+cvwfnZ6eXpRxSDkxMYFdu1T3k/TqBcnJhb6LyuUq42TjxMFbB/NdNjQmlPe2vUfb79ry58M/WeGxgr/G/MVgh8Gv1e3VyroVvd7uxdLTS4mJjylwPcXV3BNzuffsHnv77uXhJw/Z2nMr7WzasfH3jbj94IbVMisG7x3Mvhv7SE4v/M9ckl5Hjonj/fffZ/To0Wzbto3IyMiijEn6rzp1YNMmuHQJPv5YK7vwqOvBxQcXiU2IzdP2t+Ju8b/d/6PZN80I+TsEPxc/bo+7zbjW4wpt+BI/Fz+S0pKYHzK/UOorLv6I/YPlZ5YzpNkQ2r/RnkrlKuHb2Jed7+/k4ScP2dNnD552nuwJ28N7P75H9SXV6bOzD9v/3F6qu+6kkiPHn4S7d+8mKiqKkJAQFixYQExMDC1atKBjx460atVKzkFe1N57Dz79FL74Atq1U3VdFSIPWw+m/zKdw7cP08KwRY7bRT6PZN6Jeay/vB5jA2OmtJ/CJ20/oYpJlUKNB6CeeT2GNR/Gtxe/5eM2H1O3at1C30dRyxAZjAocRUXjiix2W5xlfXnD8ng38Ma7gTepylR+ufsLu8N2E3AjgO1/bsdY35gudbvQs2FP3qv/HlVN5Fhtkg6IPEpNTRWnTp0SixcvFj4+PuLDDz/Ma1Gdu3btmk7KFrq0NCGcnYUwMREiNLRQq1ZmKEX1L6qL/+36X7ZtfpjwUEwMmiiM5xkLw7mGYkzgGPHgxYNCjSE795/fF+X9ygvfnb5a3U9Rfc7fXfpOMBux/tL6fJVLV6aLkPAQ8fHBj8UbX74hmI3Qn6MvOm/qLNaeWyvuP7+f71iK1d92EZFtLpyyeU4c/xUdHV3gYIpaqUkcQgjx4IEQVlZC2NkJ8exZoVbdf3d/Ue2LauKPP/9QL3uW/EzM+mWWMF1gKvTm6IlBAYPE3Sd3C3W/mkw9OlUwG3Hp/iWt7aMoPudHCY+E+WJz0W59O6HMUBa4noyMDHE+6rz4/Ojnot6qeoLZCMVshWi7vq1YdmqZuBN3J0/1FLu/7SIg21w4ZQt8VZWlFu9mlnJhZQU//QR37qju8SjEQRg96nrwKPER155cIyktiaWnllJnRR3mnJiDe113/hj1Bxu6b+DNym8W2j7z4tN2n1LVpCpTgqcU6X4L22dHP+Np8lO+9vz6tYZTUSgUtKzZkgWuC7j+0XX+HP0nc96ZQ2JaIpMOT6LOyjo0/7Y580PmE/awbE1aJBUNeTluSdSxIyxcqLraqhBHB+5StwsKFHx19StsV9nyyZFPaFmzJec/PM/O93fSsLpuBoerVK4S0zpM4/DtwwTfCdZcoBg6ee8k6y+vZ6LTRBpbNi60ehUKBW9Xf5sZzjO4POIyt8fdZqnbUkwMTZjxywzeXvs2Ddc0ZFrwNC7ev5jtJGmSlF8aE0dKSkqWZXmZrlXSssmTwdsbPvkETp4slCqrV6hOy5otORVzijcrv8nxD44T1D+IljV1P7DlaMfRvFHpDaYETylxX35pyjRGBY7CpqINM51nanVfdarUYVLbSZwccpKoiVGseXcN1mbWLD65mJb/15K3VrzFxEMT+e3eb/KGQ6nANCaOXr16ceXKFfXrQ4cO4esrB5/TOYUCNmyA2rXh/fchNm+X0WqyqccmvnP+jt8G/4bzm86FUmdhKGdQjrnvzOXC/QvsvLZT1+Hky4qzK7gae5WVXVdiamRaZPutaVaT0Y6jOTrwKDGTY9jQfQONLRuz5vwaOmzowIchH/I48XGRxSOVHhoTx9KlS5k3bx6LFy9m0qRJbN++nY0bN+ap8pCQENzd3XFzc8Pf3z/LeiEE8+fPx83NDS8vL/XIu3fu3KF79+7qR/Pmzfn+++8BWLVqFR06dFCvO3HiRD6aW8pUrgw7d0JcHPTrB0rla1fZoFoD2li2KZYTKfVv0p9GFo2YemxqibmzOuJZBLOPz8arnhfd63fXWRzm5c0Z1GwQ+3z38eiTR6zquooLDy/g+H+OhMaE6iwuqYTKy5n1I0eOiGbNmol27dqJ8PDwPJ2NT09PF66uruLevXsiJSVFeHl5iZs3b2ba5vjx42Lo0KEiIyNDXL58WfTq1Svbetq2bSsiIyOFEEKsXLlSrFu3Lk8xvFSqrqrKzvr1QoAQ06YVSnXFuc37buwTzEZ8ff7rQq1XW23u8WMPYTLfpMivRMuLbb9uEzWX1RTl/cqLHX/u0HU4RaI4/21ri06uqpo6dSobN27k559/ZuHChYwcOZItW7ZoTEihoaHUrl0bGxsbjIyM8PT0JDg484nN4OBgvL29USgUNGvWjOfPnxP7ny6X06dPY2Njox7mXcrGkCGqh58fBAbqOhqt8rTzpMMbHZhzYg4JqQm6DidX+//az57re5jpPLPIr0TLi6bmTbnw4QWaWjal947eTAuehjLj9Y9apdJP42BC9erVw8/PD4VCgY2NDdu3b2fhwoUaK46JicHKykr92tLSktDQ0Fy3sbKyIiYmBgsLC/WywMBAunXrlqncli1bCAgIoFGjRkyZMoVKlSrlGktKSgphYQW7LDE5ObnAZYuSYswY3jx1CsN+/bi7axdpr5Foi3ubR9mOot+xfkzdN5WRb48slDoLu81J6UmMPDSSuhXr0rVy12L5fiYnJ/M08ilft/4aPyM/Fvy2gJO3T7K49WIqGr3GXDLFWHH/29YGrbS5wMcwGhw4cEBMnTpV/XrPnj1i7ty5mbb58MMPxfnz59WvBw4cKK5evap+nZKSIlq1aiUePnyoXvbw4UORnp4ulEqlWL58uZgyZYrGWEp9V9VLt24JUamSEC1bCpGcXOBqSkKbvX/0FmYLzMTDhIeaN86Dwm7z50c/F8xGnAg/Uaj1FqZX25yRkSG+Pv+1MJhrIOqtqieuxRb/v4GCKAl/24VNJ11V4eHhjBs3jnfffRdXV1f1QxMrKyuio6PVr/97JJHdNtHR0Zm2CQkJwd7enmrVqqmXVatWDX19ffT09OjduzdXr17VGEuZUbcubNwIFy5obTDE4mKBywIS0hJY8OsCXYeSxbWH11hyagkfNP2AjrU76jqcPFEoFIxsOZJjA4/xNPkprde1Zt+NfboOSyqmNCaOzz//HF9fX/T19dm0aRPe3t5076756pDGjRsTHh5OREQEqampBAYG4uLikmkbFxcXAgICEEJw5coVzMzMsnRTeXp6Zirz6jmQo0ePYmdnpzGWMqV7d9W9Hd98A5s36zoarWlYvSGDmw1mzfk1/P30b12HoyaEYHTgaMyMzFjitkTX4eRbh9oduPDhBeqZ1+O9H99j3ol58n4PKYs83QDo5OQEqOYhHzt2LGfOnNFYsYGBATNnzmTYsGG8++67dO3aFTs7O7Zt28a2bdsAcHZ2xsbGBjc3N2bMmMGsWbPU5ZOSkjh16hRdunTJVO+SJUvw8vLCy8uLM2fO8Pnnn+erwWXCggWqu8uHD4c//tB1NFoz+53Z6Cn0mHlcuzfV5ccPoT9w4u8TLO68mOoVqus6nAKxqWTDr4N/ZUCTAcw8PpPeO3rL4dylTDSeHDcyMiIjI4PatWuzefNmLC0tefw4bzcNOTs74+yc+SayV28eVCgUmZLFq0xMTDh7NutUpkuWlLxfcUXOwAB+/BEcHMDHR9V1ZWam66gKXa2KtRjXahxLTi1hktMkmlg20Wk8cUlxTDo8CadaTgxtPlSnsbwuE0MTNnpvpHmN5kw+PBmnR04E9A3AtqqtrkOTioE8XY6blJTE9OnT+fPPP9m7dy+LF2edR0AqZmrUUA2GePu2at7yEjZMR15NaT+FSuUqMTV4qq5D4fOjn/Mk6clrD2JYXCgUCj5u8zGH+h/iQfwDHP/PkUO3Duk6LKkY0PjX3aRJEypUqICVlRULFy5k9erVNGvWrAhCk16bs7Oq22rHDli5UtfRaEUVkyp83v5zAm8GEvJ3iM7iOB1xGv9L/oxvPZ6mVk11Foc2uNZx5cKHF3ij0hu8u/VdlpxcUuLGC5MKV45dVSNH5n59/DfffFPowUha8MkncOqUalBER0do21bXERW6sa3GsvLsSj47+hmnhpwq8uFS0jPSGRU4Cmsza2a/M7tI911U3qryFqeGnGLIz0P49OinXI6+zLr31lHesLyuQ5N0IMfEceXKFWrUqIGnpydNmzaVvzBKKoUCvv8eWrRQDYZ46RL857Loks7E0IQ578xh2L5hBFwPoEfDHkW6/1VnV/F7zO/s7L0TM+PSdy7ppQpGFfjR50ccrByYGjyVsEdhBPQJoHbl2roOTSpiOXZVnTx5kgkTJnDz5k38/Pw4efIkVapUoVWrVrRq1aooY5Re18vBEB89gv/9r1AGQyxuPmj2AQ2qNWDqsamkZ6QX2X4jn0cy8/hM3rV7l54NexbZfnVFoVAwpf0UAvsFcvfJXVr+X0uOhx/XdVhSEcsxcejr69OxY0cWL17M9u3bqV27NgMGDOCHH34oyvikwuLgAGvWwNGjMGeOrqMpdAZ6Bix0Xcj1R9f5/sr3RbbfCYcmkJ6Rzqquq4rliMLa0tWuK+c+PEf18tXpvKkzq86ukr0SZUiuJ8dTU1M5fPgwkydPZsuWLQwYMCDLfRVSCTJ0KAweDPPmwcGDuo6m0HWv3x2nWk7MPj6bxLREre/v4M2D7Ly2k+kdplOnSh2t76+4qWdejzPDzuBZz5NxQeMY8vMQktOTdR2WVARyTByfffYZffv25c8//2TMmDHs2rWLjz76SM41XtKtWQNNm0L//vB38bnjujAoFAoWdV5E1IsoVp1dpdV9JaUlMebgGBpUa8DktpO1uq/irKJxRfb02cMs51l8f+V7nL93Jup5lK7DkrQsx8Sxd+9e7t69y6ZNm+jbty/NmzenefPmODg40Lx586KMUSpMJiaq8x3p6dCrF2QzNXBJ1rF2RzztPFl0chFxSdqb4njBrwu48+QOa99di7GBsdb2UxLoKfSY/c5s9vTZw7WH12jh34KT9wpnOmOpeMoxcVy/fp3Lly9z+fJlLl26pH68fC2VYLa2/w6GOGGCrqMpdAtdF/Is+RmLfluklfqvP7rO4pOL6d+kP53e6qSVfZRE3g28OTP0DGbGZnTa2An/i1ln/ZRKh5J/e6tUMN7eqns7vv4a8jAxV0nS2LIxA5oOYOXZlUQ8iyjUul8OYljBqAJL3ZYWat2lgb2FPeeGncO1jisj9o9g1P5RpCpTdR2WVMhk4ijLFi6EDh1UgyH+M997aTH3nbkIBLOPzy7Uerde3cov4b+w0HUhlqbyfF92qphUYb/vfj5r9xnfXPwG102uxMTH6DosqRDJxFGWGRioxrMyM1MNhvii9IyAWrtybT5y/Ijvf/+eaw+vFUqdT5KeMPHwRFpZt2J4i+GFUmdppa+nz6LOi/jR50cu3r9IC/8WnI86r+uwpEIiE0dZV6OGaiTdmzdh2LBSNRji1A5TMTUyLbQBEKcdm8ajxEd84/lNqRjEsCj0adSHU0NPYaBnQIcNHdj0+yZdhyQVAvnXL8E776gGQ9y+HVZp4TLWjAxISFDduX7vHly/rhr65ORJOHIE9u5VJa/t2yG58O4DqFa+Gp+2/ZS9N/ZyKuLUa9V1Luoc31z4hrGtxuJQw+H1gxMCAgJU3YT795fKu/lfambVjAvDL9DWpi0fBHzAqP2juP/ivq7Dkl6Dxvk4pDLi5WCIkyZRccEC1T0eiYmqR1LSv881vc5uXX6SQZ06sGIFdOtWKM36uM3HrD6/ms+OfkbIoJAC3d2dnpHOyP0jqWFWg7md5r5+ULdvw9ixqpswDQ3h//4PbGxUR3xDh4K19evvo5ipVr4ahwcc5tMjn/LVma9Yf3k9fRv1ZUKbCYWTiKUipdXEERISgp+fHxkZGfTu3ZvhwzP3Cwsh8PPz48SJE5QrV45FixZhb28PqKaVrVChAnp6eujr67N7924Anj59yoQJE4iKisLa2pqvvvqKSpUqabMZZYOenmowxJYtsf7005y3UyigfPl/HyYm/z43MwNLy6zLX32e27q7d2HSJPDyAk9P+Oor1aXDr6GCUQVmOc9iVOAoAm8G0q1e/hPS2vNruRx9me29tlPRuGLBg0lOhkWLVA9DQ1i+HEaOVCWQb76BWbNg7lxV0hwxArp0AX39gu+vmDHQM2C5+3I+cvyIFWdX8N3l7/gh9AfeefMdJraZiGc9T9kFWFIILUlPTxeurq7i3r17IiUlRXh5eYmbN29m2ub48eNi6NChIiMjQ1y+fFn06tVLva5Tp07i8ePHWepdvHix+Pbbb4UQQnz77bfiiy++0BjLtWvXCtyO1ylbIj15IsK/+06I06eF+P13IW7eFCIyUoi4OCGSk4XIyNDu/lNThVi6VAhTUyGMjISYNk2IhITXqzI9VdittBP2a+xFujI9221y+pyjnkcJswVmwv0Hd5HxOm0PDBSiTh0hQIi+fYWIisq6za1bQnz2mRAWFqrtatcWYv58Ie7fL/h+c6Hrv+0nSU/EkpNLhM1yG8FshN1KO7H67GoRnxKvtX3qus26oI3vP62l99DQUGrXro2NjQ1GRkZ4enoSHBycaZvg4GC8vb1RKBQ0a9aM58+fExsbm2u9L8sAeHt7c/ToUW01oWyqXJnENm2gTRto0kT1i9/aGqpUAWNj1RGHNhkaqo46/vpLNQy8nx80bAi7dhX4xL2hviF+Ln78+fBPNoduzlfZCYcmkKpMZfW7qws2iOHff0OPHqojKEND1SCT27ZBzZpZt61bV3U0EhGhutqtbl2YPh3eeEN11dvhw6rzRaVE5XKVmdx2MrfH3eZHnx+pYlKFMQfHYPOlDVOOTiHyeaSuQ5RyoLWuqpiYGKysrNSvLS0tCQ0NzXUbKysrYmJisPhnvoihQ4eiUCjo06cPffr0AeDx48fq9RYWFsTFaR5WIiUlhbCwsAK1Izk5ucBlS6pi0+apUzHp0gUrPz/K9epFfNu2xEydSmqd/A8oaK+wp3HVxkw5MoWmBk0x1s88TEh2bT4ZfZLtf25nrP1Y0mLSCIvJx3uSmor5xo1U+/prUCh4NGECjz/4AIyMIC/vbePGsHo1huHhVNmxg0oBARjs3k2qjQ1Pe/XiaY8eKKtVy3s82Sg2nzPQRL8JG9pu4MrjK2z8ayNLTi1h2alluNu480G9D2hUtVGh7Kc4tbmoaKXNBT6G0eDAgQNi6tSp6td79uwRc+fOzbTNhx9+KM6fP69+PXDgQHH16lUhhBDR0dFCCCEePXokvLy8xLlz54QQQrRo0SJTHS1bttQYi+yqyp9i1+a0NCFWrRKiUiUhDAyE+OQTIZ4/z3c1wXeCBbMRy04ty7Luv21OTE0UdVfUFfVW1RPJacn53FGwEA0aqLqbevQQ4u+/8x1rFsnJQmzdKoSzs6peQ0MhevcW4uhRIZTKAlVZ7D7nV9yJuyMmBE0QZgvMBLMRHb7rIPaE7cmxqzGvinObtaVEdVVZWVkRHR2tfv3qkURO20RHR6u3eTkKr7m5OW5ubuqjFXNzc3V3VmxsLFWrVtVWE6TiwsAAxoxRdV8NHAhLlkCDBqoun3x0X7m85YJ7XXf8fvXjWfKzXLdd9Nsibj+5nb9BDO/fB19fcHWF1FQIDITdu1VdTa/L2FhV9/HjqiOWMWMgOBg6d4b69WHpUtXlzqXEW1XeYrn7ciInRrK8y3LuPbtHj596UH91fVadXUV8aryuQyzTtJY4GjduTHh4OBEREaSmphIYGIiLi0umbVxcXAgICEAIwZUrVzAzM8PCwoLExETi41V/GImJiZw8eRI7O7tMZQACAgJwdXXVVhOk4sbCAtavhzNnVDcu9usHnTrB1at5rmKh60LikuL44uQXOW7z1+O/WHRyEf0a98O1Th7+vtLSVFdI1a8Pe/aoro764w949908x5UvDRqo9hcVBT/8AFZWqsupra1V78mJE6XmRs6KxhWZ4DSBW+NusaP3DiwqWDAuaBy1ltfi0yOfFvpYZFIeFfgYJg+OHz8uunTpIlxdXcXatWuFEEJs3bpVbN26VQghREZGhpg9e7ZwdXUV3bp1E6GhoUIIIe7duye8vLyEl5eXePfdd9VlhRAiLi5ODBw4ULi5uYmBAweKJ0+eaIxDdlXlT4loc3q6EN9+K0TVqkLo6wsxfrwQefhbEEKIfrv6CZP5JiLq+b9XNr1sc0ZGhui8qbOotLCSePDigebKfv1ViMaNVd1HXbuqrozShT/+EGLcOCEqV1bF0qCBEMuXC5HNlYkvlYjPORunI06L93e8L/Tn6Av9Ofqi786+4mzk2TyVLaltfh3a+P7TauIoLmTiyJ8S1eZHj4QYNUoIhUJ1Gev332vs878dd1sYzjUUI/aNUC972eatoVsFsxGrz67Ofb8xMUJ88IHqS9rGRojdu7V/qXJeJCSo3gMnJ1VsxsZC9O+vSnD/ia9Efc7Z+Pvp32Lyocmi4sKKgtmIduvbiZ1/7sz1PEhJb3NBlKhzHJJUJMzNYe1a1dwiderAoEHQvr1qSJMc1KlSh5EtR7Lu0jpuPLqhXv4s+RkTD0+kZc2WjGw5MvvCSqVqf/Xrw9atMGWK6pxDjx7av1Q5L8qXhw8+UI0CEBqquhv9559VoyA3agQrV8KTJ7qOslC8UekNlnRZQuSESL5y/4r7L+7Ta0cv7FbZ8dWZr3ie8lzXIZZaMnFIpUPz5qqxrzZsUA3p0bIljB4NOVyuPb3jdEwMTZj+y/R/lx2bTmxCLN94foO+XjZ3bJ87B61bw0cfqfYXGqoamr5CBW216vX8c0kv9++rzg2ZmsL48ap7SAYNotKuXaq71q9cgdjYEnuPiJmxGePbjOfm2Jvsfn831hWtmXBoAjZf2jDp0CTCn4brOsTSp8DHMCWI7KrKnxLf5idPVOc89PWFMDdXnQtJz9p9MfuX2YLZiLORZ8X237YLvTl6YkzgmKz1PXokxPDhqu6wGjWE2LateHRLFcTly0KMHKm6M191Cv3fh4GBENbWQjg6CvHee0KMGCHEnDlC+PsLsW+fEBcvqu5iz+a9LG7ORZ4Tvjt9hf4cfaE3R0/03t5bnLx3UlwMvfh6IwCUQNr4/lMIUUouv8hFWFgYDRs2LPKyJVWpaXNoqGowwZAQ1RHI6tWqI4Z/vEh5Qd2VdXm7+ts8fv6YR2mPuP7RdSqV+2fss4wM1RHMZ5/B06cwbhzMng0VX2O8quIiNZVbISHYVqigOiJ58ED1ePn85b/ZXeKrp6cak6xGDdWjZs3sn1taqu6W16GIZxGsPrca/0v+PE1+CoC+Qh8zYzPMjMyoaFwRM+N//jUyUz03ymbZP89fbv/yeXnD8gUbUaAIaeP7T46OK5VeTZqo7nvYtk01TW6bNjBkiGpYj+rVMTM2Y6bzTMYeHAvANp9t/yaNK1dUXV2nT0O7dqrzGk2a6Kwphc7IiDRra9VwLrlJTYXo6JwTS1QUnD8PDx9mvQRYoYDq1bMmFBsbsLODevVUlxBr8YvXppINi90WM8N5BnvC9nDl9hVMKpvwIuUFz1Ofq/5Nec7zlOdEPo9Uv36R+oIMobnrTk+hp04u2Saif5aZGplSwagC5Q3LU8FQ9W95w/JZlr18baRvpLX3pDDIxCGVbgqF6t4GLy+YNw++/FJ1U968eTByJMNbDOfrC19T3aA6fez7wLNnMGMGrFmjOvG+YYPqpkO9Mno60MhIdQOjppsY09JU50n+m1hefX75ctZzKeXL/5tE/vsoxJt7TY1MGdB0AC2NWubp17cQgqT0JFUSeSWZvHz96vNM6/7598GLB5m2UYr8zbdioGfwb3L5T1JRv85DAqphWoNylCvo25ZzfIVeYyny8CHs3FkZyzI2tfSDB5WpUaPo92tgAPb2qh/2JiaFXLmZGXzxheqIY+xY1WPdOoxWr+bi8IvcvnELxZYtqiOT2FgYNQrmz1cN7ihpZmioOnrQNJdIerrqKOXmTdVIAC//vXxZldBfndDK3Dz7hGJrq0o4WqRQKNRfylamVpoL5EIIQXJ6MolpiSSmJZKQlvDv89SEvC3753lCagJPkp5k2T5FmZLj/vd77Kchhdv1LBNHLrZvh5kzdfANqnO6bbO+vqoHpXlz1aNFC2jaVPXd/9oaNFCNMrt7N0yYAB06UO5//+PNv/5Sdbk4Oqpm5GvZshB2JmVhYAC1a6senTtnXpeaqpqT5a+/Mj+OHIGNGzNva2OTfVJ5803VPooRhUKBiaEJJoYmmGOulX0oM5TqRPJqojHWN8boSeF3exWvd7iYGT0aGjf+C1vberoOpUjdvHlTPcRLUUpKUp3PvnRJ9Th0CDb9M0W1QqH6Xng1mTg4QOXKBdiRQqEaptzDQ3U57ZIllCtXTjWZ0rBhpWrypBLFyEh1f0z9+lnXxcf/e3Ty8nHzpur81dOn/25nYKC6nye7pJLdUPalhL7ePyf8jbP+ugp7UvijAcvEkQvVuT1laf57y9azZ+k6a3Pduqp76V568AAuXvw3mfz2m+q74qU6df5NJi8f1avncWcVKqi6oz7+mFt371Lf0bFQ2yIVIlNT1S8Fh/9MMysEPH6c9Sjlr79Uc5+8Om2xiQl2JiaqBGVgoPqB8PLfV5/nZV1BttfXV50r09MruudVq2rlyjaZOKRirUYN1Uyqr05B/vDhv4nk5WPnzn/X29hkTiQtWpD7OZtq1ch4+FBrbZC0SKGAatVUj7ZtM6/LyIDIyH8Tya1bvLh/nyoVK6rOtSiVqsfL5zktS0tTHQ7ndfuc1unoBkuj/fs1Xz2XTzJxSCVO9erg7q56vPTkieoK2lePTn7++d8rRK2ssh6ZvPFG8RglRNISPb1/rwj753xKdFgYVXR5j5IQ/yaRjAztP69YkdRCv9JEJg6plKhSRTXCeqdO/y578QJ+/z1zMgkK+veHn7n5v0lET69Kmbt6Liam7LX54cMq2Nioem9ePoyMMr8uyCPPV2srFDmevBdCdXDz8iDn5b9pQLqAtAxIV/6zLLvtsnluagp2dvIchyTlmZmZarzD9u3/XZaYqJq+49Vksnw5pKW93iWXJZNsc2HR1889sSgUuX/Rv+zNKmwGBvDzz4Y0KpyZd/+tt3Crk6TirXx51agjr4w8QmoqXLp0g/rZXc1Tit24UbbaLASEhf3FW2/VU39xp6WpPv9XXxfkoakOIVQJxMAg87/aXlapEjx6lFbo76VMHFKZZ2QElSpllLl7/cpim6tWLXtXSWpjRmGtjqMQEhKCu7s7bm5u+Pv7Z1kvhGD+/Pm4ubnh5eXFn3/+CcCDBw8YMGAAXbt2xdPTk42v3PyzatUqOnToQPfu3enevTsnTpzQZhMkSZKk/9DaEYdSqWTu3Lls2LABS0tLevXqhYuLC7a2tuptQkJCCA8P5/Dhw/z+++/Mnj2bHTt2oK+vz5QpU7C3tyc+Ph4fHx/atWunLjto0CCGDh2qrdAlSZKkXGjtiCM0NJTatWtjY2ODkZERnp6eBAcHZ9omODgYb29vFAoFzZo14/nz58TGxmJhYYG9vT0Apqam1KlTh5iYGG2FKkmSJOWD1o44YmJisLL69woGS0tLQkNDc93GysqKmJgYLCws1MsiIyMJCwujadOm6mVbtmwhICCARo0aMWXKFCpVqpRrLCkpKYSFFeyStOTk5AKXLalkm8sG2eayQRtt1lriyG5+qP9OeKJpm4SEBMaNG8fUqVMxNTUFwNfXl9GjR6NQKFixYgWLFi1i4cKFucZibGwsJ3LKB9nmskG2uWx43YmcsqO1riorKyuio6PVr/97JJHdNtHR0ept0tLSGDduHF5eXnTp0kW9TbVq1dDX10dPT4/evXtz9epVbTVBkiRJyobWEkfjxo0JDw8nIiKC1NRUAgMDcXFxybSNi4sLAQEBCCG4cuUKZmZmWFhYIIRg2rRp1KlTh8GDB2cqExsbq35+9OhRnYziKkmSVJZpravKwMCAmTNnMmzYMJRKJT4+PtjZ2bHtn6FNfX19cXZ25sSJE7i5uWFiYsKCBQsAuHjxInv37qVevXp0794dgIkTJ+Ls7MySJUu4fv06ANbW1sydO1dbTZAkSZKyoRDZnWgoZa5cuYKxsbGuw5AkSSpRUlJSaNasWZblZSJxSJIkSYVHq3eOS5IkSaWPTBySJElSvsjEIUmSJOWLTBySJElSvsjEIUmSJOWLTBySJElSvsjEkQtN84mUNrnNg1KaKZVKvL29GTFihK5DKRLPnz9n3LhxeHh40LVrVy5fvqzrkLTu+++/x9PTk27dujFx4kRSUlJ0HVKh+/zzz3FycqJbt27qZU+fPmXw4MF06dKFwYMH8+zZs0LZl0wcOXg5n8i6desIDAxk//793Lp1S9dhadXLeVAOHjzITz/9xNatW0t9mwE2bdpE3bp1dR1GkfHz86NDhw4EBQWxd+/eUt/2mJgYNm3axK5du9i/fz9KpZLAwEBdh1Xoevbsybp16zIt8/f3x8nJicOHD+Pk5FRoP4Bl4shBXuYTKW3K4jwo0dHRHD9+nF69euk6lCIRHx/P+fPn1e01MjKiYsWKOo5K+5RKJcnJyaSnp5OcnJxlwNXSwNHRMcsUEy/nPALw9vbm6NGjhbIvmThykN18IqX9S/RV2c2DUhotWLCATz75BD29svFfISIigqpVq/L555/j7e3NtGnTSExM1HVYWmVpacmQIUPo1KkT7du3x9TUlPbt2+s6rCLx+PFjdZK0sLAgLi6uUOotG/9bCiAv84mUVtnNg1Ia/fLLL1StWpVGjRrpOpQik56ezrVr1/D19SUgIAATE5NSf/7u2bNnBAcHExwczK+//kpSUhJ79+7VdVglmkwcOcjLfCKlUU7zoJRGly5d4tixY7i4uDBx4kTOnDnD5MmTdR2WVllZWWFlZaU+kvTw8ODatWs6jkq7Tp06Ra1atahatSqGhoZ06dKlTFwQAGBubq6eiiI2NpaqVasWSr0yceQgL/OJlDa5zYNSGk2aNImQkBCOHTvG8uXLadOmDUuXLtV1WFpVvXp1rKysuHPnDgCnT58u9SfHa9asye+//05SUhJCiDLR5pdeznkEEBAQgKura6HUq7X5OEq6nOYTKc1ymwdFKj1mzJjB5MmTSUtLw8bGRuPUyyVd06ZNcXd3p0ePHhgYGNCwYUP69Omj67AK3cSJEzl37hxPnjyhY8eOjB07luHDh/Pxxx+zc+dOatSowYoVKwplX3JYdUmSJClfZFeVJEmSlC8ycUiSJEn5IhOHJEmSlC8ycUiSJEn5IhOHJEmSlC/yclxJysajR49YuHAhV65coVKlShgaGjJs2DDc3NyKPJazZ89iaGhI8+bNAdi2bRsmJibqMYgkqajJxCFJ/yGE4KOPPsLb25tly5YBEBUVxbFjx7S2z/T0dAwMsv/veO7cOcqXL69OHL6+vlqLQ5LyQt7HIUn/cfr0adasWcPmzZuzrFMqlSxdupRz586RmprK//73P/r27cvZs2dZvXo1VapU4a+//sLe3p6lS5eiUCj4448/WLRoEYmJiVSpUoWFCxdiYWHBgAEDcHBw4NKlS7i4uPDmm2/y9ddfk5aWRuXKlVm6dCnJycn06dMHPT09qlatyowZMzh9+jTly5dn6NChhIWFMWvWLJKSknjjjTdYsGABlSpVYsCAATRp0oSzZ8/y4sUL/Pz8aNmypQ7eTak0kuc4JOk/bt68ydtvv53tup07d2JmZsauXbvYtWsX27dvJyIiAoBr164xdepUDhw4QGRkJBcvXiQtLY358+ezcuVKdu/ejY+PD19++aW6vufPn7N582aGDBlCixYt2L59OwEBAXh6erJu3Tpq1apF3759GTRoEHv37s3y5f/pp58yefJk9u3bR7169Vi9erV6nVKpZOfOnUydOjXTckl6XbKrSpI0mDNnDhcvXsTQ0BBra2tu3LjBoUOHAHjx4gV///03hoaGNGnSRD0Uf4MGDYiKiqJixYr89ddf6rG/MjIyqF69urrud999V/08OjqaCRMm8PDhQ1JTU6lVq1aucb148YIXL17QqlUrAHr06MH48ePV61+ej7G3tycqKqoQ3glJUpGJQ5L+w87OjsOHD6tfz5o1i7i4OHr16kXNmjWZPn06HTp0yFTm7NmzGBkZqV/r6+ujVCoRQmBnZ8dPP/2U7b5MTEzUz+fPn8+gQYNwdXVVd329jpfx6OnpoVQqX6suSXqV7KqSpP9o06YNKSkpbN26Vb0sOTkZgPbt27Nt2zbS0tIAuHv3bq4TIb311lvExcWph/FOS0vj5s2b2W774sULLC0tAdQjmgJUqFCBhISELNubmZlRsWJFLly4AMDevXtxdHTMR0slqWDkEYck/YdCoWDNmjUsXLiQdevWUbVqVUxMTJg8eTIeHh5ERUXRs2dPhBBUqVKFtWvX5liXkZERK1euZP78+bx48QKlUskHH3yQ7UjLY8aMYfz48VhaWtK0aVMiIyMB6NSpE+PGjSM4OJgZM2ZkKrN48WL1yfGyMNKtVDzIq6okSZKkfJFdVZIkSVK+yMQhSZIk5YtMHJIkSVK+yMQhSZIk5YtMHJIkSVK+yMQhSZIk5YtMHJIkSVK+/D8hkdaj71G0QAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 45.66154041687648 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 1 , Number of neurons: 50\n",
      "Batch size 4 , Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.029963</td>\n",
       "      <td>0.029963</td>\n",
       "      <td>36.807385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030514</td>\n",
       "      <td>0.030514</td>\n",
       "      <td>32.301245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030569</td>\n",
       "      <td>0.030569</td>\n",
       "      <td>66.451287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>86.387059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031686</td>\n",
       "      <td>0.031686</td>\n",
       "      <td>33.835428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031721</td>\n",
       "      <td>0.031721</td>\n",
       "      <td>42.301822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032678</td>\n",
       "      <td>0.032678</td>\n",
       "      <td>104.080725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035165</td>\n",
       "      <td>0.035165</td>\n",
       "      <td>68.486476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035576</td>\n",
       "      <td>0.035576</td>\n",
       "      <td>61.230749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035758</td>\n",
       "      <td>0.035758</td>\n",
       "      <td>70.076653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035964</td>\n",
       "      <td>0.035964</td>\n",
       "      <td>64.599619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036378</td>\n",
       "      <td>0.036378</td>\n",
       "      <td>83.390077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036535</td>\n",
       "      <td>0.036535</td>\n",
       "      <td>83.436033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036612</td>\n",
       "      <td>0.036612</td>\n",
       "      <td>83.068761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036826</td>\n",
       "      <td>0.036826</td>\n",
       "      <td>36.398877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036994</td>\n",
       "      <td>0.036994</td>\n",
       "      <td>52.598531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037456</td>\n",
       "      <td>0.037456</td>\n",
       "      <td>56.866235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.037467</td>\n",
       "      <td>0.037467</td>\n",
       "      <td>13.513292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.037483</td>\n",
       "      <td>0.037483</td>\n",
       "      <td>15.024228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038587</td>\n",
       "      <td>0.038587</td>\n",
       "      <td>64.609475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.038722</td>\n",
       "      <td>0.038722</td>\n",
       "      <td>21.340798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.039088</td>\n",
       "      <td>0.039088</td>\n",
       "      <td>143.353655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.040338</td>\n",
       "      <td>0.040338</td>\n",
       "      <td>38.194977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.042238</td>\n",
       "      <td>0.042238</td>\n",
       "      <td>47.411570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.049958</td>\n",
       "      <td>0.049958</td>\n",
       "      <td>26.058431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.051922</td>\n",
       "      <td>0.051922</td>\n",
       "      <td>41.354287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.053998</td>\n",
       "      <td>0.053998</td>\n",
       "      <td>142.931085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.056795</td>\n",
       "      <td>0.056795</td>\n",
       "      <td>42.010567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.060742</td>\n",
       "      <td>0.060742</td>\n",
       "      <td>78.961207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.067912</td>\n",
       "      <td>0.067912</td>\n",
       "      <td>203.424181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.078842</td>\n",
       "      <td>0.078842</td>\n",
       "      <td>126.644751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.079812</td>\n",
       "      <td>0.079812</td>\n",
       "      <td>203.417532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.088741</td>\n",
       "      <td>0.088741</td>\n",
       "      <td>53.726791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.095968</td>\n",
       "      <td>0.095968</td>\n",
       "      <td>204.613830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.101070</td>\n",
       "      <td>0.101070</td>\n",
       "      <td>126.016402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.189320</td>\n",
       "      <td>0.189320</td>\n",
       "      <td>21.339504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.230126</td>\n",
       "      <td>0.230126</td>\n",
       "      <td>63.236682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             1        200         0.0010           4  0.029963  0.029963   \n",
       "1             1        200         0.0010           4  0.030514  0.030514   \n",
       "2             1        200         0.0010           4  0.030569  0.030569   \n",
       "3             2        150         0.0001           2  0.030900  0.030900   \n",
       "4             1        200         0.0010           4  0.031686  0.031686   \n",
       "5             1        200         0.0010           4  0.031721  0.031721   \n",
       "6             2        100         0.0001           2  0.032678  0.032678   \n",
       "7             2        200         0.0001           4  0.035165  0.035165   \n",
       "8             2        200         0.0001           4  0.035576  0.035576   \n",
       "9             2        200         0.0001           4  0.035758  0.035758   \n",
       "10            2        100         0.0001           4  0.035964  0.035964   \n",
       "11            2        100         0.0001           4  0.036378  0.036378   \n",
       "12            2        100         0.0001           4  0.036535  0.036535   \n",
       "13            2        100         0.0001           4  0.036612  0.036612   \n",
       "14            1        200         0.0010           4  0.036826  0.036826   \n",
       "15            2        200         0.0001           4  0.036994  0.036994   \n",
       "16            1        200         0.0010           4  0.037456  0.037456   \n",
       "17            1        150         0.0010          16  0.037467  0.037467   \n",
       "18            1        200         0.0010          16  0.037483  0.037483   \n",
       "19            2        100         0.0001           4  0.038587  0.038587   \n",
       "20            4        200         0.0001          16  0.038722  0.038722   \n",
       "21            2        100         0.0001           2  0.039088  0.039088   \n",
       "22            1        200         0.0010           4  0.040338  0.040338   \n",
       "23            3        200         0.0010           8  0.042238  0.042238   \n",
       "24            2        200         0.0001          16  0.049958  0.049958   \n",
       "25            4        200         0.0010          16  0.051922  0.051922   \n",
       "26            1        100         0.0001           2  0.053998  0.053998   \n",
       "27            2        200         0.0001          16  0.056795  0.056795   \n",
       "28            2        100         0.0010           4  0.060742  0.060742   \n",
       "29            3        200         0.0010           2  0.067912  0.067912   \n",
       "30            2        200         0.0010           2  0.078842  0.078842   \n",
       "31            3        200         0.0010           2  0.079812  0.079812   \n",
       "32            1        100         0.0001           4  0.088741  0.088741   \n",
       "33            4        200         0.0010           2  0.095968  0.095968   \n",
       "34            3        100         0.0010           2  0.101070  0.101070   \n",
       "35            1        200         0.0001          16  0.189320  0.189320   \n",
       "36            1        100         0.0001           4  0.230126  0.230126   \n",
       "\n",
       "    Elapsed time  \n",
       "0      36.807385  \n",
       "1      32.301245  \n",
       "2      66.451287  \n",
       "3      86.387059  \n",
       "4      33.835428  \n",
       "5      42.301822  \n",
       "6     104.080725  \n",
       "7      68.486476  \n",
       "8      61.230749  \n",
       "9      70.076653  \n",
       "10     64.599619  \n",
       "11     83.390077  \n",
       "12     83.436033  \n",
       "13     83.068761  \n",
       "14     36.398877  \n",
       "15     52.598531  \n",
       "16     56.866235  \n",
       "17     13.513292  \n",
       "18     15.024228  \n",
       "19     64.609475  \n",
       "20     21.340798  \n",
       "21    143.353655  \n",
       "22     38.194977  \n",
       "23     47.411570  \n",
       "24     26.058431  \n",
       "25     41.354287  \n",
       "26    142.931085  \n",
       "27     42.010567  \n",
       "28     78.961207  \n",
       "29    203.424181  \n",
       "30    126.644751  \n",
       "31    203.417532  \n",
       "32     53.726791  \n",
       "33    204.613830  \n",
       "34    126.016402  \n",
       "35     21.339504  \n",
       "36     63.236682  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla1.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 45.658 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
