{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 18:55:03.130657: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-16 18:55:03.294038: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:55:03.294068: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-16 18:55:04.438864: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:55:04.438984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:55:04.438997: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,5e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.5         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 18:55:05.736151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-16 18:55:05.736471: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:55:05.736566: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:55:05.736643: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:55:05.736717: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:55:05.736796: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:55:05.736869: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:55:05.736938: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:55:05.737009: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:55:05.737018: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-16 18:55:05.737698: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0964 - mean_squared_error: 0.0964\n",
      "Loss: 0.09644750505685806 , Elapsed time: 117.52193140983582\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0560 - mean_squared_error: 0.0560\n",
      "Loss: 0.05601862818002701 , Elapsed time: 25.95320200920105\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0493 - mean_squared_error: 0.0493\n",
      "Loss: 0.04933018237352371 , Elapsed time: 38.31600308418274\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0648 - mean_squared_error: 0.0648\n",
      "Loss: 0.06479229032993317 , Elapsed time: 57.23782253265381\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Loss: 0.036969006061553955 , Elapsed time: 53.84309411048889\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin     \tavg      \tmax      \n",
      "0  \t5     \t0.036969\t0.0607115\t0.0964475\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0833 - mean_squared_error: 0.0833\n",
      "Loss: 0.08332592993974686 , Elapsed time: 20.669532775878906\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Loss: 0.03662353381514549 , Elapsed time: 68.34205365180969\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0606 - mean_squared_error: 0.0606\n",
      "Loss: 0.06055491045117378 , Elapsed time: 23.791932582855225\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Loss: 0.03576063737273216 , Elapsed time: 66.79233384132385\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.0357606\t0.0506468\t0.0833259\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Loss: 0.03624458611011505 , Elapsed time: 60.02493953704834\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0352 - mean_squared_error: 0.0352\n",
      "Loss: 0.03515426069498062 , Elapsed time: 55.283509731292725\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0739 - mean_squared_error: 0.0739\n",
      "Loss: 0.0738597959280014 , Elapsed time: 52.929720878601074\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "Loss: 0.03706561028957367 , Elapsed time: 20.256110429763794\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t4     \t0.0351543\t0.0437896\t0.0738598\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Loss: 0.03604631498456001 , Elapsed time: 69.88966846466064\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Loss: 0.037826430052518845 , Elapsed time: 77.45789861679077\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Loss: 0.036832988262176514 , Elapsed time: 24.807308435440063\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0391 - mean_squared_error: 0.0391\n",
      "Loss: 0.039058152586221695 , Elapsed time: 22.696759462356567\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t4     \t0.0360463\t0.0372775\t0.0390582\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0458 - mean_squared_error: 0.0458\n",
      "Loss: 0.04583952948451042 , Elapsed time: 106.03705525398254\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Loss: 0.03531928360462189 , Elapsed time: 63.781370401382446\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t2     \t0.0353193\t0.037975 \t0.0458395\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0504 - mean_squared_error: 0.0504\n",
      "Loss: 0.05035410821437836 , Elapsed time: 25.781704425811768\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Loss: 0.035974759608507156 , Elapsed time: 83.17837262153625\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Loss: 0.03397902846336365 , Elapsed time: 76.56500244140625\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Loss: 0.03655868023633957 , Elapsed time: 64.74742245674133\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.033979 \t0.038698 \t0.0503541\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0511 - mean_squared_error: 0.0511\n",
      "Loss: 0.05109205096960068 , Elapsed time: 52.65855693817139\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Loss: 0.03574330359697342 , Elapsed time: 67.44621562957764\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Loss: 0.03224414214491844 , Elapsed time: 79.6523540019989\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Loss: 0.03513967990875244 , Elapsed time: 66.11411166191101\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t4     \t0.0322441\t0.0376396\t0.0510921\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Loss: 0.032837990671396255 , Elapsed time: 69.79109334945679\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Loss: 0.036995697766542435 , Elapsed time: 60.95638847351074\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Loss: 0.032518189400434494 , Elapsed time: 70.00159549713135\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Loss: 0.03418194130063057 , Elapsed time: 78.34930062294006\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.0325182\t0.0341026\t0.0369957\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Loss: 0.033283837139606476 , Elapsed time: 77.53374767303467\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Loss: 0.034796103835105896 , Elapsed time: 84.06654787063599\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0441 - mean_squared_error: 0.0441\n",
      "Loss: 0.04411734268069267 , Elapsed time: 76.1752998828888\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t3     \t0.0325182\t0.0355107\t0.0441173\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0472 - mean_squared_error: 0.0472\n",
      "Loss: 0.04717651382088661 , Elapsed time: 71.86420750617981\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Loss: 0.03725827857851982 , Elapsed time: 83.28590393066406\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t2     \t0.0325182\t0.0364618\t0.0471765\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Loss: 0.03736226633191109 , Elapsed time: 83.25333881378174\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Loss: 0.03420207276940346 , Elapsed time: 76.71583795547485\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Loss: 0.033401887863874435 , Elapsed time: 76.8630063533783\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t3     \t0.0325182\t0.0340645\t0.0373623\n",
      "-- Best Individual =  [1, 1, 1, 0, 0, 0, 1]\n",
      "-- Best Fitness =  0.032518189400434494\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 40.84819445212682 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 2 , Number of neurons: 100\n",
      "Batch size 2 , Learning rate: 0.005\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032244</td>\n",
       "      <td>0.032244</td>\n",
       "      <td>79.652354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032518</td>\n",
       "      <td>0.032518</td>\n",
       "      <td>70.001595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032838</td>\n",
       "      <td>0.032838</td>\n",
       "      <td>69.791093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033284</td>\n",
       "      <td>0.033284</td>\n",
       "      <td>77.533748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033402</td>\n",
       "      <td>0.033402</td>\n",
       "      <td>76.863006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033979</td>\n",
       "      <td>0.033979</td>\n",
       "      <td>76.565002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034182</td>\n",
       "      <td>0.034182</td>\n",
       "      <td>78.349301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034202</td>\n",
       "      <td>0.034202</td>\n",
       "      <td>76.715838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034796</td>\n",
       "      <td>0.034796</td>\n",
       "      <td>84.066548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035140</td>\n",
       "      <td>0.035140</td>\n",
       "      <td>66.114112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035154</td>\n",
       "      <td>0.035154</td>\n",
       "      <td>55.283510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035319</td>\n",
       "      <td>0.035319</td>\n",
       "      <td>63.781370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035743</td>\n",
       "      <td>0.035743</td>\n",
       "      <td>67.446216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035761</td>\n",
       "      <td>0.035761</td>\n",
       "      <td>66.792334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035975</td>\n",
       "      <td>0.035975</td>\n",
       "      <td>83.178373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036046</td>\n",
       "      <td>0.036046</td>\n",
       "      <td>69.889668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036245</td>\n",
       "      <td>0.036245</td>\n",
       "      <td>60.024940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036559</td>\n",
       "      <td>0.036559</td>\n",
       "      <td>64.747422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036624</td>\n",
       "      <td>0.036624</td>\n",
       "      <td>68.342054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.036833</td>\n",
       "      <td>0.036833</td>\n",
       "      <td>24.807308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036969</td>\n",
       "      <td>0.036969</td>\n",
       "      <td>53.843094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036996</td>\n",
       "      <td>0.036996</td>\n",
       "      <td>60.956388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.037066</td>\n",
       "      <td>0.037066</td>\n",
       "      <td>20.256110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037258</td>\n",
       "      <td>0.037258</td>\n",
       "      <td>83.285904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037362</td>\n",
       "      <td>0.037362</td>\n",
       "      <td>83.253339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037826</td>\n",
       "      <td>0.037826</td>\n",
       "      <td>77.457899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.039058</td>\n",
       "      <td>0.039058</td>\n",
       "      <td>22.696759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.044117</td>\n",
       "      <td>0.044117</td>\n",
       "      <td>76.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.045840</td>\n",
       "      <td>0.045840</td>\n",
       "      <td>106.037055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.047177</td>\n",
       "      <td>0.047177</td>\n",
       "      <td>71.864208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.049330</td>\n",
       "      <td>0.049330</td>\n",
       "      <td>38.316003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.050354</td>\n",
       "      <td>0.050354</td>\n",
       "      <td>25.781704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.051092</td>\n",
       "      <td>0.051092</td>\n",
       "      <td>52.658557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.056019</td>\n",
       "      <td>0.056019</td>\n",
       "      <td>25.953202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.060555</td>\n",
       "      <td>0.060555</td>\n",
       "      <td>23.791933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.064792</td>\n",
       "      <td>0.064792</td>\n",
       "      <td>57.237823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.073860</td>\n",
       "      <td>0.073860</td>\n",
       "      <td>52.929721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.083326</td>\n",
       "      <td>0.083326</td>\n",
       "      <td>20.669533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.096448</td>\n",
       "      <td>0.096448</td>\n",
       "      <td>117.521931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             4        150         0.0001           4  0.032244  0.032244   \n",
       "1             4        150         0.0001           4  0.032518  0.032518   \n",
       "2             3        200         0.0001           4  0.032838  0.032838   \n",
       "3             4        150         0.0001           4  0.033284  0.033284   \n",
       "4             3        200         0.0001           4  0.033402  0.033402   \n",
       "5             4        150         0.0001           4  0.033979  0.033979   \n",
       "6             4        150         0.0001           4  0.034182  0.034182   \n",
       "7             3        200         0.0001           4  0.034202  0.034202   \n",
       "8             4        150         0.0001           4  0.034796  0.034796   \n",
       "9             2        200         0.0001           4  0.035140  0.035140   \n",
       "10            2        200         0.0001           4  0.035154  0.035154   \n",
       "11            2        200         0.0001           4  0.035319  0.035319   \n",
       "12            2        200         0.0001           4  0.035743  0.035743   \n",
       "13            2        200         0.0001           4  0.035761  0.035761   \n",
       "14            2        200         0.0001           4  0.035975  0.035975   \n",
       "15            2        200         0.0001           4  0.036046  0.036046   \n",
       "16            2        200         0.0001           4  0.036245  0.036245   \n",
       "17            2        200         0.0001           4  0.036559  0.036559   \n",
       "18            2        200         0.0001           4  0.036624  0.036624   \n",
       "19            4        100         0.0001          16  0.036833  0.036833   \n",
       "20            2        100         0.0001           4  0.036969  0.036969   \n",
       "21            2        150         0.0001           4  0.036996  0.036996   \n",
       "22            4        100         0.0001          16  0.037066  0.037066   \n",
       "23            2        150         0.0001           4  0.037258  0.037258   \n",
       "24            2        150         0.0001           4  0.037362  0.037362   \n",
       "25            2        200         0.0001           4  0.037826  0.037826   \n",
       "26            4        100         0.0001          16  0.039058  0.039058   \n",
       "27            3        200         0.0050           4  0.044117  0.044117   \n",
       "28            2         50         0.0001           2  0.045840  0.045840   \n",
       "29            3        200         0.0050           4  0.047177  0.047177   \n",
       "30            4        200         0.0050          16  0.049330  0.049330   \n",
       "31            2        200         0.0001          16  0.050354  0.050354   \n",
       "32            1        200         0.0001           4  0.051092  0.051092   \n",
       "33            2        200         0.0001          16  0.056019  0.056019   \n",
       "34            4        100         0.0050          16  0.060555  0.060555   \n",
       "35            1        100         0.0001           4  0.064792  0.064792   \n",
       "36            2        200         0.0050           4  0.073860  0.073860   \n",
       "37            2        100         0.0001          16  0.083326  0.083326   \n",
       "38            3        100         0.0050           2  0.096448  0.096448   \n",
       "\n",
       "    Elapsed time  \n",
       "0      79.652354  \n",
       "1      70.001595  \n",
       "2      69.791093  \n",
       "3      77.533748  \n",
       "4      76.863006  \n",
       "5      76.565002  \n",
       "6      78.349301  \n",
       "7      76.715838  \n",
       "8      84.066548  \n",
       "9      66.114112  \n",
       "10     55.283510  \n",
       "11     63.781370  \n",
       "12     67.446216  \n",
       "13     66.792334  \n",
       "14     83.178373  \n",
       "15     69.889668  \n",
       "16     60.024940  \n",
       "17     64.747422  \n",
       "18     68.342054  \n",
       "19     24.807308  \n",
       "20     53.843094  \n",
       "21     60.956388  \n",
       "22     20.256110  \n",
       "23     83.285904  \n",
       "24     83.253339  \n",
       "25     77.457899  \n",
       "26     22.696759  \n",
       "27     76.175300  \n",
       "28    106.037055  \n",
       "29     71.864208  \n",
       "30     38.316003  \n",
       "31     25.781704  \n",
       "32     52.658557  \n",
       "33     25.953202  \n",
       "34     23.791933  \n",
       "35     57.237823  \n",
       "36     52.929721  \n",
       "37     20.669533  \n",
       "38    117.521931  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla1.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 40.844 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
