{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 17:43:57.597380: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-18 17:43:57.762999: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:43:57.763024: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-18 17:43:58.908720: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:43:58.908862: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:43:58.908875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,1e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.2         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 17:44:00.232051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 17:44:00.232733: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:44:00.232821: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:44:00.232873: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:44:00.232944: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:44:00.233021: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:44:00.233092: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:44:00.234163: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:44:00.234333: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-18 17:44:00.234351: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-18 17:44:00.235223: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0907 - mean_squared_error: 0.0907\n",
      "Loss: 0.09068860858678818 , Elapsed time: 131.7903482913971\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0546 - mean_squared_error: 0.0546\n",
      "Loss: 0.05459306761622429 , Elapsed time: 27.837312698364258\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0703 - mean_squared_error: 0.0703\n",
      "Loss: 0.07029956579208374 , Elapsed time: 42.73398041725159\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0700 - mean_squared_error: 0.0700\n",
      "Loss: 0.06999687105417252 , Elapsed time: 55.836687088012695\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Loss: 0.03814941644668579 , Elapsed time: 57.46640586853027\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax      \n",
      "0  \t5     \t0.0381494\t0.0647455\t0.0906886\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Loss: 0.031241219490766525 , Elapsed time: 116.280592918396\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0528 - mean_squared_error: 0.0528\n",
      "Loss: 0.05283268168568611 , Elapsed time: 42.17478704452515\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Loss: 0.03866754472255707 , Elapsed time: 35.096771478652954\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0572 - mean_squared_error: 0.0572\n",
      "Loss: 0.057215671986341476 , Elapsed time: 24.8199520111084\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.0312412\t0.0436213\t0.0572157\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Loss: 0.03649171069264412 , Elapsed time: 65.96852493286133\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Loss: 0.0463419146835804 , Elapsed time: 41.70397138595581\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Loss: 0.03732448071241379 , Elapsed time: 62.9994912147522\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Loss: 0.030320338904857635 , Elapsed time: 74.93382954597473\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t4     \t0.0303203\t0.0363439\t0.0463419\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Loss: 0.03775429725646973 , Elapsed time: 83.38612198829651\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Loss: 0.037253428250551224 , Elapsed time: 83.21887183189392\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Loss: 0.03210141882300377 , Elapsed time: 83.47017288208008\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t3     \t0.0303203\t0.03355  \t0.0377543\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 1s 4ms/step - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Loss: 0.034759771078825 , Elapsed time: 144.22929430007935\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0394 - mean_squared_error: 0.0394\n",
      "Loss: 0.039376452565193176 , Elapsed time: 35.11963152885437\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0425 - mean_squared_error: 0.0425\n",
      "Loss: 0.04250708967447281 , Elapsed time: 83.98227334022522\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t3     \t0.0303203\t0.0354568\t0.0425071\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Loss: 0.03364616632461548 , Elapsed time: 67.11720728874207\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Loss: 0.03109033778309822 , Elapsed time: 76.58006024360657\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t2     \t0.0303203\t0.0311395\t0.0336462\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Loss: 0.035379327833652496 , Elapsed time: 143.4980719089508\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t1     \t0.0303203\t0.0316401\t0.0353793\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Loss: 0.030001895502209663 , Elapsed time: 83.45300149917603\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Loss: 0.03054686449468136 , Elapsed time: 74.43880152702332\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t2     \t0.0300019\t0.03061  \t0.0310903\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Loss: 0.030829712748527527 , Elapsed time: 80.55383276939392\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Loss: 0.03093518130481243 , Elapsed time: 78.71807479858398\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Loss: 0.03174439072608948 , Elapsed time: 75.1433777809143\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0434 - mean_squared_error: 0.0434\n",
      "Loss: 0.04336593672633171 , Elapsed time: 83.51912879943848\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t4     \t0.0303203\t0.0334391\t0.0433659\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Loss: 0.03067902848124504 , Elapsed time: 57.372939348220825\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Loss: 0.03084574267268181 , Elapsed time: 64.30044341087341\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Loss: 0.03205670043826103 , Elapsed time: 57.48608064651489\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Loss: 0.033253565430641174 , Elapsed time: 66.1794536113739\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t4     \t0.0303203\t0.0314311\t0.0332536\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Loss: 0.03187628090381622 , Elapsed time: 63.54466891288757\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Loss: 0.03092357888817787 , Elapsed time: 63.583484411239624\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t2     \t0.0303203\t0.0308239\t0.0318763\n",
      "-- Best Individual =  [1, 0, 0, 1, 0, 0, 1]\n",
      "-- Best Fitness =  0.030320338904857635\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABibklEQVR4nO3dd1RURxvA4d/SUbGgAlGwYe8NSyxEFBcFFMWGirFFY2KPMdUSa4zRJNbEGFuMxg4qFgxGsJdY+KJY0BCxgFFURCnLMt8fGzeiwFJ2WYF5ztnDlnvnvsPCvjsz984ohBACSZIkSXqJibEDkCRJkl5PMkFIkiRJGZIJQpIkScqQTBCSJElShmSCkCRJkjIkE4QkSZKUIZkgCqk7d+7QpEkT1Gq1sUPBzc2NY8eOGTuMfLVhwwbefPNNmjRpwsOHD2nSpAnR0dHGDksygOHDh7Njxw5jh2EQMkHkkJubG/Xr1ycuLi7d8927d6dWrVrcunXLoMffvn07tWrVYu7cueme/+2336hVqxYff/wxABUqVODcuXOYmpoaNB59Wbx4MbVq1SI8PNzYoeSZSqXiyy+/ZNWqVZw7d44yZcpw7tw5nJycAPj444/55ptvjBzl6+N///sfI0eOxMXFhebNm9O1a1e++eYbHj9+bOzQXrF48WImTZqU7rmVK1fSo0cPI0VkWDJB5ELFihUJCgrSPr5y5QpJSUn5dvxKlSqxZ88eUlNTtc8FBARQpUqVfItBn4QQBAYGUrp0aYN9E8vPltSDBw9ITk6mevXq+XbMguDFv9fnzp49y6BBg2jatCl79+7lzJkzrFy5ElNTUy5fvmz0+Io6mSByoXv37gQEBGgfBwQE4OPjk26bQ4cO4ePjQ9OmTXF1dWXx4sXa1/bs2UPHjh1JSEgAIDQ0lDZt2rzSKslMuXLlqFmzJkeOHAHg0aNHnDt3Djc3N+02t27dolatWto/en9/f7799lv69etHkyZNGDp0aKbHe/z4MSNHjqRVq1a4uLgwcuRIYmJitK/rKisgIIAOHTrQsmVLli9frrM+Z86c4d69e3z66afs2bOHlJQUAIYNG8b69evTbdutWzeCg4MBuH79OkOGDKFFixYolUr27Nmj3e7jjz9m2rRpvPPOOzRu3JiTJ09m+Z68HPfSpUvTdY2lpaWxYsUKOnXqRMuWLRk3bhyPHj16pS5//fUXHh4eALi4uDBo0CAAatWqxd9//82mTZvYtWsXP/30E02aNOHdd98FNC3Tn376CW9vb5o1a8b48eNJTk7Wlvv777/TvXt3mjdvTr9+/dJ9eK5YsYJ27drRpEkTlEolx48fByA8PJyePXvStGlT3nzzzVdanS/avHkz7u7utGjRgnfffZfY2FgApk6dyrx589JtO2rUKFavXg1AbGwsY8aMoVWrVri5ubFu3TrtdosXL2bs2LFMmjSJpk2bZpj858+fT8+ePRk5ciTlypUDNK3fsWPH0rJlS+12W7dupUuXLri4uDBs2DBu376tfa1WrVps3LiRzp074+LiwhdffMGLE0To2veXX36hc+fOdO7cGYBZs2bh6upK06ZN6dmzJ2fOnAEgLCyMH374gb1799KkSRO6desGaP4ftmzZAmj+TpYtW0aHDh1o3bo1kydP5smTJ8B//5M7duzgrbfeeuX/IyfvV74RUo506NBBHD16VHTu3FlERkaK1NRU0b59e3Hr1i1Rs2ZNER0dLYQQ4sSJE+Ly5ctCrVaLiIgI0bp1a3HgwAFtORMnThQfffSRiIuLE23atBEHDx7M1vG3bdsm+vXrJ3bu3CnGjRsnhBBi/fr1YsqUKWLhwoXio48+EkIIER0dLWrWrClUKpUQQoiBAweKjh07ihs3bojExEQxcOBAMX/+/AyPERcXJ/bt2yeePXsmnjx5IsaMGSNGjRqlfT2rsq5duyYaN24sTp06JZKTk8WcOXNEnTp1xNGjRzOt0yeffCLGjh0rUlJSRIsWLcT+/fuFEELs2LFD9O3bV7vdtWvXRLNmzURycrJ4+vSpaN++vdi6datQqVTizz//FC1atBBXr14VQgjx0UcfiaZNm4ozZ84ItVotkpKSsnxPnsd9+vRpkZycLL788ktRt25dbdyrV68WvXv3Fnfv3hXJycliypQpYsKECRnW5+XfvRBC1KxZU0RFRWljW7hwYbp9OnToIHx9fUVMTIx4+PCh8PDwEBs2bBBCCPHnn3+KVq1aifPnz4vU1FSxfft20aFDB5GcnCyuX78u2rdvL2JiYrTH/vvvv4UQQvTp00fs2LFDCCFEQkKCOHfuXIbxHjt2TLRo0UL8+eefIjk5WcyYMUP0799fCCHEqVOnRPv27UVaWpoQQohHjx6JBg0aiJiYGKFWq0WPHj3E4sWLRXJysrh586Zwc3MTYWFhQgghFi1aJOrWrSsOHDgg1Gq1SExMTHfcp0+fitq1a4sTJ05kGNdzBw4cEJ06dRKRkZFCpVKJpUuXpvu7qFmzphgxYoR4/PixuH37tmjZsqUIDQ3N9r6DBw8WDx8+1MYXEBAg4uLihEqlEj/99JN48803RVJSkrZOH3zwQbr4Bg4cKDZv3iyEEGLLli2iU6dO4ubNmyIhIUG8//77YtKkSdr3pmbNmuKzzz4TiYmJIiIiQtSrV09ERkbm6P3KT7IFkUvPWxFHjx6lWrVq2Nvbp3u9ZcuW1KpVCxMTE2rXro2npyenTp3Svj5t2jROnDjBoEGDcHNzo0OHDjk6vru7O6dOneLJkycEBgbSvXt3nfv07NmTqlWrYmVlhYeHBxERERluV6ZMGZRKJdbW1pQoUYJRo0Zx+vTpbJW1b98+3nrrLVxcXLCwsGDcuHGYmGT+Z5aYmMi+ffvw9vbG3NwcpVKp/abZqVMnLl++rP3Gt2vXLtzd3bGwsODQoUNUrFgRX19fzMzMqFevHkqlkv3792vL7tixI82aNcPExARLS8ss35N9+/bRoUMHmjdvjoWFBWPHjkWhUGjL2rRpExMmTMDBwQELCwtGjx7N/v379dot4e/vj729PaVLl6ZDhw7a3+nmzZvp27cvjRo1wtTUlB49emBubs758+cxNTUlJSWF69evo1KpcHR0pFKlSgCYmZlx8+ZN4uLiKF68OI0bN87wuLt27cLX15d69ephYWHBxIkTOX/+PLdu3aJ58+YoFArtt+j9+/fTuHFj7O3t+d///kdcXByjR4/GwsICJycn+vTpk64l17hxYzp16oSJiQlWVlbpjhsfH09aWpq25QDw1Vdf0bx5cxo3bsyyZcsA+PXXXxkxYgTOzs6YmZnx7rvvEhERka4l8M4771CyZEkqVKhAy5YttS2s7Ow7YsQISpcurY2ve/fulClTBjMzM4YOHUpKSgp//fVXtt7DXbt2MXjwYJycnChevDgTJ058pTt49OjRWFlZUbt2bWrXrq2NNbvvV34yM3YABVX37t0ZOHAgt27dyvDD+cKFC3z99ddcu3YNlUpFSkqKtusBoGTJknh4eLB69WoWLVqU4+NbWVnh6urKsmXLePjwIc2aNSMsLCzLfcqXL6+9b21tzbNnzzLcLjExkblz53L48GHtQOHTp09Rq9XaQe/Myrp37x4ODg7a14oVK0bp0qUzjenAgQOYmZnRvn17ALy9vRkyZAhxcXHY2tri6upKUFAQI0aMICgoiJkzZwJw+/ZtwsPDad68ubYstVqtbfYDvPHGG+mOldV78nLc1tbW6eK+c+cO77//frpkZ2JiwoMHD175cpBbL/9O7927pz12QEBAuu42lUrFvXv3aNGiBZ9++imLFy8mMjKStm3b8vHHH2Nvb8/s2bNZtGgRXbp0wdHRkdGjR2f4ReTevXvUq1dP+7h48eKULl2a2NhYHB0d6dq1K7t378bFxYVdu3Zpf8e3b9/m3r17r7wHLz5+8Xf6spIlS2JiYsI///yDs7MzAJMnT2by5MlMmjRJO250584d5syZk66rSwhBbGwsFStWzPB39/Tp02zv+/LfyapVq9iyZQv37t1DoVCQkJDAw4cPM63Hi+7du6ctFzTjlampqTx48ED73IsJ8cX/ney+X/lJJohcqlixIo6OjoSGhjJ79uxXXv/ggw8YOHAgK1euxNLSktmzZ6f7I4uIiGDbtm14eXkxa9YsfvrppxzH4OPjw9tvv83o0aPzVJeXrVq1ir/++ovNmzdTvnx5IiIi8PHxSdevmxk7OzuuX7+ufZyYmJhhX/1zAQEBPHv2TPuPIIRApVKxe/duBg0ahJeXF0uWLMHFxYWkpCRtv/Qbb7yBi4uLti88O7J6T+zs7NJ9S0xKSkoXt4ODA3PmzKFZs2bZPl5mXmyZZMcbb7zBu+++y6hRozJ83dvbG29vbxISEpg6dSpff/018+fPp0qVKixcuJC0tDSCg4MZO3YsJ0+epFixYun2t7OzS/eN+tmzZzx69Eib+Ly8vBg6dCgjRowgPDycpUuXauNydHTUjgnltK7FihWjUaNGHDhwgFatWums/4vJP7uys++LMZ45c4Yff/yRNWvWUKNGDUxMTHBxcdH+7et6717+Xd65cwczMzPKli2bbhwvI9l9v/KT7GLKg9mzZ7N27doM38CnT59SqlQpLC0tCQ8PZ/fu3drXkpOT+fDDD5kwYQJz587l3r17/PLLL9rX/f39XxlAzUiLFi1YvXo1AwcO1E+FXojd0tKSkiVL8ujRI5YsWZLtfZVKJYcOHeLMmTOkpKSwaNEi0tLSMtw2NjaW48eP8/333xMQEEBAQACBgYG888472pMAXF1duXPnDosWLaJr167ab/BvvfUWUVFRBAQEoFKpUKlUhIeHp0tOGdUrs/dEqVRy8OBBzp49q437xYTo5+fHt99+q/3nj4uL47fffsv27+VFZcuWzdHp0L179+bXX3/lwoULCCF49uwZhw4dIiEhgRs3bnD8+HFSUlKwsLDA0tJS28oLDAwkLi4OExMTSpYsCZDhac/e3t5s376diIgIUlJSWLhwIQ0bNsTR0RGAunXrYmtry+eff07btm21ZTVs2JASJUqwYsUKkpKSUKvVXL16NUenKk+aNIlt27axYsUK7bfsmJiYdL+ffv36sWLFCq5duwbAkydP2Lt3b7bKz+m+T58+xdTUFFtbW1JTU1myZIn2ZBLQvHe3b9/O9G/ay8uLtWvXEh0dzdOnT/nmm2/o0qULZma6v4tn9/3KTzJB5EGlSpVo0KBBhq9NmzaNRYsW0aRJE5YuXUqXLl20ry1YsAB7e3v69++PhYUF8+fP57vvviMqKgqAu3fv0rRpU53HVygUtG7dOssunNx4++23SU5OplWrVvTt25d27dple98aNWowdepUJk2aRLt27ShZsmSm3QyBgYHUqVOHtm3bUr58ee3N39+fK1eucPXqVSwsLHB3d+fYsWN4eXlp9y1RogQ//fQTe/bsoV27drRt25avv/5aewZURrJ6T2rUqMGUKVOYOHEi7dq1o3jx4tja2mJhYQGgHSsaOnQoTZo0oU+fPrm+ZqNXr15ERkbSvHlz3nvvPZ3bN2jQgJkzZzJjxgxcXFzo3Lkz27dvByAlJYUFCxbQsmVL2rZtS1xcHBMmTADg8OHDeHp60qRJE2bPns0333yDpaXlK+W3bt2acePGMWbMGNq2bUt0dPQr12l4enq+8h6YmpqyfPlyLl++TMeOHWnVqhWff/55ug9UXZo3b87atWs5ffo0SqWS5s2bM3z4cFq2bKn94uPu7s7w4cOZOHEiTZs2xcvLS2d36nM53bdt27a0b98epVKJm5sblpaW6bqgnndJtmzZMsNrH3x9fenWrRsDBw6kY8eOWFhYMGXKlGzFmt33Kz8pRHb6DaR8ExMTw7hx49i0aZOxQynSnj59iouLC/v379de4CZJRY1MEJL0r4MHD9K6dWuEEHz55ZeEh4ezY8eOHI8ZSFJhIbuYJOlfISEhtGvXjnbt2vH333+zcOFCmRykIk22ICRJkqQMyRaEJEmSlKFCdR3E+fPncz3qn5ycbPQzBvKbrHPhV9TqC7LOudk3s6u2C1WCsLS0pE6dOrnaNyIiItf7FlSyzoVfUasvyDrnZt/MyC4mSZIkKUMyQUiSJEkZkglCkiRJylChGoOQJEnSRaVScevWrXxdBdLQVCpVlmMJoJkB2tHREXNz82yXKxOEJElFyq1bt7CxsaFKlSqF5kLIxMRErK2tM31dCMGDBw+4desWVatWzXa5sotJkqQiJSkpibJlyxaa5JAdCoWCsmXL5rjVJBOEJElFTlFKDs/lps4GTRBhYWEolUrc3d1ZsWLFK68LIZg1axbu7u54e3tz8eJF7Wtr167Fy8sLT09P1qxZY8gw2X11N3ef3TXoMSRJkgoagyUItVrNjBkzWLlyJUFBQezevZvIyMh024SFhREVFUVwcDAzZ85k+vTpAFy9epUtW7awZcsWAgMDOXTokHatBEMYt28cC8MXGqx8SZKkF9WqVYsPP/xQ+zg1NZVWrVoxcuRIQDNxZEZfqvObwRJEeHg4lStXxsnJCQsLCzw9PQkJCUm3TUhICD4+PigUCho3bkx8fDz37t3j+vXrNGrUCGtra8zMzHBxceHAgQOGCpU3nd7kWOwx0kTGq0RJkiTpU7Fixbh27Zp2TODo0aPp1jbv2LEjI0aMMFZ4WgY7iyk2NjbdSmL29vavrMD18jYODg7ExsZSs2ZNvv32Wx4+fIiVlRVhYWHUr19f5zGTk5N1nuqVkfrW9VmfvJ7tx7ZTz7ae7h0KiaSkpFz9vgqyolbnolZf0F1nlUpFYmJiPkb0KiEErVu3Jjg4GHd3d3bu3IlSqeTs2bMkJiYSGBjIpUuX+OSTT5gyZQolSpTg4sWLPHjwgPHjx+Pu7v5KedmpU3ZOh32RwRJERrOIvzxIktk2zs7ODB8+nKFDh1KsWDFq1aqVrbVZczsXU9lKZfn45MdcUV+hV51eOd6/oJJz1hR+Ra2+oLvOERER2lNC162DVav0e/yhQ2HQoKy3USgUdO/enWXLlqFUKomMjKRPnz5cuHABa2trLCwsMDMz0/aixMXFsWnTJm7cuMGoUaPo1q1buvJ0neb6nLm5+Su/G6PMxeTg4EBMTIz2cWxsLHZ2dlluExMTo92md+/e7Nixg19++YXSpUtTuXJlQ4WKXXE76papy/7r+w12DEmSpBfVrl2bW7dusXv3blxdXbPctlOnTpiYmFC9enXu37+fTxEasAXRoEEDoqKiiI6Oxt7enqCgIBYsWJBuGzc3N9avX4+npycXLlzAxsZGmyAePHhA2bJluXPnDsHBwQZfo7mNfRtWXVnF46THlLIqZdBjSZL0ehg0SPe3fUNyc3Pjq6++Yt26dTx69CjT7SwsLPIvqBcYLEGYmZkxdepUhg8fjlqtxtfXlxo1arBx40YA/Pz8cHV1JTQ0FHd3d6ytrZkzZ452/zFjxvDo0SPMzMyYNm0apUoZ9kO73Rvt+PHyjxz86yA96vQw6LEkSZIAevXqhY2NDbVq1eLkyZPGDucVBp1qw9XV9ZWmk5+fn/a+QqFg2rRpGe67YcMGQ4b2ikZlG2FjYcO+yH0yQUiSlC8cHBx4++23jR1GpuRcTP8yNzGnY7WO7L++HyFEkbzSUpKk/HHu3LlXnmvZsiUtW7YEoGfPnvTs2ROAL7/8Uue+hiKn2niB0lnJ34//5sqDK8YORZIkyehkgniB0lkJwP5IeTaTJEmSTBAvqFqmKjXL1mTf9X3GDkWSJMnoZIJ4iYezB6FRoSSlFp7FRCRJknJDJoiXKKsrSUxN5PDfh40diiRJklHJBPES18quWJpasi9SdjNJklS0yQTxkuIWxWlXuZ2cdkOSJIPRNd3360ImiAx4OHtw8Z+LRD+ONnYokiQVQrqm+35dyASRAWV1zemuwdeDjRyJJEmFVfv27Tl06BAAQUFBeHp6al979uwZn3zyCb6+vvj4+PDbb78BcOvWLfr370+PHj3o0aMHZ8+eBeD06dP4+/szduxYPDw8+OCDDzKcLTun5JXUGahXvh4VbSqy7/o+hjUdZuxwJEkykHUX1rHqnH7n+x7aZCiDGumeAbBr164sW7aMDh06cOXKFXx9ffnjjz8A+P7772nVqhVz584lPj6e3r178+abb1K2bFlWr16NpaUlUVFRTJw4ke3btwNw6dIlgoKCsLOzw8/Pjz/++IPmzZvnqS4yQWRAoVCgdFay/fJ2UtNSMTORvyZJkvQrq+m+jxw5wsGDB1n172IVycnJ3L17Fzs7O2bMmMHly5cxMTFJtxRzw4YNtQuw1a5dm9u3b8sEYSjK6kpWnV/FqduneNPpTWOHI0mSAQxqNChb3/YNJavpvhctWkS1atXSPbd48WLKlStHYGAgaWlpNGzYUPvai1OCm5qaolar8xyfHIPIRKdqnTBRmMhpNyRJMphevXrx3nvvUatWrXTPt23blvXr12vHES5dugTAkydPKF++PCYmJgQGBuolCWRFJohM2Frb0qJiCznthiRJBpPZdN/vvfceqampdOvWDS8vL7777jsA+vfvz44dO+jTpw9RUVEUK1bMoPHJLqYseDh78EXoFzx49oCyxcoaOxxJkgoJXdN9W1lZMWPGjFe2qVKlCrt27dI+/uCDDwBwcXGhffv22uenTp2qlzhlCyILyupKBIIDNw4YOxRJkqR8JxNEFlwquFDGqoy8qlqSpCJJJogsmJqY4u7szv7I/Xq56ESSJKkgkQlCBw9nD+4m3OV/9/5n7FAkSZLylUwQOnR27gzIVeYkSSp6ZILQoWLJijSwayBPd5UkqciRCSIblM5Kjtw8QkJKgrFDkSSpEJDTfRciHtU9SFGncCjqkLFDkSSpECiU032npaWRkJD9b9FhYWEolUrc3d1ZsWLFK68LIZg1axbu7u54e3tz8eJF7Wtr1qzB09MTLy8vJk6cSHJyck5C1au2ldpSzLyYHIeQJElvspruOzw8nH79+uHj40O/fv24ceMGAKtXr+aTTz4B4MqVK3h5eZGYmGiwGHVeSf3BBx/wxRdfYGJiQs+ePUlISGDw4MEMHz48y/3UajUzZsxg9erV2Nvb06tXL9zc3Khevbp2m7CwMKKioggODubChQtMnz6dLVu2EBsby7p169izZw9WVlaMGzeOoKAgevbsmfca54KlmSUdqnSQ4xCSVNisWwer9DvdN0OHwqC8TfddrVo11q9fj5mZGceOHeObb75h8eLFvP322/j7+3PgwAGWL1/OF198gbW1tcGShM4WRGRkJCVKlOC3337D1dWV33//ncDAQJ0Fh4eHU7lyZZycnLCwsMDT05OQkJB024SEhODj44NCoaBx48bEx8dz7949QJNgkpKSSE1NJSkpCTs7u1xWUT+Uzkoi4yK5HnfdqHFIklQ4ZDXd95MnTxg3bhxeXl7MnTuXa9euAWBiYsKXX37J5MmTadGiBc2aNTNojDpbEKmpqahUKn777TcGDhyIubk5CoVCZ8GxsbHauckB7O3tCQ8Pz3IbBwcHYmNjadCgAUOHDqVDhw5YWlrSpk0b2rZtq/OYycnJRERE6NwuI0lJSVnuW12hafmsPboWv+p+uTrG60ZXnQujolbnolZf0F1nlUr13zfu3r01N33T8Y1eCEFiYiLt27dn3rx5rFy5kkePHqFWq0lMTGTBggU0bdqUr7/+mtu3bzN8+HBtzFevXsXa2pq7d+9qn3teni4qlSpHfw86E0Tfvn1xc3Ojdu3auLi4cPv2bUqUKKGz4IyuPH45sWS2zePHjwkJCSEkJAQbGxvGjRtHYGAg3bt3z/KYlpaW1KlTR2dsGYmIiMhy39qiNlWPV+XC0wvMqPPqJFoFka46F0ZFrc5Frb6gu84RERFYW1vnY0SvUigUWFtb069fP8qUKUPDhg05efIkpqam2i4jR0dHrK2t2bt3LyYmJlhbW/PkyRPmz5/PL7/8wsyZMwkNDcXDw4PExMRs1cnc3PyV301WCUNnF9OgQYM4fPgwP/74IwqFgooVK7Ju3TqdgTg4OBATE6N9HBsb+0o30cvbxMTEYGdnx7Fjx3B0dMTW1hZzc3M6d+6c4eyH+en5KnMH/zpIijrFqLFIklQ4ZDbd9/Dhw1m4cCH9+vVLt+bDnDlz6N+/P1WrVmX27NksWLCABw8eGCw+nQli7dq1JCQkIITg008/pUePHpw4cUJnwQ0aNCAqKoro6GhSUlIICgrCzc0t3TZubm4EBAQghOD8+fPY2NhgZ2dHhQoVuHDhAomJiQghOH78OM7OzrmvpZ54VPcgISWBY9HHjB2KJEkFWGbTff/www8ANGnShP379/Prr78yfvx4Dh48CMDcuXMZ9O8A+BtvvMGBAwcoW9ZwSxHoTBDbtm2jRIkSHDlyhLi4OObOncuCBQt0FmxmZsbUqVMZPnw4Xbt2pUuXLtSoUYONGzeyceNGAFxdXXFycsLd3Z0pU6Ywbdo0ABo1aoRSqaRHjx54e3uTlpZG375981jVvOtQtQNmJmbydFdJkooEnWMQz8cJQkND8fX1pXbt2tme2dTV1fWV0Xk/v/8GeBUKhTYpvGzs2LGMHTs2W8fJLyUtS9LGqQ37ru9jbqe5xg5HkiTJoHS2IOrXr8/QoUMJCwujbdu2JCQkYGJSdC/AVjorOR9znpiEGN0bS5L0WiqK0/fnps46P+lnz57NBx98wNatW7G2tkalUjFnzpxcBVgYeFT3ACD4erCRI5EkKTesrKx48OBBkUoSQggePHiAlZVVjvbT2cWkUCiIjIzk999/Z/To0SQmJpKSUnTP4mnk0Ai74nbsv76fQY10Xy0pSdLrxdHRkVu3bvHPP/8YOxS9UalUmJubZ7mNlZUVjo6OOSpXZ4KYPn06JiYmnDhxgtGjR1O8eHHGjBnDtm3bcnSgwsJEYYLSWcneyL2kiTRMFEW3u02SCiJzc3OqVq1q7DD0ylDXu+j8dAsPD2fatGlYWloCUKpUKVQqld4DKUiUzkruP7vP2btnjR2KJEmSwehMEGZmZqjVau1V0HFxcUV6kBo0q8wpULAvUk7eJ0lS4aXzk97f35/333+fBw8e8M033+Dn5/faLWqR38oXL0/TN5qy/7q8HkKSpMJL5xhEt27dqFevHidOnEAIwbJly16Lq5qNTemsZN7ReTxOekwpq1LGDkeSJEnvstVXVKVKFTp16oSbmxvW1tbcuXPH0HG99jyqe6AWakL+CtG9sSRJUgGkswXx888/s2TJEsqVK5du7GHXrl0GDex118qxFTYWNuyL3EfPOsZZyEiSJMmQdCaIdevWsW/fPsqUKZMf8RQY5qbmdKrWif3X9yOEyNYaGZIkSQWJzi4mBwcHbGxs8iOWAkfprOTm45tcvn/Z2KFIkiTpnc4WhJOTE/7+/rz11ltYWFhonx8yZIhBAysIlNWVAOy/vp865YvWoiySJBV+OlsQFSpUoE2bNqhUKp4+faq9SVCldBVqla0lT3eVJKlQ0tmCcHZ2pkuXLume27t3r8ECKmg8qnvwwx8/kKhKxNrcuMsYSpIk6ZPOFsSKFSuy9VxRpXRWkpSaxOGbh40diiRJkl5l2oIIDQ0lLCyM2NhYZs2apX0+ISEBU1PTfAmuIHCt4oqlqSX7IvfR2bmzscORJEnSm0wThL29PfXr1+fgwYPUq1dP+3zx4sX55JNP8iW4gqCYeTHaV24vxyEkSSp0Mk0QtWvXpnbt2nh7e2NmpnOookjzqO7BB8EfEP04GqdSTsYOR5IkSS8y/eQfN24c3333HT169Mjw9aJ+JfWLlM5KPuAD9l/fz/Cmw40djiRJkl5kmiA+/vhjAL7//vt8C6agqlu+Lo4lHdkXuU8mCEmSCo1Mz2J67733AKhYsSKrVq2iYsWK6W7SfxQKBUpnJb/d+I3UtFRjhyNJkqQXmSaIFxf0PntWrpymi9JZyePkx5y8ddLYoUiSJOlFpglCH5PPhYWFoVQqcXd3z/DaCSEEs2bNwt3dHW9vby5evAjAjRs36N69u/bWtGlT1qxZk+d4DKlTtU6YKEzk2UySJBUamY5B3LhxA29vbwBu3rypvf+crkFqtVrNjBkzWL16Nfb29vTq1Qs3NzeqV6+u3SYsLIyoqCiCg4O5cOEC06dPZ8uWLVSrVo3AwEBtOe3bt8fd3T3XlcwPZazL0LJiS/ZF7mNGhxnGDkeSJCnPMk0Qe/bsyVPB4eHhVK5cGScnzWmfnp6ehISEpEsQISEh+Pj4oFAoaNy4MfHx8dy7dw87OzvtNsePH8fJyalAjHt4VPdg+qHp3H92n3LFyhk7HEmSpDzJtIvp5UHpnA5Sx8bG4uDgoH1sb29PbGxslts4ODi8sk1QUBBeXl7ZrpAxKZ2VCAS/3fjN2KFIkiTlmcGugHtxkPu5l8c1dG2TkpLCwYMH+eCDD7J1zOTkZCIiInIYqUZSUlKu932ueFpxSlmUYtMfm2hk2ihPZeUHfdS5oClqdS5q9QVZZ30yWIJwcHAgJiZG+zg2NjZd11FG28TExKTbJiwsjHr16lGuXPa6aywtLalTJ3frMkREROR63xd5RHgQ+ncotWvXfu1XmdNXnQuSolbnolZfkHXOzb6Z0TmbK2iy040bN3J00AYNGhAVFUV0dDQpKSkEBQXh5uaWbhs3NzcCAgIQQnD+/HlsbGzSJYigoCA8PT1zdFxj86juQUxCDOGx4cYORZIkKU90JoiDBw/SvXt3hg/XXCEcERHBu+++q7NgMzMzpk6dyvDhw+natStdunShRo0abNy4kY0bNwLg6uqKk5MT7u7uTJkyhWnTpmn3T0xM5NixY3TuXLBmSH0+o6s83VWSpIJOZxfTkiVL2Lp1K/7+/gDUqVOH27dvZ6twV1dXXF1d0z3n5+enva9QKNIlhRdZW1tz8mTBu+isgk0FGto3ZF/kPia3mWzscCRJknJNZwvC1NQUGxub/Iil0FA6Kzly8wgJKQnGDkWSJCnXdCaIGjVqsGvXLtRqNVFRUcycOZMmTZrkR2wFlkd1D1RpKn7/63djhyJJkpRrOhPElClTiIyMxMLCgokTJ1KiRAk+++yz/IitwGrj1IZi5sXkOIQkSQWazjEIa2trJkyYwIQJE/IjnkLB0swSt6pu7IvcZ+xQJEmSck1ngsjojCUbGxvq169Pv379sLS0NEhgBZ3SWcnuq7uJjIukum113TtIkiS9ZnR2MTk6OlK8eHH69OlDnz59KFGiBOXKlSMqKorPP/88P2IskJTOSgD2R8puJkmSCiadLYiIiAh++eUX7WM3NzcGDBjAL7/8UuAuYstP1W2rU61MNfZf38/7Ld43djiSJEk5prMFERcXx507d7SP79y5w8OHDwEwNzc3XGQF3PNV5g7+dZAUdYqxw5EkScoxnS2Ijz/+mP79+2un7b516xbTpk3j2bNn+Pj4GDq+As2jugfLzyzn6M2jdKjawdjhSJIk5YjOBOHq6kpwcDA3btxACEG1atW0A9ODBw82dHwFWocqHTAzMWP/9f0yQUiSVOBka7K+qKgobty4wZUrV9i7dy8BAQEGDqtwsLG0oW2ltvJ0V0mSCiSdCWLJkiXMnDmTWbNmcfLkSebPn8/BgwfzI7b84+5OmRcG4vVJ6azkQuwF7j65a5DyJUmSDEVngti/fz9r166lXLlyzJ07l8DAQFJSCtmgq5UV5ZYvh8REvRftUd0DgODrwXovW5IkyZB0JghLS0tMTEwwMzMjISGBsmXLEh0dnR+x5Z8JEzCLiwMDtCIa2jfEvri9nHZDkqQCR2eCqF+/PvHx8fTu3ZuePXvSo0cPGjZsmB+x5Z8OHUiqVQu+/RYyWAY1L0wUJiirKwm+How6Ta3XsiVJkgwpy7OYhBCMHDmSkiVL4ufnR7t27UhISKB27dr5FV/+UCiIe/ttKnz6KRw4AHpepEjprGTdhXWcvXsWl4ouei1bkiTJULJsQSgUCt5//7+rgB0dHQtfcvhXfNeuYG8P33yj97Ldq7mjQCHPZpIkqUDR2cXUqFEjwsML//rKwsIC3n8f9u2DS5f0Wnb54uVpVqGZHIeQJKlA0ZkgTp48Sd++fenUqRPe3t7aW6H07rtgaakZi9AzD2cPTtw6waOkR3ovW5IkyRB0Xkn9448/5kccr4fy5WHQIPj5Z5gzB8qV01vRyupKZh2eRciNEHzr+uqtXEmSJEPR2YKoWLEid+/e5cSJE1SsWBFra2vS0tLyIzbjGD8ekpLg++/1WmzLii0paVlSjkNIklRgZOtK6pUrV7JixQoAVCoVH374ocEDM5q6dUGphKVLITlZb8Wam5rTqVon9l/fj9DzqbSSJEmGoDNBHDhwgOXLl2NtbQ2Avb09T58+NXhgRjVhAsTEwK+/6rVYpbOS6PhoLt+/rNdyJUmSDEFngjA3N0ehUKBQKAB49uyZwYMyus6dNS2Jb77R64Vzz1eZk91MkiQVBDoTRJcuXZg6dSrx8fFs3ryZIUOG0KdPn2wVHhYWhlKpxN3dXdtF9SIhBLNmzcLd3R1vb28uXryofS0+Pp6xY8fi4eFBly5dOHfuXA6qlUcKhaYVceECHDqkt2Irl65M7XK15emukiQVCDoTxLBhw1AqlXTu3Jm//vqLsWPH4u/vr7NgtVrNjBkzWLlyJUFBQezevZvIyMh024SFhREVFUVwcDAzZ85k+vTp2tdmz55Nu3bt2LdvH4GBgTg7O+e8dnkxYIDmLCY9Xzjn4exB6N+hJKr0PzGgJEmSPulMEGvWrMHZ2ZmPPvqIjz76iDZt2mSr4PDwcCpXroyTkxMWFhZ4enoSEhKSbpuQkBB8fHxQKBQ0btyY+Ph47t27R0JCAqdPn6ZXr14AWFhYULJkyVxULw+srWHUKNi1C65e1VuxyupKklKTCPs7TG9lSpIkGYLO6yASEhIYNmwYpUqVwtPTE6VSSblsXB8QGxuLg4OD9rG9vf0rV2S/vI2DgwOxsbGYmZlha2vLJ598wuXLl6lXrx6fffYZxYoVy/KYycnJRERE6IwtI0lJSa/sa9qpE9W//JJH06cTO2VKrsp9mX2qPZamlmw4tYFKqkp6KTO3MqpzYVfU6lzU6guyzvqkM0GMHj2a0aNHc/nyZfbu3cvAgQNxcHBgzZo1We6X0amczwe6dW2TmprKpUuXmDJlCo0aNWLWrFmsWLGC8ePHZ3lMS0tL6tSpo6tKGYqIiHh13zp1YMAAbDdvxnbJErC1zVXZL3O94Mrph6dzHau+ZFjnQq6o1bmo1RdknXOzb2ayteQoQNmyZSlXrhylS5fmwYMHOrd3cHAgJiZG+zg2NhY7O7sst4mJicHOzg4HBwccHBxo1KgRAB4eHlzS8/xI2TZ+PDx7Bnq8otzD2YOI+xHcfHxTb2VKkiTpm84EsWHDBvz9/Rk8eDAPHz5k1qxZ7Nq1S2fBDRo0ICoqiujoaFJSUggKCsLNzS3dNm5ubgQEBCCE4Pz589jY2GBnZ0f58uVxcHDgxo0bABw/fjz/B6mfa9QI3Nxg8WJQqfRSpLK65nTX/ZHybCZJkl5fOruY7ty5w6effqptviQnJ7N37166dOmSdcFmZkydOpXhw4ejVqvx9fWlRo0abNy4EQA/Pz9cXV0JDQ3F3d0da2tr5syZo91/ypQpTJo0CZVKhZOTE3Pnzs1LPfNm4kTw8oItW6B//zwXV6dcHZxKOrHv+j7eafaOHgKUJEnSP50JYtKkSajVakJDQwkKCuLIkSM0b95cZ4IAcHV1xdXVNd1zfn5+2vsKhYJp06ZluG+dOnXYvn27zmPkiy5doFYtzSmvfn6a6yTyQKFQoHRWsvnSZlRqFeam5noKVJIkSX+yTBCnT59m165dhIaG0rBhQ86ePUtISIh22o0iw8QExo2D996Do0ehbds8F+lR3YOV51Zy8vZJ2lbKe3mSJEn6lukYRPv27VmwYAFNmzYlKCiIxYsXY2lpWfSSw3ODBkGZMrBwoV6K61itI6YKUzkOIUnSayvTBNG5c2diY2PZu3cvv//+O8+ePXvlNNUipXhxzYJCAQHw7+B5XpS2Kk1Lx5bsuy7nZZIk6fWUaYL4/PPPOXjwIIMHD+bkyZMolUri4uLYs2dP4Z/NNTPvvw+mprBokV6K863jy5k7Z1h2epleypMkSdKnLE9zVSgUtG7dmlmzZnHw4EEWLFhASEjIK6erFhkVK0LfvvDTT/D4cZ6LG9dyHN41vRmzdwx7ru3RQ4CSJEn6k+0L5czNzXFzc2PBggWEhoYaMqbX24QJkJAAK1fmuShTE1M2+G6gkX0j+m7ty4WYC3oIUJIkST+ynSBeZGVlpe84Co5mzaB9e003U2pqnosrYVGC3f13U9qqNJ4bPLkdf1sPQUqSJOVdrhJEkTdhAty8CTt26KW4CjYV2O23m8fJj/He6E1CSoJeypUkScqLTBPEDz/8YLz5j1533t5QrZpe14po5NCIzb02cyH2An7b/FCnqfVWtiRJUm5kmiAcHR1Zt24dPj4+fPzxx+zZs4fHehiYLRRMTTUXzh0/DidO6K3YLjW6sKTLEnZf3c3E/RP1Vq4kSVJuZHoltaenJ56engBcunSJw4cPM3r0aNLS0mjdujXt27enYcOG+Rboa2fIEJgyRdOK2LRJb8WOchlFZFwkC08sxNnWmbEtx+qtbEmSpJzQORcTQN26dalbty4jR44kISGBo0ePsmXLlqKdIGxsYMQITYK4eRMq6W/xn6/cv+LGoxtM2D+BqqWr4l3LW29lS5IkZVeOB6lLlCiBUqlk5syZhoinYBkzRvNz8WK9FmtqYsr6Hutp+kZT+m3rx9m7Z/VaviRJUnbIs5jyolIl8PXVLCb05Ileiy5uUZxdfrsoV6wcXhu8iH4crdfyJUmSdJEJIq8mTNBcVb16td6LdijhQFD/IJ6qnuK10YsnyfpNQpIkSVnJVoKIjY3l7NmznD59WnuT/tWqFbRuDd99B2r9n5pa364+W3pv4eK9i/Td2pfUtLxfnCdJkpQdOgep58+fz969e3F2dsbU1FT7vIuLi0EDK1AmTIA+fWDXLvDx0XvxnZ07s9xzOSN2j2Ds3rEs7bq0aM+sK0lSvtCZIH777Tf27duHhYVFfsRTMPXoAZUra85oMkCCAHin2TtExkXy1bGvqGFbgwmtJxjkOJIkSc/p7GJycnJCpVLlRywFl5mZ5oymsDD44w+DHWZup7n41vHlg+APCLgcYLDjSJIkQTZaENbW1vj4+NC6det0rYjPP//coIEVOMOHw/TpmlbE+vUGOYSJwoSfe/zMrfhb9N/Wn7AhYTSv0Nwgx5IkSdKZINzc3Iru+g85UaoUDBsGS5fCvHmatSMMwNrcmsB+gbT6qRXeG705MewElUtXNsixJEkq2nQmiB49euRHHIXD2LGaacCXLoU5cwx2GPsS9gT1D+LNn97Ea6MXR4YcoZRVKYMdT5KkoinTMYhx48YB4O3tneFNykC1appB6u+/BwMvy1q3fF229dnG5fuX6b2lNyq1HCeSJEm/Mm1BfPbZZwB8//33+RZMoTBxomadiHXrYNQogx6qY7WO/OD1A8N2DuP9Pe/zg9cP8vRXSZL0JtMEYWdnB0DFPPSlh4WFMXv2bNLS0ujduzcjRoxI97oQgtmzZxMaGoqVlRVffvkl9erVAzRjH8WLF8fExARTU1O2b9+e6zjyVZs20Lw5fPstjBwJJoa9WH1ok6Fcj7vOnCNzqGFbgw/bfGjQ40mSVHRkmiCaNGmS7tuoEAKFQqH9efZs1hPIqdVqZsyYwerVq7G3t6dXr164ublRvXp17TZhYWFERUURHBzMhQsXmD59Olu2bNG+vnbtWmxtbfNSv/ynUGgunBswAPbuhX+nTDekmW4zuf7wOpN/m0zVMlXpVbeXwY8pSVLhl2mCaN26Nffv38fd3R1PT08qVKiQo4LDw8OpXLkyTk5OgGZ9iZCQkHQJIiQkBB8fHxQKBY0bNyY+Pp579+5pWy8FVu/eMHkyLFyYLwnCRGHCGp81RMdH47/DH6eSTrR0bGnw40qSVLhlmiCWLVvGkydPCA4OZsqUKSQnJ9OlSxc8PT0pXbq0zoJjY2NxcHDQPra3tyc8PDzLbRwcHIiNjdUmiGHDhqFQKOjbty99+/bVeczk5GQiIiJ0bpeRpKSkXO+bkbJ9+2K3cCE3duwguXZtvZWbla+afIVfnB+e6z3Z2HEjjiUcs9xe33UuCIpanYtafUHWWZ+yPM3VxsYGX19fevTowZ49e5g5cyYpKSkMGTJEZ8FCiFeee3kANattNm7ciL29PQ8ePGDIkCFUq1ZN5/xPlpaW1KlTR2dsGYmIiMj1vhn6/HP4/nuq7dypmYojnxxwOkDrn1oz7tQ4jg07Rmmr0pluq/c6FwBFrc5Frb4g65ybfTOT5Qjq2bNnmTlzJj169ODs2bMsXbo0W8kBNK2BmJgY7eMXWwaZbRMTE6Pdxt7eHoCyZcvi7u7+SuvjtVemDAweDBs2wAt1NLRa5Wqxve92IuMi8d3sS4o6Jd+OLUlS4ZJpgnBzc+OLL77A3t6emTNn4uvri7W1NRcvXuTixYs6C27QoAFRUVFER0eTkpJCUFDQK1dku7m5ERAQgBCC8+fPY2Njg52dHc+ePSMhIQGAZ8+ecfToUWrUqJHHqhrBuHGQkgLLluXrYd+q8hYru63k4F8HGbV7VIYtNUmSJF0y7WJ6fnrr4cOHOXLkSLoPGYVCwbp167Iu2MyMqVOnMnz4cNRqNb6+vtSoUYONGzcC4Ofnh6urK6Ghobi7u2Ntbc2cf68+fvDgAe+//z6gORvKy8uL9u3b562mxlCzJnh7w/Ll8MknYG2db4ce1GgQ1+OuMyNsBtVtq/NJu0/y7diSJBUOmSaIn3/+Oc+Fu7q64urqmu45Pz8/7X2FQsG0adNe2c/JyYmdO3fm+fivhQkTNOtE/PKLZkK/fDT9relEPozk04OfUq1MNfrW1z3QL0mS9JxcctTQ3noLGjXSzPKaz109CoWCVd1W0bZSW94OeJtj0cfy9fiSJBVsMkEY2vML5y5dguDgfD+8pZklAX0DqFSqEt1/7c71uOv5HoMkSQVTpgkiNVWufaw3/fqBg4OmFWEEZYuVJah/EGkiDc8NnsQlxhklDkmSCpZME0SfPn1477332LhxI7du3crPmAofS0t4/33Yv1/TkjCCGmVrENA3gL8e/UXPTT3l6a+SJOmUaYLYvn27dkbXOXPm4Ovry5w5czhy5AgpKfLDJcfefResrDST+BlJu8rtWN19NaF/h/LOrnfk6a+SJGUpyyupK1asiJ+fH35+fqhUKs6cOcPhw4f59ttvsbW1ZcWKFfkVZ8FXrhz4+8PPP8Ps2VC+vFHC6N+gP9fjrjP10FTEM8GaOmswUcihKEmSXqVzRbnnzM3Nad26Na1btwY0V0ZLOTR+PPz4o2ZBoSlTjBbG5+0/Jy4xjm9PfosIEKzqtgpzU3OjxSNJ0usp118dn0+FIeVA3brg4aFZkjQ52WhhKBQKFioXMrb+WNaHr6fHph48Uz0zWjySJL2eZN9CfpswAWJj4ddfjRqGQqHg3brv8r3n9+y5tofOP3fmYeJDo8YkSdLrRWeCSM7gm25cnDxNMtfc3aFePaNcOJeRkc1Hsrn3Zk7fOY3rGlfuPrlr7JAkSXpN6EwQvXr14vz589rH+/fvTzddhpRDCoVmLOLCBfj9d2NHA0Cvur0I6h/EjYc3aLOqDZFxkcYOSZKk14DOBPH1118zc+ZM5s2bxwcffMDmzZtZu3ZtfsRWeA0YoDmLyUgXzmWkU7VO/P7278Qnx9N2VVvOx5w3dkiSJBmZzgRRq1YtRo0axa+//srJkyeZOnVqulXgpFywtoZRo2D3brh61djRaLlUdOHI0CNYmFrgusaVsL/DjB2SJElGpDNBfPrpp6xdu5adO3cyd+5c3n33XX755Zf8iK1we+89sLCA774zdiTp1C5Xm6NDj1LBpgLK9Up2Xikks+pKkpRjOhNEzZo1WbduHU5OTrRr147Nmzdna8EgSQd7e01X0/PrIl6DAevnnEo5cXjIYRraN6Tnpp6sOb/G2CFJUrY8SX7CufvnjB1GoaEzQQwePDjdWtI2NjbahX2kPFq4UHNW06hRmuVJn70+1yKUK1aOkEEhuFV1Y0jgEL4+9rWxQ5KkLKnT1Phs8mHAwQEsP73c2OEUCjqvpI6KimLhwoVERkamO+U1JCTEoIEVCaVLaxYTmjULpk+H8+dh+3ZwdjZyYBolLEqwy28X/jv8+fDAh9x/dp+5Heem+8IgSa+LL0K/4OBfB3Eu6cz7e96njHUZ+tXvZ+ywCjSdLYhPPvkEPz8/TE1NWbduHT4+PnTv3j0/YisaTExg6lTYsweio6FZM03SeE1Ymlmy0Xcj7zZ7l3lH5/HOrndITSt4U8FffXCVmaEzuZ9039ihSAawP3I/s8JmMaTxELZ02kK7yu3w3+HPvsh9xg6tQMvWhXLP51+qWLEiY8aM4cSJEwYPrMjx8ICzZzWth27d4PPPQa02dlQAmJqYssxzGVPaT+Gncz/RZ0sfklKTjB1Wttx4eIMhgUOos7QOUw9Nxf+gPzcf3zR2WJIeRT+OZsD2AdS3q8+SrkuwMrNiZ7+dNLBrQM9NPeVKinmgM0FYWFiQlpZG5cqVWb9+PQcOHODBgwf5EVvRU6UKHD0Kw4ZpZnzt0gXuvx7feBUKBTM6zOA7j+/YcXkHXX/pSnxyvLHDytTfj/7mnZ3vUHNxTX7981fGtxxPYL9AHiQ/oN3qdvJiwEJCpVbRd2tfUtQpbO2zlWLmxQAoZVWKfQP34VjSEc8Nnvwv9n9GjrRgytZpromJiXz++edcvHiRwMBA5s2blx+xFU1WVrBypebsprAwTZfT6dPGjkprbMuxrO+xnsM3D9NhbQfuPb1n7JDSuRV/i1G7R1FjcQ3Wha/jPZf3uDH2BguUC+hWqxtr3lrD05SntFvdjj/v/WnscKU8+ui3jzh+6zgru62kZtma6V6zK25HsH8wxc2Lo1yv5MbDG0aKsgAThcilS5eMsq/BnDkjROXKQlhYCPHDD0Kkpem1+LzUOehqkLCeZS1qLKoh/nr4l/6CyqU78XfEmD1jhMVMC2E+w1yM2j1KRD+OfmW7S5cuiYv3Loo3vn5D2M6zFadvnzZCtPnntfy71pNtl7YJpiNGB41O9/zLdb5476KwnWcrnL9zFnef3M3PEPONoT77Mj2L6d13380ysXz//fd6T1bSS5o1gz/+0FwvMXIkHD8Oy5ZprsQ2sq41unLA/wBeG71os6oNwQODqWdXL9/jiE2IZd7ReSw/s5zUtFSGNB7CZ+0+o3LpypnuU7d8XY4MPULHdR1xW+tGUP8g2lVul49RS3kVGRfJkMAhuFRw4evOWZ+CXbd8Xfb030PHdR1RrlcSOjiU0lal8yfQAi7TLqbz588TGxtL8+bNGTZsGEOHDk13y46wsDCUSiXu7u4Zrj4nhGDWrFm4u7vj7e39ygV4arUaHx8fRo4cmcNqFSJly0JQkOZMpzVr4M034cbr0VRuU6kNYYPDEELQbnU7jkcfz7dj3392n8kHJlP1u6p8d/I7+tXvx5XRV1jhvSLL5PBctTLVODzksPaK8eDrwfkQtaQPSalJ9N7SG1OFKVt6b8HSzFLnPi0dW7Kj7w4i/onAa4OXXP8kuzJrWqSmporQ0FAxefJk0b17d7Fw4UJx9erVbDdbUlNTRceOHcXNmzdFcnKy8Pb2FteuXUu3zaFDh8SwYcNEWlqaOHfunOjVq1e611etWiUmTpwoRowYka1jFrouppft3i1E6dKaW1BQnovTV51vxN0Q1RdVF8VmFxN7r+3VS5mZefDsgfj0t09FiTklhGK6QgzcPlBcuX8l2/u/XOfYhFjRaHkjYTHTQmy/tF3f4Rpdgfi7zqERO0cIpiN2X9md4etZ1XnLxS1CMV0huv7SVaSkphgqxHxnqM++TFsQpqamtG/fnnnz5rF582YqV66Mv78/P//8c7YST3h4OJUrV8bJyQkLCws8PT1fubguJCQEHx8fFAoFjRs3Jj4+nnv3NIOeMTExHDp0iF69euUh/RUynp6aLqcqVTT3p017LU6FrVqmKkeGHKFm2Zp4b/Rm4/826v0Yj5IeMf3QdKp+V5W5R+biWcOTi+9d5OceP78yOJkTdsXt+P3t32n6RlN6b+nN+vD1eoxa0rf14etZcXYFH7f5GM+anjnev1fdXvzg9QN7ru1hcOBg0kSaAaIsPLK8kjolJYVDhw6xe/dubt++jb+/P507d85WwbGxselmfbW3tyc8PDzLbRwcHIiNjcXOzo45c+bw4Ycf8vTp02xXJjk5mYiIiGxv/6KkpKRc75vfFKtW4TBjBqVnzCDh4EHufPUV6tKlc1yOvuv8Q6sfGH10NAO2D+DiXxcZUGNAnstMUCWw/tp61lxZQ7wqns6OnXmv7nvULF0T7kPE/ZzFn1mdF7ssZnTKaAbtGMS1v6/Rr3rhuAK3IP1d63Lt8TVG/DYCl/Iu9H+jf6b10lXntsXaMqHBBL753zeQCJ81+azAzw5gqPc50wTx0Ucfce3aNdq1a8fo0aOpWTNn39JEBpPPvfwmZLbN77//jq2tLfXr1+fkyZPZPqalpSV16tTJUZzPRURE5Hpfo9i+HX78kRJjxlDTzw+2bdMMaueAIeocVi+Mflv7MfvcbExtTJn+1vRc/fMlpCSw5NQS5h+bT1xiHN1rdWf6W9Np7NA4T/FlVedDdQ/Re0tvZpydQQnbEnzY5sM8Het1UOD+rjORkJKA74++lLQqSaB/IG/YvJHpttmp84LaCzApYcKC4wuoWbEm096apu+Q81Ve3uesEkumCSIwMBBra2v++uuvdN1KQggUCgVnz57N8qAODg7ExMRoHz9vGWS1TUxMDHZ2duzfv5+DBw8SFhZGcnIyCQkJTJo0ia+/lhPGaSkUMGIENGkCvr7Qpg0sWQLDhxs1LCszK7b22cqIXSOYETaD+8/us6jLIkxNTLO1/zPVM5afXs68o/P459k/dK3RlS/e+oLmFZobOHJN7Nv7bMd/hz+Tf5vMk5QnfPHWFwX+22VBJ4Rg5O6RXHlwhd/8f8syOWSXQqFgvrvmy8f00OnYWtsypuUYPURbyOR6ZEMHlUol3Nzc0g1SvzzI/fvvv6cbpPb19X2lnBMnTshBal3++UcId3chQIihQ4V49ixbuxmyzmlpaeLD4A8F0xF9t/QVyanJWW6fqEoU3x7/VtjPtxdMR3T+ubM4Hn1c73Flp86p6lQxNGCoYDpi/N7xIk3P15/kpwL9d/2v5aeXC6YjZobOzNb2OamzSq0SPr/6CKYj1l9Yn9sQjS7fr4PIKzMzM6ZOncrw4cNRq9X4+vpSo0YNNm7UDGD6+fnh6upKaGgo7u7uWFtby2nEc6tcOdi7VzNoPXu2ZlbYbds0g9lGolAo+Mr9K8oXK8/k3ybzMOkh2/pso4RFiXTbJacms/LsSuYcmcOdJ3dwq+rG1re20rZSWyNFrpl76sduP2JjacO3J78lISWB772+z3YrSNKfP+78wbh94/Co7sGn7T7Ve/lmJmZs9N1Il1+6MDhwMGWsy9C1Rle9H6fAynXaeQ3lNoteuCBEaGj2T5V8re3cKUSpUkLY2gqxN+tTTvPr2+Wqs6uEyRcmouWPLcX9p/eFEEIkpyaLH878IJwWOgmmI9qtaid+/+t3g8eSkzqnpaWJz0M+F0xH9Nvar0CeFlmQWxBxz+JE1W+rCseFjuKfp/9ke7/c1Plx0mPR7IdmwmqWlTj89+Ec729s+X6aa1EyaBB061btdZplO/e8veHMGXB0hK5d4YsvIM24p/INaTKEbX22cT7mPO1Wt2PpqaXUWlKLkbtHUrFkRQ74HyB0cChvVXnLqHG+TKFQMNNtJvM6zePXP3/Fd7NvgZnFtqATQjAkcAjR8dFs7rWZcsXKGfR4JS1LsnfAXiqVqoTXBi8uxFww6PEKCpkggM2boUKFVLp1g4kTISXF2BHlUfXqmmk5Bg7ULETk7Q1xcUYNyae2D/sG7uNW/C1G7x1NuWLl2DtgL8eGHqNTtU6v9UDw5DaTWdZ1Gbuu7sJrgxcJKQnGDqnQW3h8IYFXApnvPp/WTq3z5Zjli5fngP8BbCxtUK5Xcj3uer4c93UmEwRQsyZs3BjFmDHwzTeaE4KuF/S/jWLFYO1azdxNBw5oToE9Z9y1et+q8han3jlF8MBgTg0/hUd1j9c6MbxolMso1vms4/eo3+n8c2ceJT0ydkiF1tGbR/not4/wrePLuJbj8vXYlUpVInhgMKlpqbj/7M6dJ3fy9fivG5kg/mVhIVi0SHN5QWSk5uzRTZuMHVUeKRSa9a4PH4bUVGjdGlavNmpItcvVxt3ZvcAkhhf5N/JnS+8tnLlzhg5rO/DP03+MHVKh88/Tf+i7tS9VSlfhp24/GeXvpE75OuwdsJd/nv2Dcr2SuETjtr6NSSaIl/TooTkJqF496NdPM4lqYqKxo8qjli01q9W1aQNDh2qun0iSfem50bNOT3b57eLK/Su0X9Oe2/G3jR1SoaFOUzNg+wDuP7vP1j5bKWVVymixuFR0IaBvAFcfXMVrgxdPU7I/o0NhIhNEBipX1qzV89FHsGIFtGgBly4ZO6o8Kl8e9u+Hjz/WLEbUrh3FjxyB5GRjR1bgKKsr2TdwH7fjb9NudTu5EI2ezAqbxYEbB1jcZXGer5jXh47VOrLRdyMnb5/Ed7MvKeqCPjiZczJBZMLcHL78Evbtg9hYaN4cVq2CDGYHKTjMzGDuXAgIgMhIKo0YobmGwtdX0/V07/VaHe511r5ye0IGhfA4+THtVrcj4p/CMd+RsRy4foAvQr/Av6E/w5sadzaAF/Ws05MVXivYf30/g3YMQp1m/Mkx85NMEDoolXDhgqb7ftgw8PeHJ0+MHVUede8Od+5wc/lyzZlOp05pup4cHKBVK83FdhcuFPBsaHguFV0IHRyKOk1N+zXtOXfXuCcBFFS3428zYPsA6pavy3LP5a/d+NSwpsP4qtNXbLq4iTF7x2Q4h1xhJRNENrzxBgQHw4wZsHHja3FCUN5ZW/PU1RWWL4ebNzUVmjFDkxQ+/xwaN9b0tb33nuYq7YI6ZpGcrLku5PvvYdIkrP/4Q6/F17erz+Ehh7E2s6bD2g4ciz6m1/JzKyk1ibN3zxJ2N4zk1Ne3G1GlVtF3a1+eqZ6xpfcWilsUN3ZIGfqwzYdMfnMyy88sZ9qhgj2xX47k+vK711B+zMUUGipExYqaZaIXLdL7MtH5KtM6370rxE8/CeHjI0Tx4po5nooVE6J7dyF+/FGIO3fyNc5sS0kR4vx5IVauFOLdd4Vo1kwIc3NN/CCEiYnm55tvChEQIIRarbdD//3ob+2iSQeuH9BbudkRmxArgiODxfyj88WAbQNE/WX1hekXpoLpCKYj3vj6DTH38FzxMPFhvsaVHZP2TxJMR2wI36C3Mg119XhaWpoYFjhMMB3x7fFvDXKM3DLUZ59CiMLTXsrrlLfZ3ff+fRg8WLMSqI+PZmyiTJlcHdaoslXnpCQ4dAh274ZduzStDdAMynh7g5eX5pzg/O4WUKvhyhVN6+D57dy5/1o6pUppmnouLppYmzeH8uWJ+fJLHNavh6goqFULPvxQ081mqXvZSl1iEmJw/9mdqw+usqX3FrrV6pbnMl+kTlNzLe4aF2IucD7mPBdiNT/vJtzVbuNY0pFG9o1o7NCYRvaNuB9zn4C7AQRfD6aERQlGNB3B+FbjcSrlpNfYciPwciA+m3wY1XwUyzyX6a1cQ05xnpqWSt+tfdkesZ11Puvwb+RvkOPklME++3Kddl5D+Tmba1qaEAsWaL6gVqokxLFjuT600eT495WWppm4avZsIVq1EkKh0Hwjr1hRiJEjhdi1S4inT/UfaFqaEFevCrFhgxATJwrRvr0QJUr81zIoXlyIdu2EmDBBs83Vq5m2Di5duiSESiXExo1CNGmi2f+NN4SYN0+IR4/yHOqDZw+EywoXYfqFaZ6+FccnxYsjfx8RS08tFe/sfEe0+LGFsJ5lrW0VmM0wEw2XNxSDdgwSC44tECE3QrTzXL3o+Xt87u45MWDbAGH6hakwm2Em/Lf7i/CY8FzHl1c34m6I0l+WFs1+aCaSVEl6LdvQ808lqZJEx7UdhekXpmLn5Z0GPVZ2GeqzTyaIPO578qQQVasKYWoqxJdf6rXXwuDy/I8UGyvE6tVC+Pr+94FtbS2El5cQP/wgxK1bOS8zLU2IqCghtmwR4qOPhOjYUbMG9/NkYGWlSU6jRwuxZo0QFy8KkZqa7eLT1TktTYgDB/6bKr1kSSEmTxbi9u2cx/2Cx0mPRfvV7YViukL8+MePWW6blpYm/n70t9h5eaeYcWiG8N3kK5y/c9YmAqYjynxZRnRY00GM3zterDm3Rpy7ey7bH6ovv8dRD6PE+L3jRfHZxQXTER7rPcTBGwfzdUrzRFWiaPZDM1FqbilxPe663svPjwkK45PihcsKF2E1y0qERoUa/Hi6yASRDcZaD+LRIyF699Z8xiiVms/NgkCv/0hJSUIEBwsxZowQVar894HetKkQU6cKcfp0xtnz9m3NeMDnnwvh4SFEuXL/7WturhlHGDlSM/Zx7pxmnCEPMq3zH38I0a+fZpzC3FyzrkYefj9PU54Kj/UegumIhccWCiE03zzP3T0nVp9bLcbvHS/eWvOWKPNlmXTJoPqi6sJ3k6+YGTpT7Ly8U9x8dDNPH96Z1ffBswdidthsYTffTjAd0eyHZmLTn5uESq3K9bGya9TuUYLpiICIAIOUn18z2P7z9B9RZ0kdUXJuSXH2ztl8OWZmZILIBmMuGJSWJsT332u+4Do4CBESkqfi8oXB/pHS0oT4808h5s4Vok2b/waHHRyEGD5ciOnThfD21nTtPE8GpqZCNGyo+WBetkyIU6c0SUfPdNb5xg0h3n9f0xICzcD80aO5OlaSKkn4bvIVTEfUWlxLmM0w0yYC61nWosWPLcSInSPE0lNLxdGbR0V8UnyujpMVXfVNVCWKFWdWiJqLawqmI6p+W1UsOblEPE0xQFehEOKX8F8E0xGT9k8ySPlCCHHp4kWDlf2y6MfRotI3lYTdfDtx9f5V3TsYiEwQ2fA6rCgXHi5E7dqa7vkpUzTd3a+rfFsr4J9/hFi3TtPMKllS88upU0cIf38hvvtO8wFsiLGLDGS7zvfuCTFtmmZdDdAkusDAHPchqtQqMTl4suiyvov4+MDH4tf//Soi/okQqersd4vlRXbrq05Tix0RO0Trla0F0xFl55UV036fJu4l3NNfLPcuieKzi4u2q9rqd22N1FTNIOAnnwhRr55Qm5sL4eKi6Yb8+WfNmJQBu9Au/3NZlP+qvKj8TWWx9eJW8b/Y/4lnKdlb1VFf5FlM2ZBfZzHp8vQpjB4Na9ZAu3awYYNmeYbXjVEWtFepNNcmlCihe1sDyHGdnz7VnKa2YAH8/TfUqaM586l/f72c+WRouXmPj948ylfHvmLnlZ1YmVkxtPFQJraeiLOtc67jeJrylBYrW/DP0384N/IcFUtWzHVZACQkaGYp3rVLc4bdP/+AqSm0b0+coyO2N29qzmx7+u8cSra2mjlzWrbU3Fq0gLJl8xbDC87ePUvHdR21s/wqUOBUyomaZWtS07YmNcrW0NwvW5MqpatgZqLfxTwN9dknE4Qe9s3M+vXw7rtgZaVJFl5eei0+z4ySIIws13VOTYUtW2DePM1V5hUqwPjxmtkcS5bUe5z6kqf/iX8iWHB8AT+H/0xqWiq+dXz58M0PcanokqNyhBC8HfA268PXE+wfTKdqnXIVD7duaZLBzp1w8KDmi0apUpqFsby9wcMDypT5r86pqZpJ1E6e/O928eJ/MwRUr/5fwmjZUnNxqIVF7mJDkwSvPLjC1QdXtbdrcde4cv8Kj5Mfa7czMzGjWplq2uRRs+x/CaSCTQVMFDm/flkmiGx43RIEwNWr0LevZobYiRM1UyHl4W9Qr2SCyAUhNN9cv/oKQkI0yWHUKBg3TnPJ/WtGH+/x3Sd3WXRyEcvPLOdx8mPeqvIWH775IV2qd8nWtBg//vEjI3aPYLrrdKa9lYOrkIXQXNuyc6empXD2rOb5atWgWzfNrW1bzcRpL8iyzk+eaFoWLyaNu/9eR2Jhobmm58WkUa1anq/xEUJw/9l9bcJ4MYFExkWSmPrfdNHFzItRw7aGJmH8mzyeJ5Cy1mUz/X3LBJENr2OCAM21W5MmwdKlmuu2fv1V83dnbDJB5NEff8D8+ZqWhZmZZu3aSZM0F+C9JvRZ3yfJT/jx7I98c+IbbsXfor5dfSa1noRfAz8sTDP+1nM+5jytVraiXeV27BuwD1MT06wPkpSkaR3s2qW53b6t+YBu3VqTELy9Nd18WXxo56jOQmhaJs+TxYkTmvf1+Rz/5cq92jWlx6ti00Qat+Nvp2txPL9/4+EN1OK/yQHLWJX5L2HY1kiXPKKvR8sEocvrmiCe275dMyeeEJoZt/v0MejhdJIJQk+uX4eFCzVjFcnJmskQJ0/WfKgZg0ql+aYcH8/1iAic27TRazeYSq3i1z9/Zf6x+fzv3v+oaFOR8a3GM6LZCEpa/necx0mPabaiGYmpiZwbeQ674nYZF3jvnmZagp07Na2zp0+heHHNTJne3pouJLtM9s1Ant9jlQr+/FOTME6d0vyMiPiva6pmzfStjIYNDdItoFKriHoU9UqX1dUHV4mOj063rWclT3YP2Z2r48gEYeB9cyIqCvz8NF9URo7ULHFqbW3ww2ZIJgg9++cfWLJEc4uL05yhMHmy5gPOREe/clqa5oMxPl5z+/cDPluPX34to4kVbWw0Z0o4OoKT03/3X3yuVKkcdacIIdh/fT/zj83n4F8HKWlZknebvcu4VuN4o8Qb9NrSi8DLgRwafIi2ldq+uKNmbOB519GJE5rnHB01CaFbN3jrLc3gXS4Y5D1+/FjTNXXixH+tjefT41taQtOmmmTh5KRpYZQpA6VLp79vY6O3KWmeqZ5xPe66NnEUSyrGOPfcLc8qE4SB980plUozYepXX2m6rWvU0PysUEHz8/nt+eMc/t9mm0wQBpKQ8N+ZTzdvQt264Oam+SDP7IM9ISF706tbWmpaAyVLaj5wnt9/+fG/92/fu0dFU1NNN8qtWxAdrfl59+6rxyte/NWk8XIisbXN8I/xjzt/MP/YfLZc2oKpwpQ2ldpwKOoQ893nM+nNSZo/+rAwTULYuRP++kuzY7Nm/3UdNW6slz/0fHmPhdCc1fbiWMbZs1nPemxqqkkULyeO7NwvXVrTjZkJOQaRDQUlQTwXHKz5HLlzR/P/evfuf2flvcjKKuME8vJzZcvm7P9LJggDU6lg82ZNorhxQ5Pps/pQz+RDPt3jHHZlZFpflQpiYtInjZeTyJ07mpbNi6ytM259/Hs/qkQqX19dw6rzq+n1RkfWWvZDsWuXZuWtx481Ca5Tp/8meqyYx9Ndc1JnQ0tL0yT7R4/g4UPNLSf3U3SsWFeiRKYJJLpmTZxGjcpV2EZLEGFhYcyePZu0tDR69+7NiBEj0r0uhGD27NmEhoZiZWXFl19+Sb169UhOTmbAgAGkpKSgVqtRKpWMHTtW5/EKWoLIyJMn6RPG3bsZP46Pf3Vfc/OsE8jzx+XLa3o8Xpc656eiVuc81Tc1VbOcYkZJ5Plzd+5otnuRpSVpbzigiL6FQq3WjB94eWlaCp06aVoqBlQg32MhNAPjz5NFDpNMfOvWlAwOztWhs/p96fdqjReo1WpmzJjB6tWrsbe3p1evXri5uVG9enXtNmFhYURFRREcHMyFCxeYPn06W7ZswcLCgrVr11K8eHFUKhX9+/enffv2NG7c2FDhvjZsbDQnweg6EebZs8wTyN27cO2apkUfF/fqvqammv9ZExPnl88QLPRUqqJV59RUZ2xsNK1Qa2vNLbP7r75mhrV1Rc3NtiVWFcDa9aXtLNRYP7mHWcwtFLf/SyQmt29DlSqapODiAiYmCKEZw098qPksTErS/Mzqfm5ee/q0IL7HCqDYv7cK2d/NFCgHrcvFscEAURksQYSHh1O5cmWcnDTzznt6ehISEpIuQYSEhODj44NCoaBx48bEx8dz79497OzsKP7vt4zU1FRSU1Nfu2UIja1YMXB21tyykpys6Ul4OYnExMD9+88oVeo1uSgjnzx+XLTq/PBhIlZWFuk+VB8+zPhDVlcPR8ZMgTcwMXkDa2uXdMlDfQKS1qQ/Tl5kldxKlQJ7e839xMRnlC5ddN5jgBo1DLPio8ESRGxsLA4ODtrH9vb2hIeHZ7mNg4MDsbGx2NnZoVar6dmzJzdv3qR///40atRI5zGTk5OJiMjd4vFJSUm53rcgeD7O9WJLMikpCatcnilSUBW1Ouekvmo1JCcrSEkxISlJQVKSguRkE5KTNT81j1+8//w1BUlJr25nYgJWVgJLyzQsLQVWVpqfmlvaS6/9d//5a1ZWAguLtH9/imyPrxW19xief3491r1hDhksQWQ0tPFyKyCrbUxNTQkMDCQ+Pp7333+fq1evUrNmzSyPaWlpWeDHIPKTrHPhV9TqC7LOudk3Mzmf9CObHBwciImJ0T5+3jLIapuYmJhXtilZsiQtW7bk8OHDhgpVkiRJyoDBEkSDBg2IiooiOjqalJQUgoKCcHNzS7eNm5sbAQEBCCE4f/48NjY22NnZERcXR/y/p+kkJSVx7Ngxqr0Oc1NIkiQVIQbrYjIzM2Pq1KkMHz4ctVqNr68vNWrUYOPGjQD4+fnh6upKaGgo7u7uWFtbM2fOHADu3bvHxx9/jFqtRgiBh4cHHTp0MFSokiRJUgYMliAAXF1dcXV1Tfecn5+f9r5CoWDatFdnd6xduzYBAQGGDE2SJEnSwWBdTJIkSVLBJhOEJEmSlCGZICRJkqQMyQQhSZIkZahQzeZ6/vx5LAvAQvKSJEmvi+Tk5EznuStUCUKSJEnSH9nFJEmSJGVIJghJkiQpQzJBSJIkSRmSCUKSJEnKkEwQkiRJUoZkgpAkSZIyVOQTRFhYGEqlEnd3d1asWGHscAzu7t27+Pv706VLFzw9PVm7dq2xQ8o3arUaHx8fRo4caexQ8kV8fDxjx47Fw8ODLl26cO7cOWOHZHBr1qzB09MTLy8vJk6cSHJysrFD0rtPPvmE1q1b4+XlpX3u0aNHDBkyhM6dOzNkyBAeP9bP6nJFOkGo1WpmzJjBypUrCQoKYvfu3URGRho7LIMyNTXl448/Zu/evWzatIkNGzYU+jo/t27dOpx1LeJdiMyePZt27dqxb98+AgMDC33dY2NjWbduHdu2bWP37t2o1WqCgoKMHZbe9ezZk5UrV6Z7bsWKFbRu3Zrg4GBat26tty+7RTpBhIeHU7lyZZycnLCwsMDT05OQkBBjh2VQdnZ21KtXD4ASJUpQrVo1YmNjjRyV4cXExHDo0CF69epl7FDyRUJCAqdPn9bW18LCgpIlSxo5KsNTq9UkJSWRmppKUlLSKytUFgYuLi6UKlUq3XMhISH4+PgA4OPjw2+//aaXYxXpBBEbG4uDg4P2sb29fZH4sHzu1q1bRERE0KhRI2OHYnBz5szhww8/xMSkaPzJR0dHY2tryyeffIKPjw+fffYZz549M3ZYBmVvb8/QoUPp0KEDbdu2pUSJErRt29bYYeWLBw8eaJPh81U59aFo/LdkIqNZRhQKhREiyX9Pnz5l7NixfPrpp5QoUcLY4RjU77//jq2tLfXr1zd2KPkmNTWVS5cu4efnR0BAANbW1oV+jO3x48eEhIQQEhLC4cOHSUxMJDAw0NhhFWhFOkE4ODgQExOjfRwbG1som6QvU6lUjB07Fm9vbzp37mzscAzu7NmzHDx4EDc3NyZOnMiJEyeYNGmSscMyKAcHBxwcHLStQw8PDy5dumTkqAzr2LFjODo6Ymtri7m5OZ07dy4SA/MAZcuW5d69e4BmyWZbW1u9lFukE0SDBg2IiooiOjqalJQUgoKCcHNzM3ZYBiWE4LPPPqNatWoMGTLE2OHkiw8++ICwsDAOHjzIwoULadWqFV9//bWxwzKo8uXL4+DgwI0bNwA4fvx4oR+krlChAhcuXCAxMREhRJGo83Nubm7aZZoDAgLo2LGjXso16JrUrzszMzOmTp3K8OHDUavV+Pr6UqNGDWOHZVB//PEHgYGB1KxZk+7duwMwceLEV9YOlwq+KVOmMGnSJFQqFU5OTsydO9fYIRlUo0aNUCqV9OjRAzMzM+rUqUPfvn2NHZbeTZw4kVOnTvHw4UPat2/PmDFjGDFiBOPHj2fr1q288cYbfPfdd3o5lpzuW5IkScpQke5ikiRJkjInE4QkSZKUIZkgJEmSpAzJBCFJkiRlSCYISZIkKUNF+jRXSbp//z5z587l/PnzlCpVCnNzc4YPH467u3u+x3Ly5EnMzc1p2rQpABs3bsTa2lo7x44k5TeZIKQiSwjB+++/j4+PDwsWLADg9u3bHDx40GDHTE1Nxcws43+7U6dOUaxYMW2C8PPzM1gckpQd8joIqcg6fvw4S5cuZf369a+8plar+frrrzl16hQpKSkMGDCAfv36cfLkSZYsWUKZMmW4evUq9erV4+uvv0ahUPDnn3/y5Zdf8uzZM8qUKcPcuXOxs7PD39+fJk2acPbsWdzc3KhSpQrLly9HpVJRunRpvv76a5KSkujbty8mJibY2toyZcoUjh8/TrFixRg2bBgRERFMmzaNxMREKlWqxJw5cyhVqhT+/v40bNiQkydP8uTJE2bPnk3z5s2N8NuUCiM5BiEVWdeuXaNu3boZvrZ161ZsbGzYtm0b27ZtY/PmzURHRwNw6dIlPv30U/bs2cOtW7f4448/UKlUzJo1i0WLFrF9+3Z8fX355ptvtOXFx8ezfv16hg4dSrNmzdi8eTMBAQF4enqycuVKHB0d6devH4MHDyYwMPCVD/nJkyczadIkdu3aRc2aNVmyZIn2NbVazdatW/n000/TPS9JeSW7mCTpX1988QV//PEH5ubmVKxYkStXrrB//34Anjx5wt9//425uTkNGzbUThNfu3Ztbt++TcmSJbl69ap2fqu0tDTKly+vLbtr167a+zExMUyYMIF//vmHlJQUHB0ds4zryZMnPHnyhBYtWgDQo0cPxo0bp339+XhJvXr1uH37th5+E5KkIROEVGTVqFGD4OBg7eNp06YRFxdHr169qFChAp9//jnt2rVLt8/JkyexsLDQPjY1NUWtViOEoEaNGmzatCnDY1lbW2vvz5o1i8GDB9OxY0dtl1VePI/HxMQEtVqdp7Ik6UWyi0kqslq1akVycjIbNmzQPpeUlARA27Zt2bhxIyqVCoC//vorywV3qlatSlxcnHZ6aZVKxbVr1zLc9smTJ9jb2wNoZ+AEKF68OE+fPn1lexsbG0qWLMmZM2cACAwMxMXFJQc1laTckS0IqchSKBQsXbqUuXPnsnLlSmxtbbG2tmbSpEl4eHhw+/ZtevbsiRCCMmXKsGzZskzLsrCwYNGiRcyaNYsnT56gVqt5++23M5wdePTo0YwbNw57e3saNWrErVu3AOjQoQNjx44lJCSEKVOmpNtn3rx52kHqojAzq/R6kGcxSZIkSRmSXUySJElShmSCkCRJkjIkE4QkSZKUIZkgJEmSpAzJBCFJkiRlSCYISZIkKUMyQUiSJEkZ+j+a9ZS9jexxMAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 40.47993821700414 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 2 , Number of neurons: 50\n",
      "Batch size 4 , Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030002</td>\n",
       "      <td>0.030002</td>\n",
       "      <td>83.453001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030320</td>\n",
       "      <td>0.030320</td>\n",
       "      <td>74.933830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030547</td>\n",
       "      <td>0.030547</td>\n",
       "      <td>74.438802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030679</td>\n",
       "      <td>0.030679</td>\n",
       "      <td>57.372939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030830</td>\n",
       "      <td>0.030830</td>\n",
       "      <td>80.553833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030846</td>\n",
       "      <td>0.030846</td>\n",
       "      <td>64.300443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030924</td>\n",
       "      <td>0.030924</td>\n",
       "      <td>63.583484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030935</td>\n",
       "      <td>0.030935</td>\n",
       "      <td>78.718075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031090</td>\n",
       "      <td>0.031090</td>\n",
       "      <td>76.580060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031241</td>\n",
       "      <td>0.031241</td>\n",
       "      <td>116.280593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031744</td>\n",
       "      <td>0.031744</td>\n",
       "      <td>75.143378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031876</td>\n",
       "      <td>0.031876</td>\n",
       "      <td>63.544669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032057</td>\n",
       "      <td>0.032057</td>\n",
       "      <td>57.486081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.032101</td>\n",
       "      <td>0.032101</td>\n",
       "      <td>83.470173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033254</td>\n",
       "      <td>0.033254</td>\n",
       "      <td>66.179454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033646</td>\n",
       "      <td>0.033646</td>\n",
       "      <td>67.117207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034760</td>\n",
       "      <td>0.034760</td>\n",
       "      <td>144.229294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035379</td>\n",
       "      <td>0.035379</td>\n",
       "      <td>143.498072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036492</td>\n",
       "      <td>0.036492</td>\n",
       "      <td>65.968525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037253</td>\n",
       "      <td>0.037253</td>\n",
       "      <td>83.218872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037324</td>\n",
       "      <td>0.037324</td>\n",
       "      <td>62.999491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037754</td>\n",
       "      <td>0.037754</td>\n",
       "      <td>83.386122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038149</td>\n",
       "      <td>0.038149</td>\n",
       "      <td>57.466406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.038668</td>\n",
       "      <td>0.038668</td>\n",
       "      <td>35.096771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.039376</td>\n",
       "      <td>0.039376</td>\n",
       "      <td>35.119632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.042507</td>\n",
       "      <td>0.042507</td>\n",
       "      <td>83.982273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.043366</td>\n",
       "      <td>0.043366</td>\n",
       "      <td>83.519129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.046342</td>\n",
       "      <td>0.046342</td>\n",
       "      <td>41.703971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.052833</td>\n",
       "      <td>0.052833</td>\n",
       "      <td>42.174787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.054593</td>\n",
       "      <td>0.054593</td>\n",
       "      <td>27.837313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.057216</td>\n",
       "      <td>0.057216</td>\n",
       "      <td>24.819952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.069997</td>\n",
       "      <td>0.069997</td>\n",
       "      <td>55.836687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>42.733980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.090689</td>\n",
       "      <td>0.090689</td>\n",
       "      <td>131.790348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             3        100         0.0001           4  0.030002  0.030002   \n",
       "1             3        100         0.0001           4  0.030320  0.030320   \n",
       "2             3        100         0.0001           4  0.030547  0.030547   \n",
       "3             3        100         0.0001           4  0.030679  0.030679   \n",
       "4             3        100         0.0001           4  0.030830  0.030830   \n",
       "5             3        100         0.0001           4  0.030846  0.030846   \n",
       "6             3        100         0.0001           4  0.030924  0.030924   \n",
       "7             3        100         0.0001           4  0.030935  0.030935   \n",
       "8             3        100         0.0001           4  0.031090  0.031090   \n",
       "9             2        100         0.0001           2  0.031241  0.031241   \n",
       "10            3        100         0.0001           4  0.031744  0.031744   \n",
       "11            3        100         0.0001           4  0.031876  0.031876   \n",
       "12            3        100         0.0001           4  0.032057  0.032057   \n",
       "13            4        200         0.0001           8  0.032101  0.032101   \n",
       "14            3        100         0.0001           4  0.033254  0.033254   \n",
       "15            3        100         0.0001           4  0.033646  0.033646   \n",
       "16            4        100         0.0001           2  0.034760  0.034760   \n",
       "17            3        200         0.0001           4  0.035379  0.035379   \n",
       "18            2        200         0.0001           4  0.036492  0.036492   \n",
       "19            2        100         0.0001           4  0.037253  0.037253   \n",
       "20            2        100         0.0001           4  0.037324  0.037324   \n",
       "21            3         50         0.0001           4  0.037754  0.037754   \n",
       "22            2        100         0.0001           4  0.038149  0.038149   \n",
       "23            4        200         0.0001          16  0.038668  0.038668   \n",
       "24            3        200         0.0001          16  0.039376  0.039376   \n",
       "25            4        100         0.0001           8  0.042507  0.042507   \n",
       "26            3        100         0.0010           4  0.043366  0.043366   \n",
       "27            4        100         0.0001           8  0.046342  0.046342   \n",
       "28            2        200         0.0001          16  0.052833  0.052833   \n",
       "29            2        200         0.0001          16  0.054593  0.054593   \n",
       "30            2        200         0.0010          16  0.057216  0.057216   \n",
       "31            1        100         0.0001           4  0.069997  0.069997   \n",
       "32            4        200         0.0010          16  0.070300  0.070300   \n",
       "33            3        100         0.0010           2  0.090689  0.090689   \n",
       "\n",
       "    Elapsed time  \n",
       "0      83.453001  \n",
       "1      74.933830  \n",
       "2      74.438802  \n",
       "3      57.372939  \n",
       "4      80.553833  \n",
       "5      64.300443  \n",
       "6      63.583484  \n",
       "7      78.718075  \n",
       "8      76.580060  \n",
       "9     116.280593  \n",
       "10     75.143378  \n",
       "11     63.544669  \n",
       "12     57.486081  \n",
       "13     83.470173  \n",
       "14     66.179454  \n",
       "15     67.117207  \n",
       "16    144.229294  \n",
       "17    143.498072  \n",
       "18     65.968525  \n",
       "19     83.218872  \n",
       "20     62.999491  \n",
       "21     83.386122  \n",
       "22     57.466406  \n",
       "23     35.096771  \n",
       "24     35.119632  \n",
       "25     83.982273  \n",
       "26     83.519129  \n",
       "27     41.703971  \n",
       "28     42.174787  \n",
       "29     27.837313  \n",
       "30     24.819952  \n",
       "31     55.836687  \n",
       "32     42.733980  \n",
       "33    131.790348  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla2.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 40.476 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
