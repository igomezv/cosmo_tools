{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 16:43:36.637893: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-16 16:43:36.726779: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:36.726795: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-16 16:43:37.335159: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:37.335220: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:37.335227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,5e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.8         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 16:43:38.342574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-16 16:43:38.342766: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:38.342820: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:38.342867: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:38.342911: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:38.342955: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:38.343000: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:38.343043: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:38.343086: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:38.343093: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-16 16:43:38.343287: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1135 - mean_squared_error: 0.1135\n",
      "Loss: 0.11350271105766296 , Elapsed time: 100.66146445274353\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0571 - mean_squared_error: 0.0571\n",
      "Loss: 0.05705392733216286 , Elapsed time: 22.01892924308777\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0537 - mean_squared_error: 0.0537\n",
      "Loss: 0.053685735911130905 , Elapsed time: 26.979498863220215\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0710 - mean_squared_error: 0.0710\n",
      "Loss: 0.07099916785955429 , Elapsed time: 82.88928127288818\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Loss: 0.03611806780099869 , Elapsed time: 51.16672086715698\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax     \n",
      "0  \t5     \t0.0361181\t0.0662719\t0.113503\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0394 - mean_squared_error: 0.0394\n",
      "Loss: 0.039375003427267075 , Elapsed time: 25.216501712799072\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0681 - mean_squared_error: 0.0681\n",
      "Loss: 0.06812557578086853 , Elapsed time: 79.45700335502625\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0480 - mean_squared_error: 0.0480\n",
      "Loss: 0.04796797037124634 , Elapsed time: 28.694381713867188\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0524 - mean_squared_error: 0.0524\n",
      "Loss: 0.05235118791460991 , Elapsed time: 22.486284971237183\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.0361181\t0.0487876\t0.0681256\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0556 - mean_squared_error: 0.0556\n",
      "Loss: 0.05559363216161728 , Elapsed time: 42.91843771934509\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Loss: 0.034937698394060135 , Elapsed time: 83.0978832244873\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0687 - mean_squared_error: 0.0687\n",
      "Loss: 0.0687490701675415 , Elapsed time: 42.27655839920044\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0612 - mean_squared_error: 0.0612\n",
      "Loss: 0.06124423071742058 , Elapsed time: 25.84891366958618\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t4     \t0.0349377\t0.0513285\t0.0687491\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0432 - mean_squared_error: 0.0432\n",
      "Loss: 0.043200891464948654 , Elapsed time: 54.53551197052002\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Loss: 0.03624894097447395 , Elapsed time: 53.826826333999634\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0397 - mean_squared_error: 0.0397\n",
      "Loss: 0.03972786292433739 , Elapsed time: 25.34658122062683\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t3     \t0.0349377\t0.0380467\t0.0432009\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0429 - mean_squared_error: 0.0429\n",
      "Loss: 0.04288933798670769 , Elapsed time: 53.939300775527954\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0733 - mean_squared_error: 0.0733\n",
      "Loss: 0.07325368374586105 , Elapsed time: 22.472185134887695\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 1 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1000 - mean_squared_error: 0.1000\n",
      "Loss: 0.0999845489859581 , Elapsed time: 48.6844482421875\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Loss: 0.035814058035612106 , Elapsed time: 57.13711738586426\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t4     \t0.0349377\t0.0573759\t0.0999845\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0394 - mean_squared_error: 0.0394\n",
      "Loss: 0.0394299142062664 , Elapsed time: 58.23551273345947\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0603 - mean_squared_error: 0.0603\n",
      "Loss: 0.060284484177827835 , Elapsed time: 60.105440616607666\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Loss: 0.029808340594172478 , Elapsed time: 143.14339470863342\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Loss: 0.03625805303454399 , Elapsed time: 63.08348989486694\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.0298083\t0.0401437\t0.0602845\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0384 - mean_squared_error: 0.0384\n",
      "Loss: 0.038445767015218735 , Elapsed time: 62.358652114868164\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Loss: 0.034564610570669174 , Elapsed time: 65.28889179229736\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Loss: 0.035469796508550644 , Elapsed time: 56.99921369552612\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0447 - mean_squared_error: 0.0447\n",
      "Loss: 0.04472421854734421 , Elapsed time: 76.38389706611633\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t4     \t0.0298083\t0.0366025\t0.0447242\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Loss: 0.035543955862522125 , Elapsed time: 63.7260799407959\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0567 - mean_squared_error: 0.0567\n",
      "Loss: 0.056701432913541794 , Elapsed time: 52.40444755554199\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0418 - mean_squared_error: 0.0418\n",
      "Loss: 0.04178904742002487 , Elapsed time: 99.75653529167175\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Loss: 0.032860998064279556 , Elapsed time: 120.70562076568604\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.0298083\t0.0393408\t0.0567014\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Loss: 0.03742591291666031 , Elapsed time: 61.026187896728516\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0724 - mean_squared_error: 0.0724\n",
      "Loss: 0.07242921739816666 , Elapsed time: 129.70709109306335\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Loss: 0.029905306175351143 , Elapsed time: 112.81131339073181\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Loss: 0.03700922429561615 , Elapsed time: 83.36412239074707\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t4     \t0.0298083\t0.0413156\t0.0724292\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0491 - mean_squared_error: 0.0491\n",
      "Loss: 0.049086712300777435 , Elapsed time: 21.414051055908203\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Loss: 0.033218834549188614 , Elapsed time: 71.76145648956299\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1010 - mean_squared_error: 0.1010\n",
      "Loss: 0.10100553929805756 , Elapsed time: 137.1363570690155\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0444 - mean_squared_error: 0.0444\n",
      "Loss: 0.04435298964381218 , Elapsed time: 27.706372499465942\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t4     \t0.0298083\t0.0514945\t0.101006 \n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Loss: 0.03360579535365105 , Elapsed time: 130.56702852249146\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 42 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Loss: 0.03673791140317917 , Elapsed time: 36.28482961654663\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t2     \t0.0298083\t0.0358094\t0.0490867\n",
      "-- Best Individual =  [0, 1, 1, 1, 0, 0, 0]\n",
      "-- Best Fitness =  0.029808340594172478\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 44.731496743361156 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 1 , Number of neurons: 100\n",
      "Batch size 4 , Learning rate: 0.005\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029808</td>\n",
       "      <td>0.029808</td>\n",
       "      <td>143.143395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029905</td>\n",
       "      <td>0.029905</td>\n",
       "      <td>112.811313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032861</td>\n",
       "      <td>0.032861</td>\n",
       "      <td>120.705621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033219</td>\n",
       "      <td>0.033219</td>\n",
       "      <td>71.761456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033606</td>\n",
       "      <td>0.033606</td>\n",
       "      <td>130.567029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034565</td>\n",
       "      <td>0.034565</td>\n",
       "      <td>65.288892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034938</td>\n",
       "      <td>0.034938</td>\n",
       "      <td>83.097883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035470</td>\n",
       "      <td>0.035470</td>\n",
       "      <td>56.999214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035544</td>\n",
       "      <td>0.035544</td>\n",
       "      <td>63.726080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035814</td>\n",
       "      <td>0.035814</td>\n",
       "      <td>57.137117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036118</td>\n",
       "      <td>0.036118</td>\n",
       "      <td>51.166721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036249</td>\n",
       "      <td>0.036249</td>\n",
       "      <td>53.826826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036258</td>\n",
       "      <td>0.036258</td>\n",
       "      <td>63.083490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.036738</td>\n",
       "      <td>0.036738</td>\n",
       "      <td>36.284830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037009</td>\n",
       "      <td>0.037009</td>\n",
       "      <td>83.364122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037426</td>\n",
       "      <td>0.037426</td>\n",
       "      <td>61.026188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038446</td>\n",
       "      <td>0.038446</td>\n",
       "      <td>62.358652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.039375</td>\n",
       "      <td>0.039375</td>\n",
       "      <td>25.216502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.039430</td>\n",
       "      <td>0.039430</td>\n",
       "      <td>58.235513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.039728</td>\n",
       "      <td>0.039728</td>\n",
       "      <td>25.346581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.041789</td>\n",
       "      <td>0.041789</td>\n",
       "      <td>99.756535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.042889</td>\n",
       "      <td>0.042889</td>\n",
       "      <td>53.939301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.043201</td>\n",
       "      <td>0.043201</td>\n",
       "      <td>54.535512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.044353</td>\n",
       "      <td>0.044353</td>\n",
       "      <td>27.706372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.044724</td>\n",
       "      <td>0.044724</td>\n",
       "      <td>76.383897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.047968</td>\n",
       "      <td>0.047968</td>\n",
       "      <td>28.694382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.049087</td>\n",
       "      <td>0.049087</td>\n",
       "      <td>21.414051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.052351</td>\n",
       "      <td>0.052351</td>\n",
       "      <td>22.486285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.053686</td>\n",
       "      <td>0.053686</td>\n",
       "      <td>26.979499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.055594</td>\n",
       "      <td>0.055594</td>\n",
       "      <td>42.918438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>52.404448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.057054</td>\n",
       "      <td>0.057054</td>\n",
       "      <td>22.018929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.060284</td>\n",
       "      <td>0.060284</td>\n",
       "      <td>60.105441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.061244</td>\n",
       "      <td>0.061244</td>\n",
       "      <td>25.848914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.068126</td>\n",
       "      <td>0.068126</td>\n",
       "      <td>79.457003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.068749</td>\n",
       "      <td>0.068749</td>\n",
       "      <td>42.276558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.070999</td>\n",
       "      <td>0.070999</td>\n",
       "      <td>82.889281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.072429</td>\n",
       "      <td>0.072429</td>\n",
       "      <td>129.707091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.073254</td>\n",
       "      <td>0.073254</td>\n",
       "      <td>22.472185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.099985</td>\n",
       "      <td>0.099985</td>\n",
       "      <td>48.684448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.101006</td>\n",
       "      <td>0.101006</td>\n",
       "      <td>137.136357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.113503</td>\n",
       "      <td>0.113503</td>\n",
       "      <td>100.661464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             2        200         0.0001           2  0.029808  0.029808   \n",
       "1             2        200         0.0001           2  0.029905  0.029905   \n",
       "2             2        200         0.0001           2  0.032861  0.032861   \n",
       "3             4        200         0.0001           4  0.033219  0.033219   \n",
       "4             4        200         0.0001           2  0.033606  0.033606   \n",
       "5             2        200         0.0001           4  0.034565  0.034565   \n",
       "6             2        200         0.0001           4  0.034938  0.034938   \n",
       "7             2        200         0.0001           4  0.035470  0.035470   \n",
       "8             2        200         0.0001           4  0.035544  0.035544   \n",
       "9             2        200         0.0001           4  0.035814  0.035814   \n",
       "10            2        100         0.0001           4  0.036118  0.036118   \n",
       "11            2        100         0.0001           4  0.036249  0.036249   \n",
       "12            2        200         0.0001           4  0.036258  0.036258   \n",
       "13            2        200         0.0001           8  0.036738  0.036738   \n",
       "14            2        200         0.0001           4  0.037009  0.037009   \n",
       "15            2        200         0.0001           4  0.037426  0.037426   \n",
       "16            2        150         0.0001           4  0.038446  0.038446   \n",
       "17            4        200         0.0050          16  0.039375  0.039375   \n",
       "18            2         50         0.0001           4  0.039430  0.039430   \n",
       "19            4        200         0.0050          16  0.039728  0.039728   \n",
       "20            1        200         0.0001           2  0.041789  0.041789   \n",
       "21            2         50         0.0001           4  0.042889  0.042889   \n",
       "22            2         50         0.0001           4  0.043201  0.043201   \n",
       "23            2        100         0.0001           8  0.044353  0.044353   \n",
       "24            3        200         0.0050           4  0.044724  0.044724   \n",
       "25            4        200         0.0050          16  0.047968  0.047968   \n",
       "26            2        200         0.0001          16  0.049087  0.049087   \n",
       "27            2        200         0.0001          16  0.052351  0.052351   \n",
       "28            4        200         0.0050          16  0.053686  0.053686   \n",
       "29            4        200         0.0050          16  0.055594  0.055594   \n",
       "30            2        150         0.0050           4  0.056701  0.056701   \n",
       "31            2        200         0.0001          16  0.057054  0.057054   \n",
       "32            2         50         0.0050           4  0.060284  0.060284   \n",
       "33            4        200         0.0050          16  0.061244  0.061244   \n",
       "34            4        200         0.0050           4  0.068126  0.068126   \n",
       "35            4        200         0.0050          16  0.068749  0.068749   \n",
       "36            1        100         0.0001           4  0.070999  0.070999   \n",
       "37            2        200         0.0050           2  0.072429  0.072429   \n",
       "38            2        100         0.0001          16  0.073254  0.073254   \n",
       "39            1         50         0.0001           4  0.099985  0.099985   \n",
       "40            4        200         0.0050           2  0.101006  0.101006   \n",
       "41            3        100         0.0050           2  0.113503  0.113503   \n",
       "\n",
       "    Elapsed time  \n",
       "0     143.143395  \n",
       "1     112.811313  \n",
       "2     120.705621  \n",
       "3      71.761456  \n",
       "4     130.567029  \n",
       "5      65.288892  \n",
       "6      83.097883  \n",
       "7      56.999214  \n",
       "8      63.726080  \n",
       "9      57.137117  \n",
       "10     51.166721  \n",
       "11     53.826826  \n",
       "12     63.083490  \n",
       "13     36.284830  \n",
       "14     83.364122  \n",
       "15     61.026188  \n",
       "16     62.358652  \n",
       "17     25.216502  \n",
       "18     58.235513  \n",
       "19     25.346581  \n",
       "20     99.756535  \n",
       "21     53.939301  \n",
       "22     54.535512  \n",
       "23     27.706372  \n",
       "24     76.383897  \n",
       "25     28.694382  \n",
       "26     21.414051  \n",
       "27     22.486285  \n",
       "28     26.979499  \n",
       "29     42.918438  \n",
       "30     52.404448  \n",
       "31     22.018929  \n",
       "32     60.105441  \n",
       "33     25.848914  \n",
       "34     79.457003  \n",
       "35     42.276558  \n",
       "36     82.889281  \n",
       "37    129.707091  \n",
       "38     22.472185  \n",
       "39     48.684448  \n",
       "40    137.136357  \n",
       "41    100.661464  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla2.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 44.727 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
