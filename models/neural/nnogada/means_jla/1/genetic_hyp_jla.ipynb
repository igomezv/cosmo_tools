{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 00:38:41.958985: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-18 00:38:42.135418: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:42.135449: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-18 00:38:43.323365: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:43.323497: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:43.323510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,1e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.5         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 00:38:44.583872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 00:38:44.584175: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:44.584275: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:44.584333: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:44.584415: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:44.584511: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:44.584603: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:44.584684: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:44.584770: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:38:44.584782: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-18 00:38:44.586186: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0949 - mean_squared_error: 0.0949\n",
      "Loss: 0.09487788379192352 , Elapsed time: 203.49836683273315\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0520 - mean_squared_error: 0.0520\n",
      "Loss: 0.05197332054376602 , Elapsed time: 42.20120024681091\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0707 - mean_squared_error: 0.0707\n",
      "Loss: 0.07068824023008347 , Elapsed time: 83.86995530128479\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0857 - mean_squared_error: 0.0857\n",
      "Loss: 0.08572287857532501 , Elapsed time: 70.82545304298401\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Loss: 0.03739956021308899 , Elapsed time: 85.1724100112915\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax      \n",
      "0  \t5     \t0.0373996\t0.0681324\t0.0948779\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Loss: 0.031076453626155853 , Elapsed time: 143.6923542022705\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0464 - mean_squared_error: 0.0464\n",
      "Loss: 0.0464208759367466 , Elapsed time: 143.38380193710327\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0578 - mean_squared_error: 0.0578\n",
      "Loss: 0.05780571699142456 , Elapsed time: 37.23748207092285\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Loss: 0.03563762456178665 , Elapsed time: 105.93274664878845\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.0310765\t0.041668 \t0.0578057\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Loss: 0.03308124095201492 , Elapsed time: 144.02566838264465\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Loss: 0.030408620834350586 , Elapsed time: 102.93039345741272\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Loss: 0.035572849214076996 , Elapsed time: 143.44652271270752\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Loss: 0.03689361363649368 , Elapsed time: 93.69676089286804\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t4     \t0.0304086\t0.0334066\t0.0368936\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Loss: 0.03357108682394028 , Elapsed time: 144.03049850463867\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Loss: 0.03543003648519516 , Elapsed time: 107.92514848709106\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Loss: 0.031593888998031616 , Elapsed time: 143.89665007591248\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t3     \t0.0304086\t0.032416 \t0.03543  \n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Loss: 0.0336831696331501 , Elapsed time: 145.2267017364502\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Loss: 0.03274513781070709 , Elapsed time: 142.0882523059845\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Loss: 0.03717746213078499 , Elapsed time: 109.0636830329895\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t3     \t0.0304086\t0.0330182\t0.0371775\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Loss: 0.034339483827352524 , Elapsed time: 263.820187330246\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Loss: 0.033211804926395416 , Elapsed time: 113.5908362865448\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Loss: 0.03360708802938461 , Elapsed time: 110.9442310333252\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Loss: 0.03465813398361206 , Elapsed time: 125.60874581336975\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.0310765\t0.0333786\t0.0346581\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0695 - mean_squared_error: 0.0695\n",
      "Loss: 0.06947235018014908 , Elapsed time: 265.7418625354767\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Loss: 0.033117733895778656 , Elapsed time: 99.56627893447876\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0779 - mean_squared_error: 0.0779\n",
      "Loss: 0.07791207730770111 , Elapsed time: 78.32414364814758\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Loss: 0.032342325896024704 , Elapsed time: 74.99395966529846\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t4     \t0.0310765\t0.0487842\t0.0779121\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0740 - mean_squared_error: 0.0740\n",
      "Loss: 0.07404051721096039 , Elapsed time: 83.15245604515076\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Loss: 0.0344502292573452 , Elapsed time: 92.9367344379425\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Loss: 0.03462075814604759 , Elapsed time: 84.14386415481567\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Loss: 0.0305838193744421 , Elapsed time: 77.16334748268127\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.0305838\t0.0409544\t0.0740405\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Loss: 0.030987519770860672 , Elapsed time: 79.52114152908325\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Loss: 0.03248800337314606 , Elapsed time: 70.07102274894714\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Loss: 0.034068070352077484 , Elapsed time: 70.30802297592163\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t3     \t0.0305838\t0.0318408\t0.0340681\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Loss: 0.030163085088133812 , Elapsed time: 113.26150918006897\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Loss: 0.03189261257648468 , Elapsed time: 66.85148215293884\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t2     \t0.0301631\t0.03086  \t0.0318926\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Loss: 0.03617680445313454 , Elapsed time: 57.48555326461792\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Loss: 0.0311106089502573 , Elapsed time: 101.85804462432861\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Loss: 0.03415700048208237 , Elapsed time: 107.79185009002686\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Loss: 0.032912835478782654 , Elapsed time: 107.13630557060242\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t4     \t0.0301631\t0.0329041\t0.0361768\n",
      "-- Best Individual =  [1, 1, 0, 0, 0, 0, 0]\n",
      "-- Best Fitness =  0.030163085088133812\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABoT0lEQVR4nO3deVhU1RvA8e+w7+6AAqK4ouKO4gqi44Ym7ktqaqYtpuVSarnkXlmWppU/zTTTsjRIsVIxJcc9F1wwVxQ0IMUN2Yf7++PGBAoMy2zA+TwPD8zMvWfey8C8c8+55z0KSZIkBEEQBOEpZsYOQBAEQTBNIkEIgiAIeRIJQhAEQciTSBCCIAhCnkSCEARBEPIkEoQgCIKQJ5Egyqg7d+7QokUL1Gq1sUMhMDCQw4cPGzsMg9qyZQvt27enRYsW3L9/nxYtWhATE2PssAQ9GD9+PD/99JOxw9ALkSCKKDAwkCZNmpCYmJjr/n79+tGgQQNiY2P1+vw7duygQYMGLF26NNf9+/bto0GDBsycOROAGjVqcPr0aczNzfUaj66sWrWKBg0aEBkZaexQSiwjI4Nly5bx1Vdfcfr0aSpVqsTp06fx8PAAYObMmaxYscLIUZqOc+fOMXHiRHx9fWndujW9e/dmxYoVPHz40NihPWPVqlVMnz49133r1q2jf//+RopIv0SCKAY3NzfCwsI0t//66y9SU1MN9vw1a9Zk9+7dZGZmau4LCQmhVq1aBotBlyRJIjQ0lIoVK+rtk5ghz6Tu3btHWloadevWNdhzlgY5/16znTp1itGjR9OyZUt++eUXTp48ybp16zA3N+fSpUtGj6+8EwmiGPr160dISIjmdkhICMHBwbm2OXDgAMHBwbRs2RJ/f39WrVqleWz37t107dqVpKQkAA4ePEiHDh2eOSvJT9WqValfvz6HDh0C4MGDB5w+fZrAwEDNNrGxsTRo0EDzRz9q1Cg++eQThg0bRosWLRg3bly+z/fw4UMmTpyIn58fvr6+TJw4kbi4OM3j2toKCQmhS5cutG3bls8//1zr8Zw8eZKEhARmz57N7t27SU9PB+DFF19k8+bNubZ97rnn2LNnDwDXrl1j7NixtGnThh49erB7927NdjNnzmTevHm89NJLNG/enGPHjhX4mjwd9+rVq3N1jWVlZbF27Vq6detG27ZtmTJlCg8ePHjmWG7cuEHPnj0B8PX1ZfTo0QA0aNCAmzdv8v3337Nz507Wr19PixYtePnllwH5zHT9+vX07duXVq1a8cYbb5CWlqZp9/fff6dfv360bt2aYcOG5XrzXLt2LZ06daJFixb06NGDI0eOABAZGcmAAQNo2bIl7du3f+asM6dt27ahVCpp06YNL7/8MvHx8QDMnTuX999/P9e2r7zyChs2bAAgPj6e119/HT8/PwIDA9m0aZNmu1WrVjF58mSmT59Oy5Yt80z+H374IQMGDGDixIlUrVoVkM9+J0+eTNu2bTXb/fjjj/Tq1QtfX19efPFFbt++rXmsQYMGbN26le7du+Pr68t7771HzgIR2vb99ttv6d69O927dwdg0aJF+Pv707JlSwYMGMDJkycBiIiI4Msvv+SXX36hRYsWPPfcc4D8//DDDz8A8t/JmjVr6NKlC+3ateOtt97i8ePHwH//kz/99BMBAQHP/H8U5fUyGEkoki5dukgqlUrq3r27dPXqVSkzM1Pq3LmzFBsbK9WvX1+KiYmRJEmSjh49Kl26dElSq9VSVFSU1K5dO2nv3r2adqZOnSq9/fbbUmJiotShQwdp//79hXr+7du3S8OGDZN+/vlnacqUKZIkSdLmzZulOXPmSB9//LH09ttvS5IkSTExMVL9+vWljIwMSZIkaeTIkVLXrl2l69evSykpKdLIkSOlDz/8MM/nSExMlH799VcpOTlZevz4sfT6669Lr7zyiubxgtq6cuWK1Lx5c+n48eNSWlqatGTJEsnb21tSqVT5HtOsWbOkyZMnS+np6VKbNm2k3377TZIkSfrpp5+koUOHara7cuWK1KpVKyktLU168uSJ1LlzZ+nHH3+UMjIypPPnz0tt2rSRLl++LEmSJL399ttSy5YtpZMnT0pqtVpKTU0t8DXJjvvEiRNSWlqatGzZMqlRo0aauDds2CANHjxY+vvvv6W0tDRpzpw50ptvvpnn8Tz9u5ckSapfv74UHR2tie3jjz/OtU+XLl2kgQMHSnFxcdL9+/elnj17Slu2bJEkSZLOnz8v+fn5SWfOnJEyMzOlHTt2SF26dJHS0tKka9euSZ07d5bi4uI0z33z5k1JkiRpyJAh0k8//SRJkiQlJSVJp0+fzjPew4cPS23atJHOnz8vpaWlSQsWLJBGjBghSZIkHT9+XOrcubOUlZUlSZIkPXjwQPLx8ZHi4uIktVot9e/fX1q1apWUlpYm3bp1SwoMDJQiIiIkSZKklStXSo0aNZL27t0rqdVqKSUlJdfzPnnyRGrYsKF09OjRPOPKtnfvXqlbt27S1atXpYyMDGn16tW5/i7q168vTZgwQXr48KF0+/ZtqW3bttLBgwcLve+YMWOk+/fva+ILCQmREhMTpYyMDGn9+vVS+/btpdTUVM0xTZs2LVd8I0eOlLZt2yZJkiT98MMPUrdu3aRbt25JSUlJ0muvvSZNnz5d89rUr19feuedd6SUlBQpKipKaty4sXT16tUivV6GJM4giin7LEKlUuHl5YWLi0uux9u2bUuDBg0wMzOjYcOGBAUFcfz4cc3j8+bN4+jRo4wePZrAwEC6dOlSpOdXKpUcP36cx48fExoaSr9+/bTuM2DAAGrXro2NjQ09e/YkKioqz+0qVapEjx49sLW1xcHBgVdeeYUTJ04Uqq1ff/2VgIAAfH19sbKyYsqUKZiZ5f9nlpKSwq+//krfvn2xtLSkR48emk+a3bp149KlS5pPfDt37kSpVGJlZcWBAwdwc3Nj4MCBWFhY0LhxY3r06MFvv/2mabtr1660atUKMzMzrK2tC3xNfv31V7p06ULr1q2xsrJi8uTJKBQKTVvff/89b775Jq6urlhZWTFp0iR+++03nXZLjBo1ChcXFypWrEiXLl00v9Nt27YxdOhQmjVrhrm5Of3798fS0pIzZ85gbm5Oeno6165dIyMjA3d3d2rWrAmAhYUFt27dIjExEXt7e5o3b57n8+7cuZOBAwfSuHFjrKysmDp1KmfOnCE2NpbWrVujUCg0n6J/++03mjdvjouLC+fOnSMxMZFJkyZhZWWFh4cHQ4YMyXUm17x5c7p164aZmRk2Nja5nvfRo0dkZWVpzhwAPvjgA1q3bk3z5s1Zs2YNAN999x0TJkygTp06WFhY8PLLLxMVFZXrTOCll17CycmJGjVq0LZtW80ZVmH2nTBhAhUrVtTE169fPypVqoSFhQXjxo0jPT2dGzduFOo13LlzJ2PGjMHDwwN7e3umTp36THfwpEmTsLGxoWHDhjRs2FATa2FfL0OyMHYApVW/fv0YOXIksbGxeb45nz17luXLl3PlyhUyMjJIT0/XdD0AODk50bNnTzZs2MDKlSuL/Pw2Njb4+/uzZs0a7t+/T6tWrYiIiChwn2rVqml+trW1JTk5Oc/tUlJSWLp0KX/88YdmoPDJkyeo1WrNoHd+bSUkJODq6qp5zM7OjooVK+Yb0969e7GwsKBz584A9O3bl7Fjx5KYmEjlypXx9/cnLCyMCRMmEBYWxsKFCwG4ffs2kZGRtG7dWtOWWq3WnPYDVK9ePddzFfSaPB23ra1trrjv3LnDa6+9livZmZmZce/evWc+HBTX07/ThIQEzXOHhITk6m7LyMggISGBNm3aMHv2bFatWsXVq1fp2LEjM2fOxMXFhcWLF7Ny5Up69eqFu7s7kyZNyvODSEJCAo0bN9bctre3p2LFisTHx+Pu7k7v3r3ZtWsXvr6+7Ny5U/M7vn37NgkJCc+8Bjlv5/ydPs3JyQkzMzP++ecf6tSpA8Bbb73FW2+9xfTp0zXjRnfu3GHJkiW5urokSSI+Ph43N7c8f3dPnjwp9L5P/5189dVX/PDDDyQkJKBQKEhKSuL+/fv5HkdOCQkJmnZBHq/MzMzk3r17mvtyJsSc/zuFfb0MSSSIYnJzc8Pd3Z2DBw+yePHiZx6fNm0aI0eOZN26dVhbW7N48eJcf2RRUVFs376dPn36sGjRItavX1/kGIKDg3nhhReYNGlSiY7laV999RU3btxg27ZtVKtWjaioKIKDg3P16+bH2dmZa9euaW6npKTk2VefLSQkhOTkZM0/giRJZGRksGvXLkaPHk2fPn347LPP8PX1JTU1VdMvXb16dXx9fTV94YVR0Gvi7Oyc61NiampqrrhdXV1ZsmQJrVq1KvTz5SfnmUlhVK9enZdffplXXnklz8f79u1L3759SUpKYu7cuSxfvpwPP/yQWrVq8fHHH5OVlcWePXuYPHkyx44dw87OLtf+zs7OuT5RJycn8+DBA03i69OnD+PGjWPChAlERkayevVqTVzu7u6aMaGiHqudnR3NmjVj7969+Pn5aT3+nMm/sAqzb84YT548yf/+9z++/vpr6tWrh5mZGb6+vpq/fW2v3dO/yzt37mBhYUGVKlVyjePlpbCvlyGJLqYSWLx4MRs3bszzBXzy5AkVKlTA2tqayMhIdu3apXksLS2NGTNm8Oabb7J06VISEhL49ttvNY+PGjXqmQHUvLRp04YNGzYwcuRI3RxQjtitra1xcnLiwYMHfPbZZ4Xet0ePHhw4cICTJ0+Snp7OypUrycrKynPb+Ph4jhw5whdffEFISAghISGEhoby0ksvaS4C8Pf3586dO6xcuZLevXtrPsEHBAQQHR1NSEgIGRkZZGRkEBkZmSs55XVc+b0mPXr0YP/+/Zw6dUoTd86EOHz4cD755BPNP39iYiL79u0r9O8lpypVqhTpcujBgwfz3XffcfbsWSRJIjk5mQMHDpCUlMT169c5cuQI6enpWFlZYW1trTnLCw0NJTExETMzM5ycnADyvOy5b9++7Nixg6ioKNLT0/n4449p2rQp7u7uADRq1IjKlSvz7rvv0rFjR01bTZs2xcHBgbVr15Kamopareby5ctFulR5+vTpbN++nbVr12o+ZcfFxeX6/QwbNoy1a9dy5coVAB4/fswvv/xSqPaLuu+TJ08wNzencuXKZGZm8tlnn2kuJgH5tbt9+3a+f9N9+vRh48aNxMTE8OTJE1asWEGvXr2wsND+Wbywr5chiQRRAjVr1sTHxyfPx+bNm8fKlStp0aIFq1evplevXprHPvroI1xcXBgxYgRWVlZ8+OGHfPrpp0RHRwPw999/07JlS63Pr1AoaNeuXYFdOMXxwgsvkJaWhp+fH0OHDqVTp06F3rdevXrMnTuX6dOn06lTJ5ycnPLtZggNDcXb25uOHTtSrVo1zdeoUaP466+/uHz5MlZWViiVSg4fPkyfPn00+zo4OLB+/Xp2795Np06d6NixI8uXL9dcAZWXgl6TevXqMWfOHKZOnUqnTp2wt7encuXKWFlZAWjGisaNG0eLFi0YMmRIsedsDBo0iKtXr9K6dWteffVVrdv7+PiwcOFCFixYgK+vL927d2fHjh0ApKen89FHH9G2bVs6duxIYmIib775JgB//PEHQUFBtGjRgsWLF7NixQqsra2fab9du3ZMmTKF119/nY4dOxITE/PMPI2goKBnXgNzc3M+//xzLl26RNeuXfHz8+Pdd9/N9YaqTevWrdm4cSMnTpygR48etG7dmvHjx9O2bVvNBx+lUsn48eOZOnUqLVu2pE+fPlq7U7MVdd+OHTvSuXNnevToQWBgINbW1rm6oLK7JNu2bZvn3IeBAwfy3HPPMXLkSLp27YqVlRVz5swpVKyFfb0MSSEVpt9AMJi4uDimTJnC999/b+xQyrUnT57g6+vLb7/9ppngJgjljUgQgvCv/fv3065dOyRJYtmyZURGRvLTTz8VecxAEMoK0cUkCP8KDw+nU6dOdOrUiZs3b/Lxxx+L5CCUa+IMQhAEQciTOIMQBEEQ8lSm5kGcOXOm2KP+aWlpRr9iwNDEMZd95e14QRxzcfbNb9Z2mUoQ1tbWeHt7F2vfqKioYu9bWoljLvvK2/GCOObi7Jsf0cUkCIIg5EkkCEEQBCFPIkEIgiAIeSpTYxCCIAjaZGRkEBsba9BVIPUtIyOjwLEEkCtAu7u7Y2lpWeh2RYIQBKFciY2NxdHRkVq1apWZiZApKSnY2trm+7gkSdy7d4/Y2Fhq165d6HZFF5MgCOVKamoqVapUKTPJoTAUCgVVqlQp8lmTSBCCIJQ75Sk5ZCvOMYsEAey6vIu45IIX8xAEQShvRIIApvw6hRXnVmjfUBAEQQcaNGjAjBkzNLczMzPx8/Nj4sSJgFw4cu3atcYKT0MMUgNt3dqy9+peJEkql6eegiAYlp2dHVeuXCE1NRUbGxtUKlWutc27du1K165djRihTJxBAEovJXdT73LhnwvGDkUQhHKic+fOHDhwAICwsDCCgoI0j+3YsYMFCxYAMHPmTBYtWsSwYcPo2rUrv/76q8FiFGcQgLKOEoC91/bSxLmJkaMRBMFQNm2Cr77SbZvjxsHo0dq36927N2vWrKFLly789ddfDBw4kD///DPPbRMSEtiyZQvXr1/nlVde0Sx9qm/iDAJwd3LHy9GLvdf3GjsUQRDKiYYNGxIbG8uuXbvw9/cvcNtu3bphZmZG3bp1uXv3roEiFGcQGn4ufoTcDCEtMw1ri/JVKlgQyqvRowv3aV9fAgMD+eCDD9i0aRMPHjzIdzsrKyvDBZWDOIP4V3uX9iRnJHMk9oixQxEEoZwYNGgQr776Kg0aNDB2KHkSCeJfbZzbYK4wZ+810c0kCIJhuLq68sILLxg7jHyJLqZ/OVg64Ofux74b+1jMYmOHIwhCGXb69Oln7mvbti1t27YFYMCAAQwYMACAZcuWad1XX8QZRA5KLyUn75zkfsp9Y4ciCIJgdCJB5NDNqxtZUhb7b+w3diiCIAhGJxJEDm3c2uBo5SgudxUEQUDPCSIiIoIePXqgVCrzrCsiSRKLFi1CqVTSt29fLlz4bybzxo0b6dOnD0FBQXz99df6DFPD0tySLrW7iAQhCIKAHhOEWq1mwYIFrFu3jrCwMHbt2sXVq1dzbRMREUF0dDR79uxh4cKFzJ8/H4DLly/zww8/8MMPPxAaGsqBAweIjo7WV6i5KL2UXL9/nev3rxvk+QRBEEyV3hJEZGQknp6eeHh4YGVlRVBQEOHh4bm2CQ8PJzg4GIVCQfPmzXn06BEJCQlcu3aNZs2aYWtri4WFBb6+vuzda5hP9UovuezGvuv7DPJ8giAIpkpvl7nGx8fj6uqque3i4kJkZGSB27i6uhIfH0/9+vX55JNPuH//PjY2NkRERNCkifYaSWlpaVrXZc1PamoqUVFRSJKEq60r289sp5Ndp2K1VVpkH3N5Ut6OubwdL2g/5oyMDFJSUgwY0bOaN29OUFAQixfLl9RnZmaiVCpp0qQJq1atKnJ7kiQV6pgKs3Z1TnpLEJIkPXPf06W089umTp06jB8/nnHjxmFnZ0eDBg0wNzfX+pzW1tZ4e3sXK96oqCjNvr2u9CLkUgj1G9TH3Ez785ZWOY+5vChvx1zejhe0H3NUVFSB6zcbgp2dHdevX0ehUGBjY8PBgwdxcXHB3Ny8WLFpW5M6m6Wl5TO/m4ISht66mFxdXYmL+2+Vtvj4eJydnQvcJi4uTrPN4MGD+emnn/j222+pWLEinp6e+gr1GUovJfdT73Pq71MGe05BEMqXgsp9JycnM2vWLAYOHEhwcDD79sld3rGxsYwYMYL+/fvTv39/Tp2S36NOnDjBqFGjmDx5Mj179mTatGl5fgAvKr2dQfj4+BAdHU1MTAwuLi6EhYXx0Ucf5domMDCQzZs3ExQUxNmzZ3F0dNQkiHv37lGlShXu3LnDnj17+P777/UV6jO6eskLdey9vhdfN1+DPa8gCIa16ewmvjqt23rf41qMY3Qz7RUACyr3/cUXX+Dn58fSpUt59OgRgwcPpn379lSpUoUNGzZgbW1NdHQ0U6dOZceOHQBcvHiRsLAwnJ2dGT58OH/++SetW7cu0bHoLUFYWFgwd+5cxo8fj1qtZuDAgdSrV4+tW7cCMHz4cPz9/Tl48CBKpRJbW1uWLFmi2f/111/nwYMHWFhYMG/ePCpUqKCvUJ/hbO9Mc9fm7Lu+j9mdZhvseQVBKD8KKvd96NAh9u/fz1f/LlaRlpbG33//jbOzMwsWLODSpUuYmZnlurqzadOmmjHdhg0bcvv2bdNNEAD+/v7PHPjw4cM1PysUCubNm5fnvlu2bNFnaFp1q92NlcdXkpyRjJ2lnVFjEQRBP0Y3G12oT/v6UlC575UrV+Ll5ZXrvlWrVlG1alVCQ0PJysqiadOmmsdylgQ3NzdHrVaXOD4xkzofyjpK0tXpRNyMMHYogiCUUfmV++7YsSObN2/WjCNcvHgRgMePH1OtWjXMzMwIDQ3VSRIoiEgQ+ehUsxPW5tai/LcgCHqTX7nvV199lczMTJ577jn69OnDp59+CsCIESP46aefGDJkCNHR0djZ6bd3Q5T7zoetpS0da3YUZTcEQdA5beW+bWxsWLBgwTPb1KpVi507d2puT5s2DQBfX186d+6suX/u3Lk6iVOcQRRA6aXkXMI54pLitG8sCIJQxogEUYBuXt0ACL8ermVLQRCEskckiAK0qN6CKrZVRDeTIAjlkkgQBTBTmNHVqyt7r+/VyaxEQRCE0kQkCC2UXkruPL5D1N3yVfBMEARBJAgtsst/i8tdBUEob0SC0MKzoid1K9dl3w2xPoQgCLrRoEEDZsyYobmdmZmJn58fEydONGJUzxIJohCUXkoORB8gQ51h7FAEQSgD7OzsuHLlCqmpqQCoVCpcXFyMHNWzipQgsrKySEpK0lcsJkvppSQpPYmjsUeNHYogCGVEQeW+IyMjGTZsGMHBwQwbNozr1+UlkDds2MCsWbMA+Ouvv+jTp49eFz/SOpN62rRpvPfee5iZmTFgwACSkpIYM2YM48eP11tQpqZL7S6YKczYe30vnTzL9ipzglCubNoEX+m23DfjxsHokpX79vLyYvPmzVhYWHD48GFWrFjBqlWreOGFFxg1ahR79+7l888/57333sPW1lZvSULrGcTVq1dxcHBg3759+Pv78/vvvxMaGqqXYExVRZuKtHFrI+ZDCIKgMwWV+378+DFTpkyhT58+LF26lCtXrgBgZmbGsmXLeOutt2jTpg2tWrXSa4xazyAyMzPJyMhg3759jBw5EktLy2eWDi0PutXuxpJDS3iY+pAKNoZbm0IQBD0aPbpQn/b1Jb9y359++ilt27Zl9erVxMbGMjpHjNlF+hISEvQen9YziKFDhxIYGEhKSgq+vr7cvn0bBwcHvQdmapR1lGRJWfwe/buxQxEEoYzIr9z348ePNYPWP/30U677Fy9ezObNm3nw4AG//vqrXuPTmiBGjx7NH3/8wf/+9z8UCgVubm5s2rRJr0GZIj93P+wt7cV8CEEQdCa/ct/jx4/n448/ZtiwYbnWfFiyZAkjRoygdu3aLF68mI8++oh79+7pLT6tXUwbN25k4MCB2Nvb88477xAVFcW0adPo2LGj3oIyRVbmVgTUChDjEIIglJi2ct8tWrTgt99+0zz2xhtvALB06VLNfdWrV2fvXvn9yGiD1Nu3b8fBwYFDhw6RmJjI0qVL+eijj/QSjKlTeim5kniFmw9uGjsUQRAEvdOaILKL1B08eJCBAwfSsGHDclu4Lrv8tziLEAShPNCaIJo0acK4ceOIiIigY8eOJCUlYWZWPidgN6rWiBqONdh3XZTdEITSrDx+yC3OMWsdg1i8eDFRUVF4eHhga2vL/fv3WbJkSbECLO0UCgXdvLqx+8pusqQszBTlM1EKQmlmY2PDvXv3qFKlSrm5ZF+SJO7du4eNjU2R9tOaIBQKBVevXuX3339n0qRJpKSkkJ6eXqjGIyIiWLx4MVlZWQwePJgJEyY8E/TixYs5ePAgNjY2LFu2jMaNGwPw9ddf88MPP6BQKKhfvz5Lly7F2tq6SAenD0ovJZvObuJM3BlaVm9p7HAEQSgid3d3YmNj+eeff4wdis5kZGRgaWlZ4DY2Nja4u7sXqV2tCWL+/PmYmZlx9OhRJk2ahL29Pa+//jrbt28vcD+1Ws2CBQvYsGEDLi4uDBo0iMDAQOrWravZJiIigujoaPbs2cPZs2eZP38+P/zwA/Hx8WzatIndu3djY2PDlClTCAsLY8CAAUU6OH3QjENc2ysShCCUQpaWltSuXdvYYehUVFQU3t7eOm9Xax9JZGQk8+bN03x6r1ChAhkZ2quaRkZG4unpiYeHB1ZWVgQFBREenntt5/DwcIKDg1EoFDRv3pxHjx5pZgeq1WpSU1PJzMwkNTUVZ2fn4hyfzrk6uNLEuYkYqBYEoczTegZhYWGBWq3W9NUlJiYWapA6Pj4eV1dXzW0XFxciIyML3MbV1ZX4+Hh8fHwYN24cXbp0wdramg4dOhRq3kVaWhpRUcVb+S01NbXQ+7aq2IqtV7dy+txpbCyK1qdnSopyzGVFeTvm8na8II5Zl7QmiFGjRvHaa69x7949VqxYwa+//qqZtFGQvEbMnx4Qym+bhw8fEh4eTnh4OI6OjkyZMoXQ0FD69etX4HNaW1sX+zSrKKdoQy2GsvHyRu7a3UVZR1ms5zMF+jotNWXl7ZjL2/GCOObi7JsfrQniueeeo3Hjxhw9ehRJklizZg116tTR+qSurq7ExcVpbsfHxz/TTfT0NnFxcTg7O3P48GHc3d2pXLkyAN27d+f06dNaE4ShdPbsjJW5FXuv7y3VCUIQBKEghbpOs1atWnTr1o3AwEBsbW25c+eO1n18fHyIjo4mJiaG9PR0wsLCCAwMzLVNYGAgISEhSJLEmTNncHR0xNnZmRo1anD27FlSUlKQJIkjR44UKikZir2VPe092otxCEEQyjStZxDffPMNn332GVWrVs019rBz586CG7awYO7cuYwfPx61Ws3AgQOpV68eW7duBWD48OH4+/tz8OBBlEoltra2mvkVzZo1o0ePHvTv3x8LCwu8vb0ZOnRoSY5T57rV7sa7v79LwpMEnO1NYwBdEARBpyQtunXrJiUmJmrbzCRcvHjRYPseiz0mMR9pS+SWYj+nsZXk91Valadjvv3otjR402Bp+8XtUnpmurHDMZjy9Bpn09d7n9YuJldXVxwdHQ2Rq0qVVtVbUcmmkii7IZiszZGb+eH6DwzcNhD3Fe7M2DODS3cvGTssoRTR2sXk4eHBqFGjCAgIwMrKSnP/2LFj9RqYqTM3MyewdiB7r+9FkqRyM2VfKD1UMSpqOtRkTd81rD+9nk+OfcLyI8vp4NGB8S3HM7jRYOyt7I0dpmDCtJ5B1KhRgw4dOpCRkcGTJ080X4JcdiPmUQyX7102diiCkIskSRyOOUzLqi0Jqh/EjqE7iH0zlg+6fcA/yf8wNnQs1T+qzsSdEzl++3i5LF4naKf1DKJOnTr06tUr132//PKL3gIqTXKW/25QtYGWrQXBcC7fu8zd5Lu0rPpfORgXBxdmdJjB9PbTUcWoWH96PZvPbWbtqbX4OPvwYosXGdl0JFXsqhgxcsGUaD2DWLt2baHuK4/qVK5D7Yq1xeWugslRxagAaFG1xTOPKRQKOtbsyIZ+G/h72t982edLbCxseOO3N6jxcQ2G/TiMfdf3kSVlGTpswcTkewZx8OBBIiIiiI+PZ9GiRZr7k5KSMDc3N0hwpYHSS8nW81vJzMrEwkzrCZkgGITqlorKtpWp7VhwUTonaycmtJrAhFYTiIyPZP2p9XwT+Q3fX/ieWhVrMa75OMY0H4NHBQ8DRS6YknzPIFxcXGjSpAnW1tY0btxY8xUYGMj69esNGaNJU9ZR8jj9McdvHzd2KIKgoYpR0d6jfZHWLGnq0pRPe33KnWl32DpwK3Ur12Xugbl4fuJJr297sf3idtLVhSv1L5QN+X7kbdiwIQ0bNqRv375YWIhPxvkJrB2IAgV7r+2lvUd7Y4cjCNxNvstf9/5iTPMxxdrfxsKGYU2GMazJMG7cv8GGMxv46vRXDPphENXsqjG62WhebPEi3tXKV72j8ijfjxdTpkwBoH///vTt2/eZL0FW2bYyrWu0FuMQgsk4HHMYgA4eHUrcVu1KtVnQZQE337jJ7hG76eTZiU+PfUqjNY3o8FUHvjr9FUnpSSV+HsE05XtqMHPmTAC++OILgwVTWnXz6sYHqg94lPYIJ2snY4cjlHOqWyoszSxpXaM10VejddKmuZk5ver1ole9XiQ8SWDT2U2sP72eF39+kSm/TmFY42G82PJF2rq1FXOCypB8zyBeffVVANzc3Pjqq69wc3PL9SX8R+mlRC2pORB9wNihCAKqGBWtarTC1tJWL+072zszvf10Lr56EdU4FYMbDWbL+S20W98On899WHFkBXeT7+rluQXDyjdB5Jw4c+rUKYMEU1q192iPnaWdKLshGF1aZhon75zUSfeSNgqFgvYe7fmq31f8Pe1v1vZZi4OVA1P3TKXGRzX45uw3eo9B0K98E4Q4TSw8awtrOnt2FuMQgtH9+fefpKnTDJIgcnKyduKlVi9xdPxRzr1yjrqV6/LFn6J7urTLdwzi+vXrmsHoW7duPTMwra3cd3mj9FIybc80Yh/F4u7kbuxwhHJKdUueIGfMK+qaODehX4N+LD+ynCfpT0S9p1Is3wSxe/duQ8ZR6mnKblzby9gW5buQoWA8qhgVdSvXxcXBxahxBNQKYJlqGYdjDotVF0uxfBOEGIguGh9nH1zsXdh7XSQIwTiyC/T1rtfb2KHQoWYHzBXmHLx5UCSIUqzw0yyFAikUCrp5dRM1bASjuZJ4hX+S/zH4+ENeHKwc8HXzFVf2lXIiQeiQ0kvJP8n/cC7+nLFDEcohUxh/yCnAM4Djt4/zJF0sD1BaFSpBpKamcv36dX3HUurlLP8tCIamilFR0aaiyZTACKgVQEZWBkdijxg7FKGYtCaI/fv3069fP8aPHw9AVFQUL7/8st4DK43cnNzwruotEoRgFMUp0KdP7T3aY64wF91MpZjWv6TPPvuMH3/8EScnuYSEt7c3t2/f1ntgpZXSS0nEzQhSM1ONHYpQjtxLvselu5dMYvwhm6O1I61rtObgzYPGDkUoJq0JwtzcHEdHx2I1HhERQY8ePVAqlXkuMiRJEosWLUKpVNK3b18uXLgAyHMw+vXrp/lq2bIlX3/9dbFiMDRlHSWpmama/mBBMARdFujTpYBaARyLPUZyRrKxQxGKQWuCqFevHjt37kStVhMdHc3ChQtp0eLZVaqeplarWbBgAevWrSMsLIxdu3Zx9erVXNtEREQQHR3Nnj17WLhwIfPnzwfAy8uL0NBQQkND2bFjB7a2tiiVpeNSOX9PfyzMLETZDcGgVDEqLMws8HXzNXYouWjGIWLEOERppDVBzJkzh6tXr2JlZcXUqVNxcHDgnXfe0dpwZGQknp6eeHh4YGVlRVBQEOHh4bm2CQ8PJzg4GIVCQfPmzXn06BEJCQm5tjly5AgeHh6lZl6Go7Uj7dzbiXEIwaBUMSpaVm+JnaWdsUPJpYNHBzEOUYppXQnI1taWN998kzfffLNIDcfHx+Pq6qq57eLiQmRkZIHbuLq6Eh8fj7Ozs+a+sLAw+vTpU6jnTEtLIyoqqkhxZktNTS32vk9r5tiM1RdWc+TMESpaV9RJm/qgy2MuLcriMaer0zkee5zhdYc/c2ymcLyNKjVid9RuRlQfYZDnM4VjNjR9HbPWBJHXFUuOjo40adKEYcOGYW1tned+OavBZnu6AKC2bdLT09m/fz/Tpk3TFiYA1tbWeHsX7xK/qKioYu/7tBEOI/jswmfEWMbQzrudTtrUB10ec2lRFo/5SMwR0rPS6dus7zPHZgrH2yu2FyuOrsCzrqdBznBM4ZgNrSTHXFBi0drF5O7ujr29PUOGDGHIkCE4ODhQtWpVoqOjeffdd/Pdz9XVlbi4OM3tp88M8tomLi4u1zYRERE0btyYqlWragvTpPi6+VLBugJ7r4luJkH/VDHyBREdaprWAHW27HGIo7FHjR2KUERaE0RUVBQfffQRgYGBBAYGsnz5ciIjI5k3bx4XL17Mdz8fHx+io6OJiYkhPT2dsLAwAgMDc20TGBhISEgIkiRx5swZHB0dn+leCgoKKsHhGYeFmQVdandh7/W9eZ4lCYIuqWJUeFXywtXBVfvGRpBdl0mMQ5Q+WhNEYmIid+7c0dy+c+cO9+/fB8DS0jLf/SwsLJg7dy7jx4+nd+/e9OrVi3r16rF161a2bt0KgL+/Px4eHiiVSubMmcO8efM0+6ekpHD48GG6d+9e7IMzJqWXkpsPb3Lt/jVjhyKUYZIkobqlMrnLW3NysnaiVY1WIkGUQlrHIGbOnMmIESPw8PAAIDY2lnnz5pGcnExwcHCB+/r7++Pv75/rvuHDh2t+VigUuZJCTra2thw7dkxbeLrxxhvYN2oEOuy3zFn+u27lujprVxByupp41WQK9BXE39OfT499SnJGssldaSXkT2uC8Pf3Z8+ePVy/fh1JkvDy8tIMTI8ZM0bf8RnG4cM479kDEyborMl6letRs0JN9l7fyyu+r+isXUHIydTHH7IF1Argw8MfcjT2KIG1A7XvIJiEQhVtiY6O5vr16/z111/88ssvhISE6DksAxs1CpuoKDh7VmdNKhQKlF5K9t/YT2ZWps7aFYScVLfkAn2NqjUydigF6lizI2YKMw5Gi7IbpUmhajEtXLiQRYsWcezYMT788EP2799viNgMZ8QIJAsL0HE5D6WXkodpDzl556RO2xWEbKoYFe3c25lMgb78OFk70ap6Kw7cPGDsUIQi0PpX9dtvv7Fx40aqVq3K0qVLCQ0NJT093RCxGU6VKjwODITNm0GHx9bVqyuAKLsh6EViSiJRd6NMfvwhW0CtAI7GHiUlI8XYoQiFpDVBWFtbY2ZmhoWFBUlJSVSpUoWYmBhDxGZQD4OD4e5d0OFa3FXtqtLCtYUouyHoRXZ9I1Mff8jm7+lPujpdzIcoRbQmiCZNmvDo0SMGDx7MgAED6N+/P02bNjVEbAaV1LEjuLjopZvpSMwRktKTdNquIGQX6Gvj1sbYoRRK9jiEuNy19CjwKiZJkpg4cSJOTk4MHz6cTp06kZSURMOGDQ0Vn+FYWMCoUfDJJ5CQAE/N+i4uZR0lHxz+gIPRBwmqX/om/QmmSxWjooVri1Jz2WgFmwq0rN5SjEOUIgWeQSgUCl577TXNbXd397KZHLKNGQOZmfDttzprsmPNjthY2IhuJkGn0tXpHL99vNSMP2QL8JTXhxALapUOWruYmjVr9kwV1jKrcWPw9YUNG0BHJTJsLGzoVLOTGKgWdOr036dJzUwtNeMP2QJqBZCmThPjEKWE1gRx7Ngxhg4dSrdu3ejbt6/mq8waMwbOnYPTp3XWZDevblz45wJ3Ht/RvrEgFIJmglwpO4MQ4xCli9aZ1P/73/8MEYfpGD4c3nxTHqxu2VInTSq9lLzN2+y7vo/RzUbrpE2hfFPFqKhdsTbVHasbO5QiqWBTgRauLUSCKCW0nkG4ubnx999/c/ToUdzc3LC1tSUrK8sQsRlHpUoQHCyPQ6Sl6aTJZq7NqGZXTYxDCDqhKdBXyrqXsmXPhxDjEKavUDOp161bx9q1awHIyMhgxowZeg/MqMaOhcRE2LVLJ82ZKczo6tWVfdf3ifLfQoldv3+d+Cfxpa57KVv2OMSxWAMV4xSKTWuC2Lt3L59//jm2traAvHTokydP9B6YUSmVUKOGTudEKL2UxCXFcT7hvM7aFMqn0jr+kE2MQ5QeWhOEpaUlCoVCsxRocnKy3oMyOnNzeU7EL79AjhXvSiK7/Le4mkkoKdUtFRWsK9DYubGxQymWijYV5XEIMR/C5GlNEL169WLu3Lk8evSIbdu2MXbsWIYMGWKI2IxrzBhQq+X6TDpQs0JN6lepL8YhhBJTxaho52H6BfoK4u/pz5GYI2IcwsRp/Qt78cUX6dGjB927d+fGjRtMnjyZUaNGGSI242rYEPz85G4mHY0bKL2UHLx5kLRM3Qx+C+XP/ZT7XPjnQqntXsomxiFKB60J4uuvv6ZOnTq8/fbbvP3223ToULr/MItk7Fi4cAFO6qZct9JLSXJGMkdij+ikPaH8yf7bKe0JopNnJxQoxDiEidOaIJKSknjxxRcZMWIE3377LXfv3jVEXKZhyBCwsdHZYHVArQDMFebsvSa6mYTiUd1SYa4wLzUF+vJT0aYiLaq34OBNsYCQKdOaICZNmkRYWBhz584lISGBkSNHlp2lRrWpWBH694ctWyC15H2lFWwq0MatDftuiIFqoXhUMSpaVG+BvZW9sUMpsQDPAI7EinEIU1boUa4qVapQtWpVKlasyL179/QZk2kZOxYePICff9ZJc0ovJSfvnOR+yn2dtCeUHxnqjFJZoC8//rX8Sc1M5fjt48YORciH1gSxZcsWRo0axZgxY7h//z6LFi1i586dhojNNAQGgru7zrqZlHWUZElZ7L9RxpZtFfTudNxpUjJTykyC6FRTjEOYOq0J4s6dO8yePZuwsDAmT56Mh4cHv/zyS6Eaj4iIoEePHiiVSs1M7JwkSWLRokUolUr69u3LhQsXNI89evSIyZMn07NnT3r16sVpHRbPKxJzc3jhBfjtN7h9u8TNtXVri6OVo7jcVSgy1a1/J8iV0hIbT6tkW4nmrs1FgjBhWhPE9OnTqV+/PgcPHuStt96iS5cuhUoQarWaBQsWsG7dOsLCwti1axdXr17NtU1ERATR0dHs2bOHhQsXMn/+fM1jixcvplOnTvz666+EhoZSp06doh+drrzwAmRl6WROhKW5JQG1AkSCEIpMFaOiVsVa1HCsYexQdCagljwOIS79Nk0FJogTJ04wd+5cAgMD+fHHH1GpVISHh7Ny5UqtDUdGRuLp6YmHhwdWVlYEBQURHh6ea5vw8HCCg4NRKBQ0b96cR48ekZCQQFJSEidOnGDQoEEAWFlZ4eTkVILDLKF69aBDB52tE9HNqxvX71/n+v3rOghOKA8kSUIVoyoz3UvZAmoFiHEIE5Zvue/OnTtTo0YNhg0bxltvvYWDgwOBgYGamkzaxMfH4+rqqrnt4uLyzMJDT2/j6upKfHw8FhYWVK5cmVmzZnHp0iUaN27MO++8g51dwUsrpqWlERUVVaj4npaamlrgvhV69qTGnDnc+P57Ups1K9ZzZPPCC4BvVN8wpI7xZqVrO+ayqLQec0xSDHFJcdSxrFOk+E39eJ3TnVGg4IcTP1A1uapO2jT1Y9YHfR1zvgmie/fuhIeH88svv2Bubk7Xrl019ZgKI6+qpU/vn982mZmZXLx4kTlz5tCsWTMWLVrE2rVreeONNwp8Tmtra7y9vQsdY05RUVEF7zt5MixZQu0DB2DYsGI9R7aGUkPcVG6cTznPPO95JWqrJLQecxlUWo/55Fl5suYA3wF4uxY+/tJwvM2ONuNC8gWdxVkajlnXSnLMBSWWfLuY3n33Xfbv38+YMWM4duwYPXr0IDExkd27dxeqmqurqytxOQrdxcfH4+zsXOA2cXFxODs74+rqiqurK83+/aTes2dPLl68qPU59crJCQYNgu++g5SUEjWlUChQ1lESfj0cdZZaRwEKZZkqRoWTtRNNnJsYOxSdC/AM4HDMYTEOYYIKHINQKBS0a9eORYsWsX//fj766CPCw8MJDAzU2rCPjw/R0dHExMSQnp5OWFjYM/sFBgYSEhKCJEmcOXMGR0dHnJ2dqVatGq6urly/LvfRHzlyxLiD1NnGjIGHDyEkpMRNKb2U3E+9z6m/T5W4LaHsU8Wo8HP3w9zM3Nih6JwYhzBdWpcczWZpaUlgYCCBgYGkFmJWsYWFBXPnzmX8+PGo1WoGDhxIvXr12Lp1KwDDhw/H39+fgwcPolQqsbW1ZcmSJZr958yZw/Tp08nIyMDDw4OlS5cW4/B0LCAAPD3lwerhw0vUVNfaXQHYe30vvm6+OghOKKsepD7gQsIFBjcabOxQ9CK7LtPBmwfp5NnJ2OEIORQ6QeRkY2NTqO38/f3x9/fPdd/wHG+sCoWCefPy7oP39vZmx44dxQlPf8zM5EteFy6EmBjw8Ch2Uy4OLjR1acq+6/uY3Wm2DoMUypojMUeQkMrcFUzZKttWpplrMw5EH+Ddzu8aOxwhh9JbUN5YRo+WL3X95psSN9WjTg8O3TrEncd3dBCYUFapYuQCfW3d2xo7FL3x9/QX4xAmKN8E8eWXXxp/YNgU1akDnTvrZE7ExFYTUUtqPj7ysY6CE8oiVYyKZq7NcLByMHYoehNQK4CUzBRO3Dlh7FCEHPJNEO7u7mzatIng4GBmzpzJ7t27efjwoSFjM11jx8LVq3D4cImaqVO5DsOaDOOLk19wL7kcFUAUCi1DncGx2GNltnspW2fPzqIukwnKN0EEBQWxbNkyQkJCGD16NDExMUyaNInnn3+ezz777JlJb+XKoEFgby+fRZTQzA4zeZLxhFXHV+kgMKGsORN3pkwV6MtPZdvKNHVpKtaHMDGFGoNo1KgREydO5JtvvuHLL7+kXr16/PDDD/qOzXQ5OMDgwbBtGxRiTkhBfFx8eK7Bc6w8tpLHaY91FKBQVqhiylaBvoIE1ApAdUtFujrd2KEI/yryILWDgwM9evRg4cKF+oin9BgzBh4/hp9+KnFTszrO4n7qfb7888uSxyWUKaoYFTUr1MTdyd3Yoeidv6e/PA5xW4xDmApxFVNxdeoEtWvrpJvJz92PwNqBfHTkI7G6lqAhSRKHYw6X+e6lbJ09OwOIcQgTIhJEcZmZyWcR+/fDzZslbm52x9nEJcXx9ZmvS9yWUDbcfHiTO4/vlJsEUcWuCk1dmnLg5gFjhyL8q1AJIj4+nlOnTnHixAnNl4A8JwJg48YSNxVYO5A2bm14X/U+mVmZJW5PKP3K2gJBhRHgKcYhTInWBPHhhx8yfPhwPv/8c9avX6/5EoBateQlSb/+Wl5QqAQUCgWzO84m+kE0353/TifhCaWbKkaFo5UjPs4+xg7FYLLnQ5y8c9LYoQgUotTGvn37+PXXX7GysjJEPKXPmDHymcShQ/IEuhLo26Avjas1ZumhpYzwGYGZQvQAlmdluUBffrJrMR2IPkB7j/ZGjkbQ+g7k4eFBRkaGIWIpnQYMAEdHnQxWmynMmNVxFhf/ucjPf/2sg+CE0uph6kPOxZ8rN+MP2araVcXH2UcMVJsIrWcQtra2BAcH065du1xnEe++K4pqAfKEuSFD5HUiVq2S50iUwNAmQ5nz+xyWHlpKvwb9irRIk1B2HI09KhfoK0fjD9kCagWw/vR60tXpWJmLngtj0noGERgYyKuvvkqLFi1o3Lix5kvIYcwYecLc9u0lbsrCzIK3O7zN8dvH2X9jf8ljE0olVYwKM4UZbd3KboG+/ATUCiA5I1mMQ5gArWcQ/fv3N0QcpVuHDlC3rtzN9MILJW7uheYv8N7B91hyaAldvbrqIEChtFHFqGjm0gxHa0djh2Jw2fMhDkYfFOMQRpbvGcSUKVMA6Nu3b55fQg4KhXwWcfAg/LsKXknYWNgwrd009t/Yz9HYoyWPTyhVMrMyy0WBvvxoxiHEfAijy/cM4p133gHgiy++MFgwpdro0TBnjjwn4r33StzcxNYTWfzHYpYeWkrosFAdBCiUFmfjzvIk40m5HH/I5u/pz1dnviJDnYGluaWxwym38j2DcHZ2BsDNzS3PL+EpHh7QrZucIEo4JwLAwcqBKW2n8PNfP3Mu/pwOAhRKC02BvnJ6BgFiHMJU5JsgWrRoQcuWLTVf2bezvwt5GDNGLrtxUDcli19v+zr2lvYsUy3TSXtC6aCKUeHh5IFHheIvaVvaibpMpiHfBNGuXTvq1q3LK6+8wq5duzh9+jSnTp3SfBfy0L8/ODnpZE4EyDXyX2n9Ct+d/45ridd00qZg2iRJQnVLVa67lwCq2VejiXMTMQ5hZPkmiDVr1rB+/XoqV67MnDlzGDlyJN9++y0PHjwwYHiljK0tDBsGP/4Ijx7ppMmp7aZiYWbBB6oPdNKeYNpuPbzF7ce3y3X3UrbsukwZajFR11gKnAfh6OjIwIED+d///sewYcNYuXIlPxVh/YOIiAh69OiBUqlk7dq1zzwuSRKLFi1CqVTSt29fLly4oHksMDCQvn370q9fPwYMGFCEQzKyMWMgJQV0tKBSdcfqjGs+jq/Pfs3tR7d10qZgusT4w3/8a/nzJOMJf/79p7FDKbcKTBCnTp1i4cKF9O/fn1OnTrF69WrGjh1bqIbVajULFixg3bp1hIWFsWvXLq5evZprm4iICKKjo9mzZw8LFy5k/vz5uR7fuHEjoaGh7Nixo2hHZUx+ftCggVzAT0dmdJiBOkvNx0c+1lmbgmlS3VLhYOWAj0v5KdCXHzEOYXz5JojAwEDee+89XFxcWLhwIQMHDsTW1pYLFy7k+qSfn8jISDw9PfHw8MDKyoqgoCDCw8NzbRMeHk5wcDAKhYLmzZvz6NEjEhISSn5UxpQ9J+LQIXgqIRaXVyUvhvsM54s/v+Be8j2dtCmYpuwCfRZmWuewlnnO9s40rtZYJAgjyvevMPtS1j/++INDhw4hSZLmMYVCwaZNmwpsOD4+HldXV81tFxcXIiMjC9zG1dWV+Ph4zSW2L774IgqFgqFDhzJ06FCtB5OWlkZUVJTW7fKSmppa7H2fZuHnR10zM+4tX84//044LKnB1QezOXIzc8Lm8HqT13XSpi6PubQw5WNOykjiXMI5XvZ+WWcxmvLxFkazCs34KfonIi9EYmlWuPkQpf2Yi0Nfx5xvgvjmm29K1HDOhJLt6cJzBW2zdetWXFxcuHfvHmPHjsXLywtfX98Cn9Pa2hpvb+9ixRsVFVXsfZ/h7Q3du1M1LIyqq1eDecnLNXvjTfDNYLZe38r7z72vkxIMOj3mUsKUj3nPtT1kSVkEtwzGu45uYjTl4y2M/lJ/tlzdQkqFFJq6Ny3UPqX9mIujJMdcUGLR24IDrq6uxMXFaW7nPDPIb5u4uDjNNi4uLgBUqVIFpVL5zNmHyRszBmJj5SVJdWRWx1k8SH3AFyfF7PaySHVLLtDn5+5n7FBMhhiHMC69JQgfHx+io6OJiYkhPT2dsLAwAgMDc20TGBhISEgIkiRx5swZHB0dcXZ2Jjk5maSkJACSk5NRqVTUq1dPX6HqR79+ULGiTger27i1oZtXNz468hGpmak6a1cwDaoYFU1dmuquQN+VK1SfM0cn9cGMxdnemUbVGon5EEaSb4LIzCzZusgWFhbMnTuX8ePH07t3b3r16kW9evXYunUrW7duBcDf3x8PDw+USiVz5sxh3rx5ANy7d48RI0bw3HPPMXjwYPz9/elcwtXaDM7GBoYPhx074OFDnTU7u+Ns4p/Es+G0bibjCaYhMyuTo7FHdXd5q1oNo0dTcft2aN0afvlFN+0aQYBnAIduHRLzIYxBykf//v2lV155RdqyZYsUExOT32Ym5eLFi0bZN1/HjkkSSNKXX+qsyaysLMlvnZ9U65NaUnpmeona0ssxmzhTPeY/7/wpMR9pS+QW3TT4ySeSBFL8G29IUrNmkqRQSNKCBZKkVuumfQPadn6bxHykozFHC7W9qb7G+qSv9758zyB27Nihqei6ZMkSBg4cyJIlSzh06BDp6ekGS2Clmq8vNGqk024mhULB7I6ziX4QzXfnv9NZu4JxqW79O0FOFyU2oqNh9mzo1Yt7L70Ehw/D88/D3LkQHAylrBqCfy1/AA7e1E2NM6HwChyDcHNzY/jw4axZs4bvvvuOLl26cPjwYUaMGMGECRMMFWPplT0n4sgRuHRJZ80G1Q/Cx9mHpYeWkiWVvHKsYHyqGBXuTu7UrFCzZA1JEkyYAGZm8MUX8t+gnR1s2iQvifvLL/IHl3Olp0KwZhxCDFQbXKEHqS0tLWnXrh1vvfUWP/74IwsXLtRnXGXHyJHyZa4bN+qsSTOFGbM6ziLqbhShl8RaEWWBKkalm/GHTZtg715Ytgxq5kg2CgVMmgQHDsjL4/r5wb9jgaWBv6c/f9z6g8ysko2NCkVT7KuYsi9DFbSoXh169pT/cdVqnTU7uPFg6lSqw5JDS/KcTyKUHrce3iL2UWzJE0R8PLz5prwE7iuv5L1Nhw7w55/QsiWMGAFvvAEZpj/4G1ArgKT0JE79LSpJG5LeLnMVchgzBu7ckT/Z6YiFmQVvd3ibk3dOsu/6Pp21Kxhe9vhDiddffv11+exg3Tq5iyk/1avL83OmTIFPP4WuXSHHfCRT5O8pj0OIbibD0pog0tLSnrkvMTFRL8GUWX37QuXKOh2sBhjdbDQ1HGuw5NASnbYrGJYqRoW9pT3NXJsVv5GQELmC8Lx50LCh9u0tLeGTT+Dbb+HkSWjVSh7MNlEuDi54V/UWCcLAtCaIQYMGcebMGc3t3377jeHDh+szprLH2lo+nQ8Jgfv3ddeshTXT203nQPQBDseY7j+3UDBVjIq27m2LX6DvwQN49VVo1gxmzCjaviNGwNGj8lomAQGwerU80G2CAmrJ8yHEOIThaE0Qy5cvZ+HChbz//vtMmzaNbdu2sVGHA67lxtixkJYG3+n20tSXWr1EFdsqLD20VKftCobxOO0xkfGRJRt/mDFDHn9Yv14+Myiqpk3hxAno3l0eyH7hBUhOLn48euLv6c/j9Mec/vu0sUMpN7QmiAYNGvDKK6/w3XffcezYMebOnZurAqtQSC1agI+PzruZHKwcmNJ2Crsu7+Js3Fmdti3o39HYo2RJWcVPEPv3y2MO06fL3UTFVakS/PwzvPcebN4M7dubXImO7PkQopvJcLQmiNmzZ7Nx40Z+/vlnli5dyssvv8y3335riNjKluw5EcePw8WLOm16UptJOFg5sEy1TKftCvp3OOYwChTFK9CXnAwvvQR168JTi20Vi5mZPJlu1y64eVMu0fHrryVvV0dcHVxpWLWhqMtkQFoTRP369dm0aRMeHh506tSJbdu2FWrBICEPI0eChYXOzyIq2Vbi1davsu3CNq7cu6LTtgX9UsWo8HHxoYJNhaLvPHeu/Cn/f/+TxxB0pXdveeC6Zk3554ULIcs0JmQGeAbwx00xH8JQtCaIMWPG5FrHwdHRkSVLxFUzxeLsLP/DffMNlLAY4tPebPcmlmaWfKD6QKftCvqjzlIXv0DfiROwYoU8azogQOexUaeOSZboCKgVwOP0x5yJO2PsUMoFrQkiOjqayZMn07t3b7p27ar5Eopp7Fj5mvPfftNps64OrrzY4kU2nt1I7KNYnbYt6Me5hHM8Tn9c9ASRng4vvgiurvCBHj8QmGCJDjEOYVhaE8SsWbMYPnw45ubmbNq0ieDgYPr162eI2Mqm3r2halWddzMBzOgwgywpi48Of6TztgXdK3aBvg8+kN+ov/gCKhSja6ooTKxEh6uDKw2qNBAJwkAKNVGuXbt2gFy87/XXX+fo0aN6D6zMsrKST9t//hnu3dNp07Uq1uL5ps+z9tRa/nnyj07bFnRPFaOihmMNPCt4Fn6nixflMYGhQ+UJmIbydImON980WomOgFoBoi6TgWhNEFZWVmRlZeHp6cnmzZvZu3cv93T8xlbujB0rdxPo4ZPYzA4zSclIYeWxlTpvW9Ct7AJ9T6/Vni+1GsaPBwcHWGmE1zdniY5PPoFu3YxSoiOgVgCP0h6JcQgDKNRlrikpKbz77rtcuHCB0NBQ3n//fUPEVnY1awbNm+ulm8m7mjf9vfuz6vgqHqU90nn7gm7EPorl1sNbRRt/WLNGLh3/ySfyBQ/GkLNEx4kTRinRIeoyGY7WBNG0aVPs7e1xdXVl6dKlfPbZZzRv3twAoZVxY8fKp+x6GPSb1XEWD9Me8vmJz3XetqAbRR5/uHkTZs2SKwOPHKnHyArp6RIda9YYrERHdcfqNKjSQCwgZAD5Fn95+eWXC9zxiy++0Hkw5cqIEfLs16+/ho90O6jcukZrutfpzoqjK5jcdjK2ljq8Rl7QCVWMCjtLO5q5FKJAnyTBxInygHH2IkCmILtEx6hR8NprcOyYHJ8u52Tkw9/Tn+8ufIc6S425mbnen6+8yjdBnDlzhurVqxMUFESzZs3EmgO6VrUq9OkDGzbInwhbtNBp87M7ziZgYwAbzmzgVd9Xddq2UHKqGBVt3dpiaV6I2knffCNfFr1qFXgWYUDbELJLdCxaJM/mjoyEHTugdm29Pm1ArQDWnlrLmbgztKpRghIjQoHy7WJSqVS8+eabXLlyhcWLF6NSqahUqRJt2rShTZs2hoyx7Fq8WB5w7NgRtm/XadOdPTvT3qM9H6g+IENt+gvClCdJ6UmcjTtbuPGHnIsAvWqiiT5niY7oaHlcQs8lOsR8CMPIN0GYm5vTuXNn3n//fbZt24anpyejRo3im2++MWR8ZZu3t1ybqWlTGDQIFizQWT+uQqFgdsfZ3Hx4k63nS8/SkuXBsdhjqCV14cYfJk+GpCTtiwCZgqdLdHz4od7GJWo41qB+lfqiLpOeFfgXl56ezp49e5g+fTrffvsto0aNonv37oVuPCIigh49eqBUKlm7du0zj0uSxKJFi1AqlfTt2/eZGk9qtZrg4GAmTpxY6OcsdVxd4fff5X7cefNg2DCdlVruXa83TV2asvTQUrIk06ilI8jdSwoUtHNvV/CGoaGwbZv86bwwiwCZguwSHUOGwFtvyaXI9VTHKbsukzpLd0v5CrnlmyDefvtthg0bxoULF5g0aRLbt2/ntddeK/Ra1Gq1mgULFrBu3TrCwsLYtWsXV69ezbVNREQE0dHR7Nmzh4ULFzL/qYqUmzZtok6dOkU/qtLGxgY2boT335dXBevcGW7fLnGz2WcRl+5eIuRSSMnjFHRCFaOiiXOTggv0ZS8C1LSp/EZbmtjZwZYt8hKoH30kX7Gnh0l1/rX8eZj2kLPxosy9vuSbIEJDQ7lx4wabNm1i2LBhtGzZkpYtW9KiRQtatmypteHIyEg8PT3x8PDAysqKoKAgwsPDc20THh5OcHAwCoWC5s2b8+jRIxISEgCIi4vjwIEDDBo0qISHWEooFPIbQWgo/PWXXPfm+PESNzuo0SDqVq7Lkj+WiAsNTIA6S82RmCPaxx/eekuehFbcRYCMzcxMXu960SK5nlP//jpfhEjMh9C/fK9iunTpUokajo+Pz7WwkIuLC5GRkQVu4+rqSnx8PM7OzixZsoQZM2bw5MmTQj9nWloaUVFRxYo3NTW12PvqVN26WH/7Le6vvopF5878vWgRj4KCStTkC14vMOfkHNYfWE8H1//emEzmmA3I2Md86cElHqc/ppZFrXzjsDt2DM///Y9748aRYG8PJYjX2MfLgAFUzMzEdcECUjp2JGbNGrJ0WD/K08GTXed30atiL819Rj9mI9DXMRdzEVzt8vq0+nRJgfy2+f3336lcuTJNmjTh2LFjhX5Oa2trvL29ix4sEBUVVex9dc7bG9q1g4EDcZsxA7cHD+QB7GIOUr5V/y2+vPwlm29uZnyX8Zr7TeqYDcTYx/z7id8BGNJ2CLUr5XEpaHKyXGOpTh2qrFpFFTu7Ej2fsY8XkMfWmjTBbsQIGrz0knyFk5ubTprufrU72y5so36D+pr5ECZxzAZWkmMuKLHo7bIIV1dX4nLUack+Myhom7i4OJydnTl16hT79+8nMDCQqVOncvToUaZPn66vUE1TtWqwb59c1nnxYvkqp6SkYjVlZW7F9HbTOXjzoGYGr2AcqhgV1R2qU6tirbw3mD8frl2TFwEqYXIwKQMHyiXDo6PlS3YvX9ZJswG1AsQ4hB7pLUH4+PgQHR1NTEwM6enphIWFERgYmGubwMBAQkJCkCSJM2fO4OjoiLOzM9OmTSMiIoL9+/fz8ccf4+fnx/Lly/UVqumyspLfKD75RB6b6NhRLrlQDONbjqeqXVWWHlqq2xiFIlHdUtGhZj4F+k6elAd1X3oJunQxfHD6Fhgolw1PTpaTxMmTJW4yexziYLQou6EPeksQFhYWzJ07l/Hjx9O7d2969epFvXr12Lp1K1v/rWLq7++Ph4cHSqWSOXPmMG/ePH2FU3opFHL1zN275U9fbdoUqziavZU9b7R9g7ArYaIKppHcfnSbmw9v5j1AnZFhmEWAjK1VK1Cp5AmiXbrAUxeuFJWbkxt1K9cV8yH0RSpDLl68aJR9DSYqSpLq1pUkKytJ2rChyLvfT7kvOS5xlIb+MFSSpFJyzDpmzGP+/vz3EvORjscef/bBRYskCSQpNFSnz2myr/Ht25LUpIn8t/zDDyVqanzoeKnisopSpjpTkiQTPmY90td7n4lPzRRyadhQLojWsaN8bfmMGfIaAYVU0aYir/m+xrYL27h8Tzd9wELhqW7JBfqauzbP/UBUlHwRwpAh8NxzRonN4GrUgIgI+XLuIUPkIn/FFFArgAepD4iMj9S+sVAkIkEgj/++80517t41diSFULmyfBXIq6/C8uXQrx88Kvy6D2/4vYG1hTUfqMpwN0Ye4pPiORp7lKSM4g3064IqRkUbtza5C/RlZRl3ESBjqlQJ9uyBoCB45RV5pbxizNURdZn0R2+XuZYmHTrAW29VwNsbVqyQVwQ1lYrKebK0hNWroXFjuVZPu3awcyd4eWnd1cXBhfEtxvPln1/S3K451q7WeFbwLDMlk5PSk7iQcIFzCec4F3+O8/+c51z8Of5JlpdgNVOY0epYK7rU6kKX2l3o4NEBR2tHg8R1Ju4MMzvOzP3AmjXymNLGjVDIKgVlip2dXP31pZfkkiIJCfIEuyJc0u3u5E6dSnU4ePMgb7Z7U4/Blj8iQSAXy6xT5wZLl3oxapRcXfnzzwv1fmtcr74qdzsNGiQPXv/4o7x4ixbT20/nm8hveF31Oq+rXsfK3AqvSl7Ur1Kf+pXrU69KPfnnKvWp7lC98EtiGlBmViZX7l3RJIJzCfLX9fvXNdvYWdrRxLkJzzV4Dh9nHzwrerLn3B7OJZ1jxdEVfHD4A8wV5vi6+dKlVhcCagXQwaMD9lb2Oo/3+O3jcoG+nAPU2YsA9egh1+Iqrywt5bL31arJZ8V378oJ08qq0E0E1ApgR9QOUXNMx0SC+Fe9emkcOiR3hc6aBU2awHvvycnDwpR/S4GBckmOvn1BqZQ/kb70UoG7eFb05MaUG+w6sYt0h3SuJF7h8r3LXEm8wm9XfyNNnabZ1t7SnnpV6lGv8n9JI/vnKnZV9H10SJLE7ce3cyWBc/HniLobRbo6HQBzhTn1q9SndY3WjGk2Bh8XH3ycfahdqTZmityfRBtIDfD29iY5I5nDMYf5/cbv/B79Ox8e/pClh5ZiaWZJG7c2BNQKoEutLrT3aK+TBZdUt/4t0OfRLvvA4OWX5e9ffmnip6wGoFDI1V+dneUyI4mJcgl8B4dC7R5QK4D1p9cTGR+JNdZ6Dta0PEl/wuP0x3pp25Tf+gzO3FxeGKtfP/n7W2/JNcfWrZOvzjNZdevKyz8OGwYTJsD58/L19AVktkq2lWhdrfUzsy+zpCxiHsb8lzTuXeFy4mVOx51mR9QO1NJ/g+KVbCo9kzSyk0lxum0epj78r2so4bwmITxIfaDZxs3RDR8XH7rX6Y6Psw8+Lj40rNoQGwubIj2XnaUd3by60c2rGyB3Aaluqfg9Wk4Yyw4tY/Efi7Eyt8LP3Y8AzwC61O6Cn7tfkZ8L5PGHxs6NqWhTUb7j22/lsaSVK01vESBjmjFDXkzrpZega1cIC5Nva5GzLlOPCj30HaVJuJp4ldXHV/PVma+o61iXP5v9qfPnUEhS2angVtLp5jn3lST46SeYNEles+WNN+QzikJ+oDGOzEw5q61YIZ9NfP+9PBCYj6L+vjLUGdx4cENOGvcua846Lt+7TMyjmFzbVneoLndVPdVl5VXJCzOFGZfuXnrmrCBnG07WTnIC+DcJ+Dj70MS5CZVs8z+ewijsMT9Ke8ShW4c0Zxin406TJWVhbW5NO4928hhGrS60cWuDtUXBn1jVWWoqf1CZ4U2G80WfL+R+dm9vuXswIkL+ZKInpbbsxM8/w9ChcvLcs0deY0KLuivr0sS5CUubLS2dx1wIkiSx7/o+Vh5fSdjlMMzNzBnSeAhDqg+hX7t+xWqzoL8RcQaRD4UCBgyQe3BmzYKPP5bPeD//HHr10r6/UVhYyIE2bixfFeLnJw9e16+vk+YtzS01b/RB5C4gmJyRzLXEa7mSxpXEK/x8+WcSniRotlOgwNzMnMysTLlNM0u8q3nT2bMzTZybaBKCh5OHUcc+nKyd6F2vN73r9QbgQeoD/rj5B79H/86B6APMPzCfeczD1sKW9h7tNYPerWu0xso8d9/5hX8u8Cjt0X/jDzkXAdJjcijVnntOXmb1uefkq0h++w0aNSpwF804RNOyNw6RlJ7EN2e/YdXxVUTdjcLZ3pk5necwsfVEajjW0FtxQpEgtKhYUU4Kzz8v99707g3Dh8vVL54qLWU6XnxRTgoDBkDbtvIaE9266fUp7Szt5E/6Lj7PPPYw9aEmaVy+d5nMrExNMqhfpX7h1mU2soo2FenboC99G/QFIDElkYibERyIPsDv0b/z7u/vwu/y76FjzY6aQe/WNVpr6l91qNlB/mT8/ffyJZ1l9FOuznTuDAcPQs+e0KmT3N3k55fv5v6e/qw/vZ7LDy/TmMYGDFR/rt+/zurjq1l/ej0P0x7SqnorNgVvYkjjIVrPXHWi2NPvTJC+Z1KnpkrSe+/Jkz8rVZKkr76SpKysYj+l/l2/Ls9WNTeXpM8+eyZYMeNUd/558o/044UfpUlhk6TGqxtLzEdiPpLDEgfJdbmr5LrcVcq6f1+SatSQJB8fSUpL00scTysTr/G1a5JUp44k2dlJ0i+/5LvZzQc3JeYjzQqZZcDgdC8rK0vad22f9NzW5yTFfIVk/p65NPSHoZLqlkrKyucNR1/vfeIMogisreVLtQcPls8mxo2TL4n98kuoV8/Y0eWhdm35Gvvnn5cHU86flwdFS+MCNCauql1VBjYayMBGAwFIeJLAgegDHIg+wMGbB+lbvy+KmTPlRYBCQop0CWe55+Ul12/q2VO+Wm/jRhgx4pnNalaoiVclL47/U/KFtozhSfoTNkduZtXxVVz45wJV7aoyu9NsXm79Mu5O7sYJqthpxwQZshaTWi1JX34pSRUqSJK1tSQtXixJ6enFfnr9ysyUpJkz5Vo/AQGSdPeuJEll5NNlERntmA8ckH//06cb9GnL1Gv84IH89wuS9OmneW4yLmScZLfITnon/B1p5187pYSkBAMHWXQ37t+QZuyZIVVaVkliPlLzL5pLG05vkFIyUgrdhjiDMDFmZvJZRN++8pjjO+/A1q3yuGPbtsaO7inm5rB0qTzIN368PKlu505x7b2hpKTIv/c6deRL4YTiqVBBXlNixAi5wvE//8g1rHL8Hb/U6iWO3jzKskPLNJdk16lUBz93P/zc/Wjr1pZmrs2euZDA0CRJ4kD0AVYdX0XoX6EoUDDAewCT206mg0c+5eCNQCSIEqpeXR4D/vlnee5Eu3Zyb87ixeCo/woORTNqlNwXFhwMfn64tW8PDRrIhdOqV//ve/Xq8uWxJvJHajSSBOnp8ht8crL8PfurKLcvX4arV2H//rK1CJAx2NjI/3CvvCKvd52QIE8O/fdqMD93P35U/ohnXU/+vPMnR2OPcvT2Ufbf2M+3574FwNrcmlY1WuHn5qdJHO5O7gZ5U07OSGbLuS2sPLaScwnnqGJbhbc7vM0rrV/Bo4KH3p+/qESC0JHnnpOrXLz7Lnz2mTyHYs0a+QzDpPj5wYkTMHky1mfPwpEjeRf7s7bOnTTySiI1asjFA00hkWRmwsOH8teDB3l/f/gQ19u35TeZwr7BF3eakLU12Nr+97VgQdlcBMgYzM3lgT9nZ/mT2N278sRDm/8mMNpZ2tHJsxOdPDsB8if22EexcsL4N2msPrGaj49+DEANxxqaMww/dz9aVW+l05Irtx7eYs2JNfzv1P9ITEmkqUtT1j+3nuFNhpd8pn58PGaP9TOTWkyU08G+Tzt6VJ4Iev68XCZp5Ur5/dTUaI75yRP4+2+4c6fg7w8fPtuIlVXuhPH0z9nfq1TJvwCbJMkx5Hgjz/dNPr/vT55oP2B7ezJtbLBwcJDftO3scr+JF+V2QY/Z2JjM/IZSO1GusD79VJ7F2qWLPPjv5FToY05XpxMZH/lf0og9yrX71wC5fEtTl6aaMww/dz/qVa5XpLMMSZL449YfrDy2kp8u/QRAcMNgprSdQqeanYp3xpKYCH/+KX/IO3lS/h4bS3KzZtidOVP09ij4b0QkCB3sm5eMDLnu2Hvvye8XH3wgd0MXoUil3hX5mJOT5UShLZk8ePDsvpaW8mppNWrIb6JPfbLXuq6FpaXcB12xYvG+OzmBhUXZf8N8Srk43m+/hTFjwMcHfvmFqMTEYh/zP0/+4djtY5qEcfz2cU2do0o2lWjr3lbTNdXGrU2eM/tTMlLYen4rK4+t5Gz8WSrZVOKlli/xqu+reFYsQlmVx4/h9Gk5CWQnhGvX/nu8Xj1o3Rp8fbnq7U3dnj2LdcxiJrURWFrKM7AHDYKJE+WvzZth7Vq5wkKpZGcnD7TWqVPwdikpzyaSnD+npMiJolGjwr/J29iYRleWYHqef17u6hw4EDp2xHLNmmJPQqxmX40+9fvQp34fQC6TcunupVxdU+9dfQ8J+XN1w6oN5TMMNz98XHzYdXkXa/9cy72UezRxbsLaPmt5vunz2FlqGXtKTYUzZ/47Kzh5Ul5IKvvze82a8uJK48fL31u2zFVGJ0PMpC6d6tWTl939+muYNg2aNYPZs2HmTLmbukyytZWvXTf5eulCmdGrl/yPFhSEV9++UKvWf92deXWBVq8un1Vq+dBhbmZOY+fGNHZuzIstXwTkOl0nbp/QJIxdl3fx9ZmvAbmUTL+G/ZjcZjIBtQLy7kbKyJD7n3N2E50/L4+jgbwuiK+vXIuqdWv5y0hlG0SCMACFQl4hNChI7i6dP1+utrB2rbx6qCAIOtCuHRw+zP2lS6mSkiKfsR479t9Z69Ps7LQnkerVn7kQw8naia5eXenq1RWQxxpuPLjB6b9P07J6S2pXqv3fc6jVcOlS7jODM2cg7d+S+pUqyQngrbc03UW4uZnM2bJIEAbk7CyXDx81Sr5Kr1Mn+ay4dm357yT7q3Ll3LcrVjSZMU/hKVlZ8kVg9+/n/kpMzH07+/3A0B4+rI67u1yF2NFR/sr589O3HRzkM1sTeX8quoYNSZg5kyo5u5gkSX6Rnu7yzNn1efasXH49r6uBcl6IkUciUVSvjlf16ng1CIYbN+DXrf8lhFOn/ruAwsFBXjdg0iQ5EbRuLZ9lm/AvW68JIiIigsWLF5OVlcXgwYOZMGFCrsclSWLx4sUcPHgQGxsbli1bRuPGjUlLS+P5558nPT0dtVpNjx49mDx5sj5DNaheveDCBZg3T55cFxYmd0EWxMnp2cSRX0LJeX+FCqY1MG6KJEl+X8jvzb2g+x88kJNEfiwt5dfCtuRrDhVLWpo9aWny8WX3YGhjYZF34ijKz46OxqvoEhdngYsL2NvL7+0KhUL+R6hQQfsA4NNX9D2dSP76Cw4ckF/8gtjYQPPmcj2e7DOD+vVL3Sc9vSUItVrNggUL2LBhAy4uLgwaNIjAwEDq1q2r2SYiIoLo6Gj27NnD2bNnmT9/Pj/88ANWVlZs3LgRe3t7MjIyGDFiBJ07d6Z58+b6Ctfg7O3lq5yWL5dvp6YW7g0p+/6LFwv36TT7f+PpxFGxIiQluRS0XESZo1ZDTEwN1Orcv8sHDwq+iMrCIvfvr2pV+X+9MMnazs64HxCjoq7i7e2tmfP3+LH8lZRUtJ8fP5bnpOV8zFhnRdr9VxjN3Fz+X7Ozk7/n/Mr7Pnvs7ev++wX2tcGu8bPb2ZunYvcoDouEHIkkPv6/weTGjctEzTO9JYjIyEg8PT3x8JBnBwYFBREeHp4rQYSHhxMcHIxCoaB58+Y8evSIhIQEnJ2dsbeXJ6lkZmaSmZlpMlPP9cXG5r+z16JKSSnaJ9/bt+Xv6elOpe0DTYkoFGBra4uLi/xG7uVVuLMxBweT7gUoFIVC7jqyti7UAm2FkpFRcFIp7BmLrsXG/k2FCtV58gTNV3IyuW4/eSInvKfv13a19X9sgFpYWdXKlTiM9f/k6+vM11/rvl29JYj4+HhcXV01t11cXIiMjCxwG1dXV+Lj43F2dkatVjNgwABu3brFiBEjaNasmdbnTEtLK/bCGampqXpbdMNQzM3lf/7CvgGkpqZik2P2aXlQlGPOftMozQz1d21m9l8vjrHJr/GDIu8nSZCRoSAlRUFKipnmKzlZQWpq9m0Fycnyz/J9/91OSTErsLtRn1xdk4mKStC+YRHpLUHkNf/u6bOAgrYxNzcnNDSUR48e8dprr3H58mXqa1kZzdra2mQmypUG4pjLvvJ2vFBej/l2id778qO34UtXV1fi4uI0t7PPDAraJi4u7pltnJycaNu2LX/88Ye+QhUEQRDyoLcE4ePjQ3R0NDExMaSnpxMWFkZgYGCubQIDAwkJCUGSJM6cOYOjoyPOzs4kJiby6N8CcqmpqRw+fBgvMelKEATBoPTWxWRhYcHcuXMZP348arWagQMHUq9ePbZu3QrA8OHD8ff35+DBgyiVSmxtbVmyZAkACQkJzJw5E7VajSRJ9OzZky6iEqYgCIJB6XUehL+/P/7+/rnuGz58uOZnhULBvHnzntmvYcOGhISE6DM0QRAEQQsxhUoQBEHIk0gQgiAIQp5EghAEQRDyJBKEIAiCkKcytaLcmTNnsC6ziywIgiDoXlpaWr517spUghAEQRB0R3QxCYIgCHkSCUIQBEHIk0gQgiAIQp5EghAEQRDyJBKEIAiCkCeRIARBEIQ8lfsEERERQY8ePVAqlaxdu9bY4ejd33//zahRo+jVqxdBQUFs3LjR2CEZjFqtJjg4mIkTJxo7FIN49OgRkydPpmfPnvTq1YvTp08bOyS9+/rrrwkKCqJPnz5MnTqVNNNdOLvYZs2aRbt27ejTp4/mvgcPHjB27Fi6d+/O2LFjefjwoU6eq1wnCLVazYIFC1i3bh1hYWHs2rWLq1evGjssvTI3N2fmzJn88ssvfP/992zZsqXMH3O2TZs2UadOHWOHYTCLFy+mU6dO/Prrr4SGhpb5Y4+Pj2fTpk1s376dXbt2oVarCQsLM3ZYOjdgwADWrVuX6761a9fSrl079uzZQ7t27XT2YbdcJ4jIyEg8PT3x8PDAysqKoKAgwsPDjR2WXjk7O9O4cWMAHBwc8PLyIj4+3shR6V9cXBwHDhxg0KBBxg7FIJKSkjhx4oTmeK2srHBycjJyVPqnVqtJTU0lMzOT1NTUZ1aoLAt8fX2p8NTi3+Hh4QQHBwMQHBzMvn37dPJc5TpBxMfH4+rqqrnt4uJSLt4ss8XGxhIVFUWzZs2MHYreLVmyhBkzZmBmVj7+5GNiYqhcuTKzZs0iODiYd955h+TkZGOHpVcuLi6MGzeOLl260LFjRxwcHOjYsaOxwzKIe/fuaZJh9qqculA+/lvykVeVEYVCYYRIDO/JkydMnjyZ2bNn4+DgYOxw9Or333+ncuXKNGnSxNihGExmZiYXL15k+PDhhISEYGtrW+bH2B4+fEh4eDjh4eH88ccfpKSkEBoaauywSrVynSBcXV2Ji4vT3I6Pjy+Tp6RPy8jIYPLkyfTt25fu3bsbOxy9O3XqFPv37ycwMJCpU6dy9OhRpk+fbuyw9MrV1RVXV1fN2WHPnj25ePGikaPSr8OHD+Pu7k7lypWxtLSke/fu5WJgHqBKlSokJCQA8pLNlStX1km75TpB+Pj4EB0dTUxMDOnp6YSFhREYGGjssPRKkiTeeecdvLy8GDt2rLHDMYhp06YRERHB/v37+fjjj/Hz82P58uXGDkuvqlWrhqurK9evXwfgyJEjZX6QukaNGpw9e5aUlBQkSSoXx5wtMDBQs0xzSEgIXbt21Um7el2T2tRZWFgwd+5cxo8fj1qtZuDAgdSrV8/YYenVn3/+SWhoKPXr16dfv34ATJ069Zm1w4XSb86cOUyfPp2MjAw8PDxYunSpsUPSq2bNmtGjRw/69++PhYUF3t7eDB061Nhh6dzUqVM5fvw49+/fp3Pnzrz++utMmDCBN954gx9//JHq1avz6aef6uS5RLlvQRAEIU/luotJEARByJ9IEIIgCEKeRIIQBEEQ8iQShCAIgpAnkSAEQRCEPJXry1wF4e7duyxdupQzZ85QoUIFLC0tGT9+PEql0uCxHDt2DEtLS1q2bAnA1q1bsbW11dTYEQRDEwlCKLckSeK1114jODiYjz76CIDbt2+zf/9+vT1nZmYmFhZ5/9sdP34cOzs7TYIYPny43uIQhMIQ8yCEcuvIkSOsXr2azZs3P/OYWq1m+fLlHD9+nPT0dJ5//nmGDRvGsWPH+Oyzz6hUqRKXL1+mcePGLF++HIVCwfnz51m2bBnJyclUqlSJpUuX4uzszKhRo2jRogWnTp0iMDCQWrVq8fnnn5ORkUHFihVZvnw5qampDB06FDMzMypXrsycOXM4cuQIdnZ2vPjii0RFRTFv3jxSUlKoWbMmS5YsoUKFCowaNYqmTZty7NgxHj9+zOLFi2ndurURfptCWSTGIIRy68qVKzRq1CjPx3788UccHR3Zvn0727dvZ9u2bcTExABw8eJFZs+eze7du4mNjeXPP/8kIyODRYsWsXLlSnbs2MHAgQNZsWKFpr1Hjx6xefNmxo0bR6tWrdi2bRshISEEBQWxbt063N3dGTZsGGPGjCE0NPSZN/m33nqL6dOns3PnTurXr89nn32meUytVvPjjz8ye/bsXPcLQkmJLiZB+Nd7773Hn3/+iaWlJW5ubvz111/89ttvADx+/JibN29iaWlJ06ZNNWXiGzZsyO3bt3FycuLy5cua+lZZWVlUq1ZN03bv3r01P8fFxfHmm2/yzz//kJ6ejru7e4FxPX78mMePH9OmTRsA+vfvz5QpUzSPZ4+XNG7cmNu3b+vgNyEIMpEghHKrXr167NmzR3N73rx5JCYmMmjQIGrUqMG7775Lp06dcu1z7NgxrKysNLfNzc1Rq9VIkkS9evX4/vvv83wuW1tbzc+LFi1izJgxdO3aVdNlVRLZ8ZiZmaFWq0vUliDkJLqYhHLLz8+PtLQ0tmzZorkvNTUVgI4dO7J161YyMjIAuHHjRoEL7tSuXZvExERNeemMjAyuXLmS57aPHz/GxcUFQFOBE8De3p4nT548s72joyNOTk6cPHkSgNDQUHx9fYtwpIJQPOIMQii3FAoFq1evZunSpaxbt47KlStja2vL9OnT6dmzJ7dv32bAgAFIkkSlSpVYs2ZNvm1ZWVmxcuVKFi1axOPHj1Gr1bzwwgt5VgeeNGkSU6ZMwcXFhWbNmhEbGwtAly5dmDx5MuHh4cyZMyfXPu+//75mkLo8VGYVTIO4ikkQBEHIk+hiEgRBEPIkEoQgCIKQJ5EgBEEQhDyJBCEIgiDkSSQIQRAEIU8iQQiCIAh5EglCEARByNP/AQ9AFYEMC/phAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 73.94479100306829 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 2 , Number of neurons: 100\n",
      "Batch size 2 , Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030163</td>\n",
       "      <td>0.030163</td>\n",
       "      <td>113.261509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030409</td>\n",
       "      <td>0.030409</td>\n",
       "      <td>102.930393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030584</td>\n",
       "      <td>0.030584</td>\n",
       "      <td>77.163347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030988</td>\n",
       "      <td>0.030988</td>\n",
       "      <td>79.521142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031076</td>\n",
       "      <td>0.031076</td>\n",
       "      <td>143.692354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031111</td>\n",
       "      <td>0.031111</td>\n",
       "      <td>101.858045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031594</td>\n",
       "      <td>0.031594</td>\n",
       "      <td>143.896650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>66.851482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032342</td>\n",
       "      <td>0.032342</td>\n",
       "      <td>74.993960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032488</td>\n",
       "      <td>0.032488</td>\n",
       "      <td>70.071023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032745</td>\n",
       "      <td>0.032745</td>\n",
       "      <td>142.088252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032913</td>\n",
       "      <td>0.032913</td>\n",
       "      <td>107.136306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033081</td>\n",
       "      <td>0.033081</td>\n",
       "      <td>144.025668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033118</td>\n",
       "      <td>0.033118</td>\n",
       "      <td>99.566279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033212</td>\n",
       "      <td>0.033212</td>\n",
       "      <td>113.590836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033571</td>\n",
       "      <td>0.033571</td>\n",
       "      <td>144.030499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033607</td>\n",
       "      <td>0.033607</td>\n",
       "      <td>110.944231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033683</td>\n",
       "      <td>0.033683</td>\n",
       "      <td>145.226702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034068</td>\n",
       "      <td>0.034068</td>\n",
       "      <td>70.308023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034157</td>\n",
       "      <td>0.034157</td>\n",
       "      <td>107.791850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034339</td>\n",
       "      <td>0.034339</td>\n",
       "      <td>263.820187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034450</td>\n",
       "      <td>0.034450</td>\n",
       "      <td>92.936734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034621</td>\n",
       "      <td>0.034621</td>\n",
       "      <td>84.143864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034658</td>\n",
       "      <td>0.034658</td>\n",
       "      <td>125.608746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035430</td>\n",
       "      <td>0.035430</td>\n",
       "      <td>107.925148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035573</td>\n",
       "      <td>0.035573</td>\n",
       "      <td>143.446523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035638</td>\n",
       "      <td>0.035638</td>\n",
       "      <td>105.932747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036177</td>\n",
       "      <td>0.036177</td>\n",
       "      <td>57.485553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036894</td>\n",
       "      <td>0.036894</td>\n",
       "      <td>93.696761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037177</td>\n",
       "      <td>0.037177</td>\n",
       "      <td>109.063683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>85.172410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.046421</td>\n",
       "      <td>0.046421</td>\n",
       "      <td>143.383802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.051973</td>\n",
       "      <td>0.051973</td>\n",
       "      <td>42.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.057806</td>\n",
       "      <td>0.057806</td>\n",
       "      <td>37.237482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.069472</td>\n",
       "      <td>0.069472</td>\n",
       "      <td>265.741863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.070688</td>\n",
       "      <td>0.070688</td>\n",
       "      <td>83.869955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.074041</td>\n",
       "      <td>0.074041</td>\n",
       "      <td>83.152456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.077912</td>\n",
       "      <td>0.077912</td>\n",
       "      <td>78.324144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.085723</td>\n",
       "      <td>0.085723</td>\n",
       "      <td>70.825453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.094878</td>\n",
       "      <td>0.094878</td>\n",
       "      <td>203.498367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             4         50         0.0001           2  0.030163  0.030163   \n",
       "1             4        100         0.0001           4  0.030409  0.030409   \n",
       "2             4        100         0.0001           4  0.030584  0.030584   \n",
       "3             4        100         0.0001           4  0.030988  0.030988   \n",
       "4             4        100         0.0001           4  0.031076  0.031076   \n",
       "5             4         50         0.0001           2  0.031111  0.031111   \n",
       "6             4        100         0.0001           4  0.031594  0.031594   \n",
       "7             4        100         0.0001           4  0.031893  0.031893   \n",
       "8             4        100         0.0001           4  0.032342  0.032342   \n",
       "9             4         50         0.0001           4  0.032488  0.032488   \n",
       "10            4        200         0.0001           4  0.032745  0.032745   \n",
       "11            4         50         0.0001           2  0.032913  0.032913   \n",
       "12            4        100         0.0001           4  0.033081  0.033081   \n",
       "13            4        100         0.0001           4  0.033118  0.033118   \n",
       "14            4        100         0.0001           4  0.033212  0.033212   \n",
       "15            4        100         0.0001           4  0.033571  0.033571   \n",
       "16            4        100         0.0001           4  0.033607  0.033607   \n",
       "17            4        100         0.0001           4  0.033683  0.033683   \n",
       "18            4        100         0.0001           4  0.034068  0.034068   \n",
       "19            4         50         0.0001           2  0.034157  0.034157   \n",
       "20            4        100         0.0001           2  0.034339  0.034339   \n",
       "21            4        100         0.0001           4  0.034450  0.034450   \n",
       "22            4        100         0.0001           4  0.034621  0.034621   \n",
       "23            4        100         0.0001           4  0.034658  0.034658   \n",
       "24            4        100         0.0001           4  0.035430  0.035430   \n",
       "25            2        100         0.0001           4  0.035573  0.035573   \n",
       "26            2        100         0.0001           4  0.035638  0.035638   \n",
       "27            2        100         0.0001           4  0.036177  0.036177   \n",
       "28            2        150         0.0001           4  0.036894  0.036894   \n",
       "29            4        100         0.0010           4  0.037177  0.037177   \n",
       "30            2        100         0.0001           4  0.037400  0.037400   \n",
       "31            2        200         0.0010           4  0.046421  0.046421   \n",
       "32            2        200         0.0001          16  0.051973  0.051973   \n",
       "33            2        200         0.0001          16  0.057806  0.057806   \n",
       "34            4        100         0.0010           2  0.069472  0.069472   \n",
       "35            4        200         0.0010          16  0.070688  0.070688   \n",
       "36            1        100         0.0001           4  0.074041  0.074041   \n",
       "37            1        100         0.0001           4  0.077912  0.077912   \n",
       "38            1        100         0.0001           4  0.085723  0.085723   \n",
       "39            3        100         0.0010           2  0.094878  0.094878   \n",
       "\n",
       "    Elapsed time  \n",
       "0     113.261509  \n",
       "1     102.930393  \n",
       "2      77.163347  \n",
       "3      79.521142  \n",
       "4     143.692354  \n",
       "5     101.858045  \n",
       "6     143.896650  \n",
       "7      66.851482  \n",
       "8      74.993960  \n",
       "9      70.071023  \n",
       "10    142.088252  \n",
       "11    107.136306  \n",
       "12    144.025668  \n",
       "13     99.566279  \n",
       "14    113.590836  \n",
       "15    144.030499  \n",
       "16    110.944231  \n",
       "17    145.226702  \n",
       "18     70.308023  \n",
       "19    107.791850  \n",
       "20    263.820187  \n",
       "21     92.936734  \n",
       "22     84.143864  \n",
       "23    125.608746  \n",
       "24    107.925148  \n",
       "25    143.446523  \n",
       "26    105.932747  \n",
       "27     57.485553  \n",
       "28     93.696761  \n",
       "29    109.063683  \n",
       "30     85.172410  \n",
       "31    143.383802  \n",
       "32     42.201200  \n",
       "33     37.237482  \n",
       "34    265.741863  \n",
       "35     83.869955  \n",
       "36     83.152456  \n",
       "37     78.324144  \n",
       "38     70.825453  \n",
       "39    203.498367  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla1.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 73.940 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
