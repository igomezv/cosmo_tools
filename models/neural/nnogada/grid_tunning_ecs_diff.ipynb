{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gS9bn7bhqpCG",
    "outputId": "38bcc44c-8019-449a-e1ee-2de1e086c68e"
   },
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iD0I8HwdqpCb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "import scipy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aFY-8_lrqpCg"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fsRv8kuNqpCw",
    "outputId": "773d68d4-3c4e-42e4-c9ca-be5c60ac5412"
   },
   "outputs": [],
   "source": [
    "#algorithm for splitting the dataset into training and validation \n",
    "def split(X,Y,porcent): #porcent must be between 0 and 1, it is the asigned porcent to the training dataset.\n",
    "    n=floor(porcent*len(X))\n",
    "    index=random.sample(range(len(X)),n)\n",
    "    X_learn=[]\n",
    "    Y_learn=[]\n",
    "    for i in index:\n",
    "        X_learn.append(X[i])\n",
    "        Y_learn.append(Y[i])\n",
    "    X_val=np.delete(X,index, axis=0)\n",
    "    Y_val=np.delete(Y,index, axis=0)\n",
    "    \n",
    "    X_learn=np.array(X_learn)\n",
    "    Y_learn=np.array(Y_learn)\n",
    "    return X_learn,Y_learn,X_val,Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_m=np.arange(0.1,0.51,0.1)\n",
    "H_0=np.arange(66,81,1)\n",
    "t=np.linspace(0,-12,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RHS(Omega_i, lna, gamma=0):\n",
    "    x, y, z, H = Omega_i\n",
    "    #x, y, z = Omega_i\n",
    "    pi = 3*x + 4*y\n",
    "    return [x*(-3 + pi), y*(-4 + pi), z*pi, -0.5*H*pi]\n",
    "    #return [x*(-3 + pi), y*(-4 + pi), z*pi]\n",
    "\n",
    "def EDO(t,Om,H0):\n",
    "    #t,Or,Om,Ol=X\n",
    "    Or=0.0001\n",
    "    Ol=1-Or-Om\n",
    "    #H0 = 70.\n",
    "    y0 = [Om, Or, Ol, H0]\n",
    "    result = odeint(RHS, y0, t)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets generate the cartesian product between the intervals\n",
    "Y0=[]\n",
    "#este ciclo llena la lista fijando un Om y pasando todos los Or\n",
    "for i in O_m:\n",
    "    for j in H_0:\n",
    "        Y0.extend(EDO(t,i,j))\n",
    "Y0=np.array(Y0)\n",
    "\n",
    "X0=[]\n",
    "for Om in O_m:\n",
    "    for H0 in H_0:\n",
    "        for T in t:\n",
    "            X0.append([T,Om,H0])\n",
    "X0=np.array(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "Y2 = scaler.fit_transform(Y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feactures= \n",
      " [[  0.           0.1         66.        ]\n",
      " [ -0.24489796   0.1         66.        ]\n",
      " [ -0.48979592   0.1         66.        ]\n",
      " ...\n",
      " [-11.51020408   0.5         80.        ]\n",
      " [-11.75510204   0.5         80.        ]\n",
      " [-12.           0.5         80.        ]]\n",
      "\n",
      "\n",
      "labels= \n",
      " [[9.47515373e-02 0.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      " [1.83608702e-01 1.41110961e-04 9.01997379e-01 1.62358493e-10]\n",
      " [3.22333902e-01 4.33937896e-04 7.48914062e-01 4.77144941e-10]\n",
      " ...\n",
      " [4.20164900e-02 9.58104047e-01 5.58400899e-11 3.78984027e-01]\n",
      " [3.19464665e-02 9.68145182e-01 5.73926010e-11 6.15280156e-01]\n",
      " [2.39153274e-02 9.76153281e-01 5.75107557e-11 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Now, here are the datasets\n",
    "print('feactures= \\n',X0)\n",
    "print('\\n')\n",
    "print('labels= \\n',Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = 0.8\n",
    "X_train, Y_train, X_test, Y_test = split(X0, Y2, split_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgjOFB8kqpC_"
   },
   "source": [
    "### Hiperparámetros del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ABViOAgTqpDI"
   },
   "outputs": [],
   "source": [
    "# HP_NUM_UNITS3 = hp.HParam('num_units3', hp.Discrete([50, 100, 150, 200]))\n",
    "# HP_NUM_UNITS4 = hp.HParam('num_units4', hp.Discrete([2, 5, 10]))\n",
    "# HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.0, 0.2))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'Adadelta']))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
    "HP_BATCHSIZE = hp.HParam('batch_size', hp.Discrete([8, 16]))\n",
    "\n",
    "\n",
    "HP_LAYERS =    hp.HParam('layers', hp.Discrete([1, 2, 3, 4]))\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([50, 100, 150, 200]))\n",
    "HP_LEARNING  = hp.HParam('learning_rate', hp.Discrete([1e-4,1e-3]))\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n",
    "                                   min_delta=0,\n",
    "                                   patience=100,\n",
    "                                   restore_best_weights=True)]\n",
    "# batch_size = 128\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lXRw_6G8qpDL"
   },
   "outputs": [],
   "source": [
    "# METRIC_ACCURACY = 'accuracy'\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning2').as_default():\n",
    "# with tf.summary.FileWriter('logs/hparam_tuning', sess.graph):\n",
    "#     init = tf.initialize_all_variables()\n",
    "#     sess.run(init)\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_LAYERS,\n",
    "                 HP_NUM_UNITS,\n",
    "                 HP_LEARNING, \n",
    "                 HP_BATCHSIZE],\n",
    "        metrics=[hp.Metric('loss', display_name=\"Loss\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6JG3WeEOqpDY"
   },
   "outputs": [],
   "source": [
    "def train_test_model(hparams):    \n",
    "    \n",
    "    # Train LSTM model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(hparams[HP_LAYERS]):        \n",
    "        model.add(Dense(hparams[HP_NUM_UNITS], activation='relu'))\n",
    "    model.add(Dense(4, activation='linear'))\n",
    "     \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING], beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse', \n",
    "            metrics=['mean_squared_error'])\n",
    "    \n",
    "    # Run with 1 epoch to speed things up for demo purposes\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_test, Y_test),\n",
    "              callbacks=callbacks, batch_size=hparams[HP_BATCHSIZE], shuffle=False, verbose=0)\n",
    "\n",
    "    _, loss = model.evaluate(X_test, Y_test)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fnLlAKGuqpDp"
   },
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(\"loss\", loss, step=1)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tu8q13lqpDs",
    "outputId": "80f9eb4f-dc3a-445e-ff6d-3d42dcb085c3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting trial: run-0\n",
      "{'layers': 1, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0085 - mean_squared_error: 0.0085\n",
      "Loss: 0.008543111383914948 Tiempo transcurrido: 236.4396116733551\n",
      "\n",
      "--- Starting trial: run-1\n",
      "{'layers': 1, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Loss: 0.020129911601543427 Tiempo transcurrido: 132.40395164489746\n",
      "\n",
      "--- Starting trial: run-2\n",
      "{'layers': 1, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 903us/step - loss: 5.0475e-04 - mean_squared_error: 5.0475e-04\n",
      "Loss: 0.0005047536105848849 Tiempo transcurrido: 244.10622334480286\n",
      "\n",
      "--- Starting trial: run-3\n",
      "{'layers': 1, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 927us/step - loss: 0.0024 - mean_squared_error: 0.0024\n",
      "Loss: 0.0023873005993664265 Tiempo transcurrido: 134.5154173374176\n",
      "\n",
      "--- Starting trial: run-4\n",
      "{'layers': 1, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0025 - mean_squared_error: 0.0025\n",
      "Loss: 0.002515699015930295 Tiempo transcurrido: 227.44119453430176\n",
      "\n",
      "--- Starting trial: run-5\n",
      "{'layers': 1, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Loss: 0.005618899594992399 Tiempo transcurrido: 131.9006838798523\n",
      "\n",
      "--- Starting trial: run-6\n",
      "{'layers': 1, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.5746e-04 - mean_squared_error: 1.5746e-04\n",
      "Loss: 0.00015745814016554505 Tiempo transcurrido: 246.2751863002777\n",
      "\n",
      "--- Starting trial: run-7\n",
      "{'layers': 1, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0035 - mean_squared_error: 0.0035\n",
      "Loss: 0.003493414493277669 Tiempo transcurrido: 136.13734102249146\n",
      "\n",
      "--- Starting trial: run-8\n",
      "{'layers': 1, 'num_units': 150, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0040 - mean_squared_error: 0.0040\n",
      "Loss: 0.003958177752792835 Tiempo transcurrido: 238.26127219200134\n",
      "\n",
      "--- Starting trial: run-9\n",
      "{'layers': 1, 'num_units': 150, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0034 - mean_squared_error: 0.0034\n",
      "Loss: 0.0034160150680691004 Tiempo transcurrido: 129.85631728172302\n",
      "\n",
      "--- Starting trial: run-10\n",
      "{'layers': 1, 'num_units': 150, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 3.5677e-04 - mean_squared_error: 3.5677e-04\n",
      "Loss: 0.0003567652020137757 Tiempo transcurrido: 226.27942061424255\n",
      "\n",
      "--- Starting trial: run-11\n",
      "{'layers': 1, 'num_units': 150, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 2.0689e-04 - mean_squared_error: 2.0689e-04\n",
      "Loss: 0.00020688833319582045 Tiempo transcurrido: 129.8748743534088\n",
      "\n",
      "--- Starting trial: run-12\n",
      "{'layers': 1, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0027 - mean_squared_error: 0.0027\n",
      "Loss: 0.002685965970158577 Tiempo transcurrido: 241.1950261592865\n",
      "\n",
      "--- Starting trial: run-13\n",
      "{'layers': 1, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Loss: 0.005341167561709881 Tiempo transcurrido: 142.5410132408142\n",
      "\n",
      "--- Starting trial: run-14\n",
      "{'layers': 1, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 8.6037e-05 - mean_squared_error: 8.6037e-05\n",
      "Loss: 8.603740570833907e-05 Tiempo transcurrido: 229.08378982543945\n",
      "\n",
      "--- Starting trial: run-15\n",
      "{'layers': 1, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.0240e-04 - mean_squared_error: 1.0240e-04\n",
      "Loss: 0.0001023991935653612 Tiempo transcurrido: 133.44994854927063\n",
      "\n",
      "--- Starting trial: run-16\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 991us/step - loss: 0.0029 - mean_squared_error: 0.0029\n",
      "Loss: 0.0028684346470981836 Tiempo transcurrido: 251.2429645061493\n",
      "\n",
      "--- Starting trial: run-17\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0100 - mean_squared_error: 0.0100\n",
      "Loss: 0.010001116432249546 Tiempo transcurrido: 134.80538296699524\n",
      "\n",
      "--- Starting trial: run-18\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 947us/step - loss: 4.1589e-05 - mean_squared_error: 4.1589e-05\n",
      "Loss: 4.1588620661059394e-05 Tiempo transcurrido: 244.16978764533997\n",
      "\n",
      "--- Starting trial: run-19\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 9.5538e-05 - mean_squared_error: 9.5538e-05\n",
      "Loss: 9.553778363624588e-05 Tiempo transcurrido: 137.1475808620453\n",
      "\n",
      "--- Starting trial: run-20\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0023 - mean_squared_error: 0.0023\n",
      "Loss: 0.002252353588119149 Tiempo transcurrido: 282.46985483169556\n",
      "\n",
      "--- Starting trial: run-21\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0022 - mean_squared_error: 0.0022\n",
      "Loss: 0.0022186175920069218 Tiempo transcurrido: 153.64702558517456\n",
      "\n",
      "--- Starting trial: run-22\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 4.7529e-04 - mean_squared_error: 4.7529e-04\n",
      "Loss: 0.0004752879904117435 Tiempo transcurrido: 156.24115586280823\n",
      "\n",
      "--- Starting trial: run-23\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 5.7415e-05 - mean_squared_error: 5.7415e-05\n",
      "Loss: 5.741481436416507e-05 Tiempo transcurrido: 148.40593218803406\n",
      "\n",
      "--- Starting trial: run-24\n",
      "{'layers': 2, 'num_units': 150, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0033 - mean_squared_error: 0.0033\n",
      "Loss: 0.0032578683458268642 Tiempo transcurrido: 240.63716983795166\n",
      "\n",
      "--- Starting trial: run-25\n",
      "{'layers': 2, 'num_units': 150, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Loss: 0.005212865304201841 Tiempo transcurrido: 153.78174901008606\n",
      "\n",
      "--- Starting trial: run-26\n",
      "{'layers': 2, 'num_units': 150, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.5704e-04 - mean_squared_error: 1.5704e-04\n",
      "Loss: 0.00015703952522017062 Tiempo transcurrido: 203.2317111492157\n",
      "\n",
      "--- Starting trial: run-27\n",
      "{'layers': 2, 'num_units': 150, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 3.6267e-05 - mean_squared_error: 3.6267e-05\n",
      "Loss: 3.6266501410864294e-05 Tiempo transcurrido: 158.20571565628052\n",
      "\n",
      "--- Starting trial: run-28\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0022 - mean_squared_error: 0.0022\n",
      "Loss: 0.0022456729784607887 Tiempo transcurrido: 295.6466908454895\n",
      "\n",
      "--- Starting trial: run-29\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0030 - mean_squared_error: 0.0030\n",
      "Loss: 0.002963722450658679 Tiempo transcurrido: 157.65183901786804\n",
      "\n",
      "--- Starting trial: run-30\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.8650e-04 - mean_squared_error: 1.8650e-04\n",
      "Loss: 0.0001864966907305643 Tiempo transcurrido: 256.53088998794556\n",
      "\n",
      "--- Starting trial: run-31\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.8197e-04 - mean_squared_error: 1.8197e-04\n",
      "Loss: 0.00018197244207840413 Tiempo transcurrido: 162.14437985420227\n",
      "\n",
      "--- Starting trial: run-32\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 2.8201e-04 - mean_squared_error: 2.8201e-04\n",
      "Loss: 0.0002820090448949486 Tiempo transcurrido: 262.9895098209381\n",
      "\n",
      "--- Starting trial: run-33\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0044 - mean_squared_error: 0.0044\n",
      "Loss: 0.004356225486844778 Tiempo transcurrido: 141.6171510219574\n",
      "\n",
      "--- Starting trial: run-34\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011\n",
      "Loss: 0.0010621838737279177 Tiempo transcurrido: 143.00086188316345\n",
      "\n",
      "--- Starting trial: run-35\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 2.6276e-04 - mean_squared_error: 2.6276e-04\n",
      "Loss: 0.00026276404969394207 Tiempo transcurrido: 144.682124376297\n",
      "\n",
      "--- Starting trial: run-36\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018\n",
      "Loss: 0.0017832478042691946 Tiempo transcurrido: 274.5540554523468\n",
      "\n",
      "--- Starting trial: run-37\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0027 - mean_squared_error: 0.0027\n",
      "Loss: 0.0027133652474731207 Tiempo transcurrido: 153.2190899848938\n",
      "\n",
      "--- Starting trial: run-38\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 7.6517e-05 - mean_squared_error: 7.6517e-05\n",
      "Loss: 7.651715714018792e-05 Tiempo transcurrido: 280.1936707496643\n",
      "\n",
      "--- Starting trial: run-39\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 5.9298e-05 - mean_squared_error: 5.9298e-05\n",
      "Loss: 5.929764665779658e-05 Tiempo transcurrido: 154.22626376152039\n",
      "\n",
      "--- Starting trial: run-40\n",
      "{'layers': 3, 'num_units': 150, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 9.9276e-04 - mean_squared_error: 9.9276e-04\n",
      "Loss: 0.0009927621576935053 Tiempo transcurrido: 319.90534806251526\n",
      "\n",
      "--- Starting trial: run-41\n",
      "{'layers': 3, 'num_units': 150, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0031 - mean_squared_error: 0.0031\n",
      "Loss: 0.003081440692767501 Tiempo transcurrido: 143.86093044281006\n",
      "\n",
      "--- Starting trial: run-42\n",
      "{'layers': 3, 'num_units': 150, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 4.7328e-04 - mean_squared_error: 4.7328e-04\n",
      "Loss: 0.0004732801462523639 Tiempo transcurrido: 235.55122804641724\n",
      "\n",
      "--- Starting trial: run-43\n",
      "{'layers': 3, 'num_units': 150, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0024 - mean_squared_error: 0.0024\n",
      "Loss: 0.0023640317376703024 Tiempo transcurrido: 171.09704089164734\n",
      "\n",
      "--- Starting trial: run-44\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 9.6555e-04 - mean_squared_error: 9.6555e-04\n",
      "Loss: 0.000965548912063241 Tiempo transcurrido: 319.6092481613159\n",
      "\n",
      "--- Starting trial: run-45\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020\n",
      "Loss: 0.001965007046237588 Tiempo transcurrido: 159.0564410686493\n",
      "\n",
      "--- Starting trial: run-46\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.4067e-04 - mean_squared_error: 1.4067e-04\n",
      "Loss: 0.00014066802395973355 Tiempo transcurrido: 330.2700905799866\n",
      "\n",
      "--- Starting trial: run-47\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 3.9027e-05 - mean_squared_error: 3.9027e-05\n",
      "Loss: 3.902720709447749e-05 Tiempo transcurrido: 118.34995412826538\n",
      "\n",
      "--- Starting trial: run-48\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 991us/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "Loss: 0.001525372383184731 Tiempo transcurrido: 251.285569190979\n",
      "\n",
      "--- Starting trial: run-49\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0021 - mean_squared_error: 0.0021\n",
      "Loss: 0.0020662571769207716 Tiempo transcurrido: 141.78075098991394\n",
      "\n",
      "--- Starting trial: run-50\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 946us/step - loss: 1.6314e-04 - mean_squared_error: 1.6314e-04\n",
      "Loss: 0.00016314099775627255 Tiempo transcurrido: 184.71923685073853\n",
      "\n",
      "--- Starting trial: run-51\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 5.6748e-05 - mean_squared_error: 5.6748e-05\n",
      "Loss: 5.674764179275371e-05 Tiempo transcurrido: 143.34595394134521\n",
      "\n",
      "--- Starting trial: run-52\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011\n",
      "Loss: 0.001094077480956912 Tiempo transcurrido: 326.3364279270172\n",
      "\n",
      "--- Starting trial: run-53\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0017 - mean_squared_error: 0.0017\n",
      "Loss: 0.0017027222784236073 Tiempo transcurrido: 157.88914680480957\n",
      "\n",
      "--- Starting trial: run-54\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 3.5746e-05 - mean_squared_error: 3.5746e-05\n",
      "Loss: 3.574568108888343e-05 Tiempo transcurrido: 295.93709206581116\n",
      "\n",
      "--- Starting trial: run-55\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 6.8361e-05 - mean_squared_error: 6.8361e-05\n",
      "Loss: 6.836149987066165e-05 Tiempo transcurrido: 164.45650553703308\n",
      "\n",
      "--- Starting trial: run-56\n",
      "{'layers': 4, 'num_units': 150, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020\n",
      "Loss: 0.002013684483245015 Tiempo transcurrido: 193.77055191993713\n",
      "\n",
      "--- Starting trial: run-57\n",
      "{'layers': 4, 'num_units': 150, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0027 - mean_squared_error: 0.0027\n",
      "Loss: 0.0027028110343962908 Tiempo transcurrido: 126.97223234176636\n",
      "\n",
      "--- Starting trial: run-58\n",
      "{'layers': 4, 'num_units': 150, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 5.0845e-05 - mean_squared_error: 5.0845e-05\n",
      "Loss: 5.0844872021116316e-05 Tiempo transcurrido: 283.0646963119507\n",
      "\n",
      "--- Starting trial: run-59\n",
      "{'layers': 4, 'num_units': 150, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 7.1815e-05 - mean_squared_error: 7.1815e-05\n",
      "Loss: 7.181494584074244e-05 Tiempo transcurrido: 190.81421422958374\n",
      "\n",
      "--- Starting trial: run-60\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step - loss: 7.7936e-05 - mean_squared_error: 7.7936e-05\n",
      "Loss: 7.793631812091917e-05 Tiempo transcurrido: 316.38970851898193\n",
      "\n",
      "--- Starting trial: run-61\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013\n",
      "Loss: 0.001325038610957563 Tiempo transcurrido: 179.74763536453247\n",
      "\n",
      "--- Starting trial: run-62\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 4.4110e-05 - mean_squared_error: 4.4110e-05\n",
      "Loss: 4.411049667396583e-05 Tiempo transcurrido: 314.2203879356384\n",
      "\n",
      "--- Starting trial: run-63\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 6.8318e-05 - mean_squared_error: 6.8318e-05\n",
      "Loss: 6.831753125879914e-05 Tiempo transcurrido: 147.12055802345276\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "datos = []\n",
    "t0 = time.time()\n",
    "for deep_layers in HP_LAYERS.domain.values:\n",
    "    for num_units in HP_NUM_UNITS.domain.values:\n",
    "        for learning_rate in HP_LEARNING.domain.values:\n",
    "            for batch_size in HP_BATCHSIZE.domain.values:\n",
    "                t = time.time()\n",
    "                hparams = {\n",
    "\n",
    "                    HP_LAYERS: deep_layers,\n",
    "                    HP_NUM_UNITS: num_units,\n",
    "                    HP_LEARNING: learning_rate,\n",
    "                    HP_BATCHSIZE: batch_size,\n",
    "                }\n",
    "                run_name = \"run-%d\" % session_num\n",
    "                print('\\n--- Starting trial: %s' % run_name)\n",
    "                print({h.name: hparams[h] for h in hparams})\n",
    "                score = run('logs/hparam_tuning2/' + run_name, hparams)\n",
    "                t = time.time()-t\n",
    "                session_num += 1\n",
    "                print(\"Loss:\", score, \"Tiempo transcurrido:\", t)\n",
    "            \n",
    "            datos.append([deep_layers, num_units, learning_rate, batch_size, score, t])\n",
    "\n",
    "print(session_num)\n",
    "tf = time.time() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1ct8OIfqpD3"
   },
   "source": [
    "### Guardar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_6xZZqEhqpD5"
   },
   "outputs": [],
   "source": [
    "filename = \"historial_ecsdiff_tunning.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep size\", \"Num units\", \"Learning rate\", \"Batch size\", \"MSE\", \"Tiempo de ejecución\"])\n",
    "\n",
    "df.sort_values(by=[\"MSE\", \"Tiempo de ejecución\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "z9BerX2yqpD-",
    "outputId": "992e08c9-0adb-4f57-e50e-df65c91fe142",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep size</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Tiempo de ejecución</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>158.205716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>118.349954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>143.345954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>148.405932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>154.226264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>147.120558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>164.456506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>190.814214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>137.147581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>133.449949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>162.144380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>129.874874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>144.682124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>179.747635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>157.889147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>159.056441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>141.780751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>153.647026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>171.097041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>134.515417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>126.972232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>153.219090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>157.651839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>143.860930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>129.856317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>136.137341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>141.617151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.005213</td>\n",
       "      <td>153.781749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.005341</td>\n",
       "      <td>142.541013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.005619</td>\n",
       "      <td>131.900684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>134.805383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.020130</td>\n",
       "      <td>132.403952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep size  Num units  Learning rate  Batch size       MSE  \\\n",
       "0           2        150         0.0010          16  0.000036   \n",
       "1           3        200         0.0010          16  0.000039   \n",
       "2           4         50         0.0010          16  0.000057   \n",
       "3           2        100         0.0010          16  0.000057   \n",
       "4           3        100         0.0010          16  0.000059   \n",
       "5           4        200         0.0010          16  0.000068   \n",
       "6           4        100         0.0010          16  0.000068   \n",
       "7           4        150         0.0010          16  0.000072   \n",
       "8           2         50         0.0010          16  0.000096   \n",
       "9           1        200         0.0010          16  0.000102   \n",
       "10          2        200         0.0010          16  0.000182   \n",
       "11          1        150         0.0010          16  0.000207   \n",
       "12          3         50         0.0010          16  0.000263   \n",
       "13          4        200         0.0001          16  0.001325   \n",
       "14          4        100         0.0001          16  0.001703   \n",
       "15          3        200         0.0001          16  0.001965   \n",
       "16          4         50         0.0001          16  0.002066   \n",
       "17          2        100         0.0001          16  0.002219   \n",
       "18          3        150         0.0010          16  0.002364   \n",
       "19          1         50         0.0010          16  0.002387   \n",
       "20          4        150         0.0001          16  0.002703   \n",
       "21          3        100         0.0001          16  0.002713   \n",
       "22          2        200         0.0001          16  0.002964   \n",
       "23          3        150         0.0001          16  0.003081   \n",
       "24          1        150         0.0001          16  0.003416   \n",
       "25          1        100         0.0010          16  0.003493   \n",
       "26          3         50         0.0001          16  0.004356   \n",
       "27          2        150         0.0001          16  0.005213   \n",
       "28          1        200         0.0001          16  0.005341   \n",
       "29          1        100         0.0001          16  0.005619   \n",
       "30          2         50         0.0001          16  0.010001   \n",
       "31          1         50         0.0001          16  0.020130   \n",
       "\n",
       "    Tiempo de ejecución  \n",
       "0            158.205716  \n",
       "1            118.349954  \n",
       "2            143.345954  \n",
       "3            148.405932  \n",
       "4            154.226264  \n",
       "5            147.120558  \n",
       "6            164.456506  \n",
       "7            190.814214  \n",
       "8            137.147581  \n",
       "9            133.449949  \n",
       "10           162.144380  \n",
       "11           129.874874  \n",
       "12           144.682124  \n",
       "13           179.747635  \n",
       "14           157.889147  \n",
       "15           159.056441  \n",
       "16           141.780751  \n",
       "17           153.647026  \n",
       "18           171.097041  \n",
       "19           134.515417  \n",
       "20           126.972232  \n",
       "21           153.219090  \n",
       "22           157.651839  \n",
       "23           143.860930  \n",
       "24           129.856317  \n",
       "25           136.137341  \n",
       "26           141.617151  \n",
       "27           153.781749  \n",
       "28           142.541013  \n",
       "29           131.900684  \n",
       "30           134.805383  \n",
       "31           132.403952  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo total: 214.43 min\n"
     ]
    }
   ],
   "source": [
    "print(\"Tiempo total: {:.2f} min\".format(tf/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xh9rbb8kqpED"
   },
   "outputs": [],
   "source": [
    "# rm -rf /tmp/tb_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm044iKXqpEM"
   },
   "source": [
    "### Now in terminal:\n",
    "`python3 -m tensorboard.main --logdir='/home/isidro/Documents/github/neurapprox/logs/hparam_tuning'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: kill: No such process\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "!kill 7439\n",
    "%tensorboard --logdir logs/hparam_tuning2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "tunning_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
