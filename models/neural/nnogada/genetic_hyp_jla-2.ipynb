{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 00:28:50.320577: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-18 00:28:50.420898: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:28:50.420936: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-18 00:28:51.116492: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:28:51.116556: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:28:51.116563: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,1e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.2         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 00:28:52.033634: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 00:28:52.033801: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:28:52.033855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:28:52.033903: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:28:52.033969: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:28:52.034080: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:28:52.034146: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:28:52.034200: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:28:52.034245: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-18 00:28:52.034252: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-18 00:28:52.035355: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0653 - mean_squared_error: 0.0653\n",
      "Loss: 0.0653250440955162 , Elapsed time: 66.0662853717804\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0509 - mean_squared_error: 0.0509\n",
      "Loss: 0.05091273784637451 , Elapsed time: 13.618656158447266\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0760 - mean_squared_error: 0.0760\n",
      "Loss: 0.07597292959690094 , Elapsed time: 17.94667673110962\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0648 - mean_squared_error: 0.0648\n",
      "Loss: 0.06479766219854355 , Elapsed time: 30.940158367156982\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Loss: 0.03691289573907852 , Elapsed time: 32.79180192947388\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax      \n",
      "0  \t5     \t0.0369129\t0.0587843\t0.0759729\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Loss: 0.0353902243077755 , Elapsed time: 37.42042398452759\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1300 - mean_squared_error: 0.1300\n",
      "Loss: 0.13001592457294464 , Elapsed time: 19.384774684906006\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t2     \t0.0353902\t0.0580289\t0.130016 \n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0501 - mean_squared_error: 0.0501\n",
      "Loss: 0.05013321712613106 , Elapsed time: 14.311742782592773\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Loss: 0.03562680631875992 , Elapsed time: 38.35105919837952\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t2     \t0.0353902\t0.0383861\t0.0501332\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0382 - mean_squared_error: 0.0382\n",
      "Loss: 0.03823214769363403 , Elapsed time: 34.72152853012085\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0565 - mean_squared_error: 0.0565\n",
      "Loss: 0.05645696073770523 , Elapsed time: 33.38976454734802\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t2     \t0.0353902\t0.0431206\t0.056457 \n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Loss: 0.03557099774479866 , Elapsed time: 39.805686712265015\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Loss: 0.03197338432073593 , Elapsed time: 70.32289981842041\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t2     \t0.0319734\t0.0353114\t0.0382321\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Loss: 0.030808739364147186 , Elapsed time: 70.5184862613678\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Loss: 0.03501225262880325 , Elapsed time: 41.16749382019043\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Loss: 0.03589974343776703 , Elapsed time: 82.95167970657349\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Loss: 0.031094925478100777 , Elapsed time: 203.43417406082153\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.0308087\t0.0329578\t0.0358997\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Loss: 0.03140030428767204 , Elapsed time: 203.68172478675842\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Loss: 0.030012404546141624 , Elapsed time: 204.3503475189209\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Loss: 0.030767327174544334 , Elapsed time: 180.9138000011444\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Loss: 0.03419089689850807 , Elapsed time: 113.62050414085388\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t4     \t0.0300124\t0.0316689\t0.0341909\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Loss: 0.031583212316036224 , Elapsed time: 183.00643825531006\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Loss: 0.03157315030694008 , Elapsed time: 170.03467106819153\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Loss: 0.030429063364863396 , Elapsed time: 186.77577233314514\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Loss: 0.030792761594057083 , Elapsed time: 190.637348651886\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.0304291\t0.0312703\t0.0319734\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Loss: 0.04625628888607025 , Elapsed time: 143.1523118019104\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t1     \t0.0304291\t0.0342049\t0.0462563\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Loss: 0.03241535276174545 , Elapsed time: 183.5813398361206\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Loss: 0.03004743903875351 , Elapsed time: 200.56205368041992\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Loss: 0.030869264155626297 , Elapsed time: 263.67645621299744\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Loss: 0.03574080765247345 , Elapsed time: 144.13003730773926\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t4     \t0.0300474\t0.0322092\t0.0357408\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Loss: 0.032246802002191544 , Elapsed time: 210.3583927154541\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Loss: 0.03112146630883217 , Elapsed time: 209.66389107704163\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Loss: 0.030902450904250145 , Elapsed time: 171.52970814704895\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t3     \t0.0309025\t0.0316435\t0.0322468\n",
      "-- Best Individual =  [0, 1, 1, 1, 0, 0, 0]\n",
      "-- Best Fitness =  0.03197338432073593\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 63.45176571210225 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 1 , Number of neurons: 100\n",
      "Batch size 4 , Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030012</td>\n",
       "      <td>0.030012</td>\n",
       "      <td>204.350348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030047</td>\n",
       "      <td>0.030047</td>\n",
       "      <td>200.562054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030429</td>\n",
       "      <td>0.030429</td>\n",
       "      <td>186.775772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030767</td>\n",
       "      <td>0.030767</td>\n",
       "      <td>180.913800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030793</td>\n",
       "      <td>0.030793</td>\n",
       "      <td>190.637349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030809</td>\n",
       "      <td>0.030809</td>\n",
       "      <td>70.518486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030869</td>\n",
       "      <td>0.030869</td>\n",
       "      <td>263.676456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030902</td>\n",
       "      <td>0.030902</td>\n",
       "      <td>171.529708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031095</td>\n",
       "      <td>0.031095</td>\n",
       "      <td>203.434174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031121</td>\n",
       "      <td>0.031121</td>\n",
       "      <td>209.663891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>203.681725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031573</td>\n",
       "      <td>0.031573</td>\n",
       "      <td>170.034671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031583</td>\n",
       "      <td>0.031583</td>\n",
       "      <td>183.006438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031973</td>\n",
       "      <td>0.031973</td>\n",
       "      <td>70.322900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032247</td>\n",
       "      <td>0.032247</td>\n",
       "      <td>210.358393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032415</td>\n",
       "      <td>0.032415</td>\n",
       "      <td>183.581340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034191</td>\n",
       "      <td>0.034191</td>\n",
       "      <td>113.620504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035012</td>\n",
       "      <td>0.035012</td>\n",
       "      <td>41.167494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035390</td>\n",
       "      <td>0.035390</td>\n",
       "      <td>37.420424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035571</td>\n",
       "      <td>0.035571</td>\n",
       "      <td>39.805687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035627</td>\n",
       "      <td>0.035627</td>\n",
       "      <td>38.351059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035741</td>\n",
       "      <td>0.035741</td>\n",
       "      <td>144.130037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>82.951680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036913</td>\n",
       "      <td>0.036913</td>\n",
       "      <td>32.791802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038232</td>\n",
       "      <td>0.038232</td>\n",
       "      <td>34.721529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.046256</td>\n",
       "      <td>0.046256</td>\n",
       "      <td>143.152312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.050133</td>\n",
       "      <td>0.050133</td>\n",
       "      <td>14.311743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.050913</td>\n",
       "      <td>0.050913</td>\n",
       "      <td>13.618656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.056457</td>\n",
       "      <td>0.056457</td>\n",
       "      <td>33.389765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.064798</td>\n",
       "      <td>0.064798</td>\n",
       "      <td>30.940158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.065325</td>\n",
       "      <td>0.065325</td>\n",
       "      <td>66.066285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.075973</td>\n",
       "      <td>0.075973</td>\n",
       "      <td>17.946677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.130016</td>\n",
       "      <td>0.130016</td>\n",
       "      <td>19.384775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             2        200         0.0001           2  0.030012  0.030012   \n",
       "1             2        200         0.0001           2  0.030047  0.030047   \n",
       "2             2        200         0.0001           2  0.030429  0.030429   \n",
       "3             2        200         0.0001           2  0.030767  0.030767   \n",
       "4             2        200         0.0001           2  0.030793  0.030793   \n",
       "5             2        200         0.0001           2  0.030809  0.030809   \n",
       "6             2        200         0.0001           2  0.030869  0.030869   \n",
       "7             2        200         0.0001           2  0.030902  0.030902   \n",
       "8             2        200         0.0001           2  0.031095  0.031095   \n",
       "9             2        200         0.0001           2  0.031121  0.031121   \n",
       "10            2        200         0.0001           2  0.031400  0.031400   \n",
       "11            2        200         0.0001           2  0.031573  0.031573   \n",
       "12            2        200         0.0001           2  0.031583  0.031583   \n",
       "13            2        200         0.0001           2  0.031973  0.031973   \n",
       "14            2        200         0.0001           2  0.032247  0.032247   \n",
       "15            2        200         0.0001           2  0.032415  0.032415   \n",
       "16            2        200         0.0001           4  0.034191  0.034191   \n",
       "17            2        200         0.0001           4  0.035012  0.035012   \n",
       "18            2        200         0.0001           4  0.035390  0.035390   \n",
       "19            2        200         0.0001           4  0.035571  0.035571   \n",
       "20            2        200         0.0001           4  0.035627  0.035627   \n",
       "21            2        200         0.0001           4  0.035741  0.035741   \n",
       "22            2        200         0.0001           4  0.035900  0.035900   \n",
       "23            2        100         0.0001           4  0.036913  0.036913   \n",
       "24            2        100         0.0001           4  0.038232  0.038232   \n",
       "25            1        200         0.0001           2  0.046256  0.046256   \n",
       "26            2        200         0.0001          16  0.050133  0.050133   \n",
       "27            2        200         0.0001          16  0.050913  0.050913   \n",
       "28            1        200         0.0001           4  0.056457  0.056457   \n",
       "29            1        100         0.0001           4  0.064798  0.064798   \n",
       "30            3        100         0.0010           2  0.065325  0.065325   \n",
       "31            4        200         0.0010          16  0.075973  0.075973   \n",
       "32            1        100         0.0001           8  0.130016  0.130016   \n",
       "\n",
       "    Elapsed time  \n",
       "0     204.350348  \n",
       "1     200.562054  \n",
       "2     186.775772  \n",
       "3     180.913800  \n",
       "4     190.637349  \n",
       "5      70.518486  \n",
       "6     263.676456  \n",
       "7     171.529708  \n",
       "8     203.434174  \n",
       "9     209.663891  \n",
       "10    203.681725  \n",
       "11    170.034671  \n",
       "12    183.006438  \n",
       "13     70.322900  \n",
       "14    210.358393  \n",
       "15    183.581340  \n",
       "16    113.620504  \n",
       "17     41.167494  \n",
       "18     37.420424  \n",
       "19     39.805687  \n",
       "20     38.351059  \n",
       "21    144.130037  \n",
       "22     82.951680  \n",
       "23     32.791802  \n",
       "24     34.721529  \n",
       "25    143.152312  \n",
       "26     14.311743  \n",
       "27     13.618656  \n",
       "28     33.389765  \n",
       "29     30.940158  \n",
       "30     66.066285  \n",
       "31     17.946677  \n",
       "32     19.384775  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla2.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 63.447 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
