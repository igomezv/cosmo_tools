{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from nnogada.elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm for splitting the dataset into training and validation \n",
    "def split(X,Y,porcent): #porcent must be between 0 and 1, it is the asigned porcent to the training dataset.\n",
    "    n=floor(porcent*len(X))\n",
    "    index=random.sample(range(len(X)),n)\n",
    "    X_learn=[]\n",
    "    Y_learn=[]\n",
    "    for i in index:\n",
    "        X_learn.append(X[i])\n",
    "        Y_learn.append(Y[i])\n",
    "    X_val=np.delete(X,index, axis=0)\n",
    "    Y_val=np.delete(Y,index, axis=0)\n",
    "    \n",
    "    X_learn=np.array(X_learn)\n",
    "    Y_learn=np.array(Y_learn)\n",
    "    return X_learn,Y_learn,X_val,Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_m=np.arange(0.1,0.51,0.1)\n",
    "H_0=np.arange(66,81,1)\n",
    "t=np.linspace(0,-12,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RHS(Omega_i, lna, gamma=0):\n",
    "    x, y, z, H = Omega_i\n",
    "    #x, y, z = Omega_i\n",
    "    pi = 3*x + 4*y\n",
    "    return [x*(-3 + pi), y*(-4 + pi), z*pi, -0.5*H*pi]\n",
    "    #return [x*(-3 + pi), y*(-4 + pi), z*pi]\n",
    "\n",
    "def EDO(t,Om,H0):\n",
    "    #t,Or,Om,Ol=X\n",
    "    Or=0.0001\n",
    "    Ol=1-Or-Om\n",
    "    #H0 = 70.\n",
    "    y0 = [Om, Or, Ol, H0]\n",
    "    result = odeint(RHS, y0, t)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets generate the cartesian product between the intervals\n",
    "Y0=[]\n",
    "#este ciclo llena la lista fijando un Om y pasando todos los Or\n",
    "for i in O_m:\n",
    "    for j in H_0:\n",
    "        Y0.extend(EDO(t,i,j))\n",
    "Y0=np.array(Y0)\n",
    "\n",
    "X0=[]\n",
    "for Om in O_m:\n",
    "    for H0 in H_0:\n",
    "        for T in t:\n",
    "            X0.append([T,Om,H0])\n",
    "X0=np.array(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "Y2 = scaler.fit_transform(Y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feactures= \n",
      " [[  0.           0.1         66.        ]\n",
      " [ -0.24489796   0.1         66.        ]\n",
      " [ -0.48979592   0.1         66.        ]\n",
      " ...\n",
      " [-11.51020408   0.5         80.        ]\n",
      " [-11.75510204   0.5         80.        ]\n",
      " [-12.           0.5         80.        ]]\n",
      "\n",
      "\n",
      "labels= \n",
      " [[9.47515373e-02 0.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      " [1.83608702e-01 1.41110961e-04 9.01997379e-01 1.62358493e-10]\n",
      " [3.22333902e-01 4.33937896e-04 7.48914062e-01 4.77144941e-10]\n",
      " ...\n",
      " [4.20164900e-02 9.58104047e-01 5.58400899e-11 3.78984027e-01]\n",
      " [3.19464665e-02 9.68145182e-01 5.73926010e-11 6.15280156e-01]\n",
      " [2.39153274e-02 9.76153281e-01 5.75107557e-11 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Now, here are the datasets\n",
    "print('feactures= \\n',X0)\n",
    "print('\\n')\n",
    "print('labels= \\n',Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = 0.8\n",
    "X_train, Y_train, X_test, Y_test = split(X0, Y2, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1, 2, 3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,1e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([8,16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=100,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=1)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 500\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces train and val splits.\n",
    "X_test, Y_test, X_val, Y_val = split(X_test, Y_test, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:6])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(4, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 3), (375, 3), (375, 3))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.2        # probability for crossover\n",
    "    P_MUTATION = 0.8         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch: 293.\n",
      "Epoch 393: early stopping\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020\n",
      "Loss: 0.0020483098924160004 , Elapsed time: 152.94090700149536\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 229.\n",
      "Epoch 329: early stopping\n",
      "12/12 [==============================] - 0s 989us/step - loss: 8.0433e-05 - mean_squared_error: 8.0433e-05\n",
      "Loss: 8.043337584240362e-05 , Elapsed time: 91.99716925621033\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0023 - mean_squared_error: 0.0023\n",
      "Loss: 0.0023316387087106705 , Elapsed time: 202.62515568733215\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0034 - mean_squared_error: 0.0034\n",
      "Loss: 0.0033559221774339676 , Elapsed time: 192.6017804145813\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 8.4648e-05 - mean_squared_error: 8.4648e-05\n",
      "Loss: 8.464844722766429e-05 , Elapsed time: 232.46580481529236\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0034 - mean_squared_error: 0.0034\n",
      "Loss: 0.0034437146969139576 , Elapsed time: 136.6266930103302\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 5.6332e-04 - mean_squared_error: 5.6332e-04\n",
      "Loss: 0.0005633212858811021 , Elapsed time: 346.8389358520508\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010    \n",
      "Loss: 0.0010227627353742719 , Elapsed time: 292.730619430542\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 9.7173e-05 - mean_squared_error: 9.7173e-05\n",
      "Loss: 9.717326611280441e-05 , Elapsed time: 262.43815517425537\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0036 - mean_squared_error: 0.0036\n",
      "Loss: 0.0035610878840088844 , Elapsed time: 216.5799069404602\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin        \tavg      \tmax       \n",
      "0  \t10    \t8.04334e-05\t0.0016589\t0.00356109\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch: 265.\n",
      "Epoch 365: early stopping\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0025 - mean_squared_error: 0.0025    \n",
      "Loss: 0.0024888268671929836 , Elapsed time: 138.4146649837494\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015    \n",
      "Loss: 0.0015057147247716784 , Elapsed time: 298.54495882987976\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0025 - mean_squared_error: 0.0025\n",
      "Loss: 0.002455042442306876 , Elapsed time: 135.27539801597595\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 4.5029e-04 - mean_squared_error: 4.5029e-04\n",
      "Loss: 0.0004502860829234123 , Elapsed time: 330.90033650398254\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Loss: 0.005737219471484423 , Elapsed time: 212.8507444858551\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0025 - mean_squared_error: 0.0025\n",
      "Loss: 0.00248501799069345 , Elapsed time: 236.89937329292297\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0034 - mean_squared_error: 0.0034\n",
      "Loss: 0.0034084764774888754 , Elapsed time: 242.09921979904175\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t7     \t8.04334e-05\t0.00207398\t0.00573722\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 9.4586e-04 - mean_squared_error: 9.4586e-04\n",
      "Loss: 0.0009458620334044099 , Elapsed time: 313.4350986480713\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch: 386.\n",
      "Epoch 486: early stopping\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0023 - mean_squared_error: 0.0023\n",
      "Loss: 0.002250208519399166 , Elapsed time: 263.2073690891266\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0021 - mean_squared_error: 0.0021    \n",
      "Loss: 0.0020987929310649633 , Elapsed time: 151.72793793678284\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 353.\n",
      "Epoch 453: early stopping\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 6.5040e-05 - mean_squared_error: 6.5040e-05\n",
      "Loss: 6.504034536192194e-05 , Elapsed time: 165.58399486541748\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Loss: 0.004653896205127239 , Elapsed time: 202.61927127838135\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 8.6772e-04 - mean_squared_error: 8.6772e-04\n",
      "Loss: 0.0008677237783558667 , Elapsed time: 305.4988317489624\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0025 - mean_squared_error: 0.0025    \n",
      "Loss: 0.0024750465527176857 , Elapsed time: 200.52425146102905\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t7     \t6.50403e-05\t0.00155657\t0.0046539 \n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014    \n",
      "Loss: 0.0014063124544918537 , Elapsed time: 290.89066910743713\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0021 - mean_squared_error: 0.0021\n",
      "Loss: 0.002063651569187641 , Elapsed time: 321.1430835723877\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 4.5635e-04 - mean_squared_error: 4.5635e-04\n",
      "Loss: 0.00045634739217348397 , Elapsed time: 121.98503160476685\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018\n",
      "Loss: 0.00181786238681525 , Elapsed time: 255.0238881111145\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 9.3715e-05 - mean_squared_error: 9.3715e-05\n",
      "Loss: 9.37147851800546e-05 , Elapsed time: 133.15041160583496\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 354.\n",
      "Epoch 454: early stopping\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 6.2134e-05 - mean_squared_error: 6.2134e-05\n",
      "Loss: 6.213378946995363e-05 , Elapsed time: 150.87184357643127\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018    \n",
      "Loss: 0.0017588607734069228 , Elapsed time: 285.59426379203796\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 369.\n",
      "Epoch 469: early stopping\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 5.3604e-05 - mean_squared_error: 5.3604e-05\n",
      "Loss: 5.360405702958815e-05 , Elapsed time: 156.48406958580017\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t8     \t5.36041e-05\t0.000785796\t0.00206365\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0025 - mean_squared_error: 0.0025    \n",
      "Loss: 0.002509435871616006 , Elapsed time: 171.67184257507324\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 4.8754e-05 - mean_squared_error: 4.8754e-05\n",
      "Loss: 4.875388913205825e-05 , Elapsed time: 170.71243119239807\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.4693e-04 - mean_squared_error: 1.4693e-04\n",
      "Loss: 0.00014693240518681705 , Elapsed time: 177.96731185913086\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 282.\n",
      "Epoch 382: early stopping\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 8.7893e-05 - mean_squared_error: 8.7893e-05\n",
      "Loss: 8.789308776613325e-05 , Elapsed time: 116.1073625087738\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 196.\n",
      "Epoch 296: early stopping\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.5550e-04 - mean_squared_error: 1.5550e-04\n",
      "Loss: 0.00015550394891761243 , Elapsed time: 90.69758319854736\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 4.8403e-05 - mean_squared_error: 4.8403e-05\n",
      "Loss: 4.840306064579636e-05 , Elapsed time: 182.58980226516724\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t6     \t4.84031e-05\t0.000362552\t0.00250944\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 399.\n",
      "Epoch 499: early stopping\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 9.8872e-05 - mean_squared_error: 9.8872e-05\n",
      "Loss: 9.88718747976236e-05 , Elapsed time: 175.87175822257996\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 366.\n",
      "Epoch 466: early stopping\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 4.8292e-05 - mean_squared_error: 4.8292e-05\n",
      "Loss: 4.829222962143831e-05 , Elapsed time: 170.56699466705322\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 8.4728e-05 - mean_squared_error: 8.4728e-05\n",
      "Loss: 8.472849731333554e-05 , Elapsed time: 169.46263098716736\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 42 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 2.5719e-04 - mean_squared_error: 2.5719e-04\n",
      "Loss: 0.0002571926452219486 , Elapsed time: 159.25933480262756\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 43 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 4.9548e-05 - mean_squared_error: 4.9548e-05\n",
      "Loss: 4.954765608999878e-05 , Elapsed time: 176.46653771400452\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 44 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 8.6428e-05 - mean_squared_error: 8.6428e-05\n",
      "Loss: 8.642774628242478e-05 , Elapsed time: 178.3894500732422\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 45 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.5498e-04 - mean_squared_error: 1.5498e-04\n",
      "Loss: 0.0001549774024169892 , Elapsed time: 180.3081991672516\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 46 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 9.5552e-04 - mean_squared_error: 9.5552e-04\n",
      "Loss: 0.0009555172291584313 , Elapsed time: 178.387357711792\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t8     \t4.82922e-05\t0.000188849\t0.000955517\n",
      "-- Best Individual =  [1, 1, 1, 0, 1, 1]\n",
      "-- Best Fitness =  6.504034536192194e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 155.65494442383448 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 10   # max of individuals per generation\n",
    "max_generations = 5    # number of generations\n",
    "gene_length = 6      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 4\n",
      "Number of neurons: 150 , Batch size 16 , Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:2])    # (8)\n",
    "    num_units_bits     = BitArray(bi[2:4])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[4:5])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[5:6])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    \n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1])\n",
    "    print('Number of neurons:', best_num_units[-1], ', Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>170.566995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>182.589802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>170.712431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>176.466538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>156.484070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>150.871844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>165.583995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>91.997169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>232.465805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>169.462631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>178.389450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>116.107363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>133.150412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>262.438155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>175.871758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>177.967312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>180.308199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>90.697583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>159.259335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>330.900337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>121.985032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>346.838936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>305.498832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>313.435099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>178.387358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>292.730619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>290.890669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>298.544959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>285.594264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>255.023888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>152.940907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>321.143084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>151.727938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>263.207369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>202.625156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>135.275398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>200.524251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>236.899373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>138.414665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>171.671843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>192.601780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.003408</td>\n",
       "      <td>0.003408</td>\n",
       "      <td>242.099220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>136.626693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>216.579907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>202.619271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.005737</td>\n",
       "      <td>0.005737</td>\n",
       "      <td>212.850744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             4        150         0.0010          16  0.000048  0.000048   \n",
       "1             4        150         0.0010          16  0.000048  0.000048   \n",
       "2             4        150         0.0010          16  0.000049  0.000049   \n",
       "3             4        150         0.0010          16  0.000050  0.000050   \n",
       "4             4        150         0.0010          16  0.000054  0.000054   \n",
       "5             4        150         0.0010          16  0.000062  0.000062   \n",
       "6             4        150         0.0010          16  0.000065  0.000065   \n",
       "7             2        150         0.0010          16  0.000080  0.000080   \n",
       "8             2        100         0.0010           8  0.000085  0.000085   \n",
       "9             3        200         0.0010          16  0.000085  0.000085   \n",
       "10            4        150         0.0010          16  0.000086  0.000086   \n",
       "11            2        150         0.0010          16  0.000088  0.000088   \n",
       "12            2        150         0.0010          16  0.000094  0.000094   \n",
       "13            1        150         0.0010           8  0.000097  0.000097   \n",
       "14            4        150         0.0010          16  0.000099  0.000099   \n",
       "15            4        150         0.0010          16  0.000147  0.000147   \n",
       "16            4        150         0.0010          16  0.000155  0.000155   \n",
       "17            2        150         0.0010          16  0.000156  0.000156   \n",
       "18            4        100         0.0010          16  0.000257  0.000257   \n",
       "19            4        200         0.0001           8  0.000450  0.000450   \n",
       "20            1        150         0.0010          16  0.000456  0.000456   \n",
       "21            4        200         0.0001           8  0.000563  0.000563   \n",
       "22            3        200         0.0001           8  0.000868  0.000868   \n",
       "23            4        100         0.0001           8  0.000946  0.000946   \n",
       "24            4        150         0.0001          16  0.000956  0.000956   \n",
       "25            3        200         0.0001           8  0.001023  0.001023   \n",
       "26            4        100         0.0001           8  0.001406  0.001406   \n",
       "27            3        200         0.0001           8  0.001506  0.001506   \n",
       "28            3        200         0.0001           8  0.001759  0.001759   \n",
       "29            3        100         0.0001           8  0.001818  0.001818   \n",
       "30            4        200         0.0001          16  0.002048  0.002048   \n",
       "31            3        200         0.0001           8  0.002064  0.002064   \n",
       "32            4         50         0.0001          16  0.002099  0.002099   \n",
       "33            3        100         0.0001           8  0.002250  0.002250   \n",
       "34            4         50         0.0001          16  0.002332  0.002332   \n",
       "35            4         50         0.0001          16  0.002455  0.002455   \n",
       "36            4        200         0.0001          16  0.002475  0.002475   \n",
       "37            2        100         0.0001           8  0.002485  0.002485   \n",
       "38            4        200         0.0001          16  0.002489  0.002489   \n",
       "39            4        150         0.0001          16  0.002509  0.002509   \n",
       "40            4        200         0.0001          16  0.003356  0.003356   \n",
       "41            2        100         0.0001           8  0.003408  0.003408   \n",
       "42            2        100         0.0001          16  0.003444  0.003444   \n",
       "43            1        150         0.0001           8  0.003561  0.003561   \n",
       "44            2        200         0.0001          16  0.004654  0.004654   \n",
       "45            1        150         0.0001           8  0.005737  0.005737   \n",
       "\n",
       "    Elapsed time  \n",
       "0     170.566995  \n",
       "1     182.589802  \n",
       "2     170.712431  \n",
       "3     176.466538  \n",
       "4     156.484070  \n",
       "5     150.871844  \n",
       "6     165.583995  \n",
       "7      91.997169  \n",
       "8     232.465805  \n",
       "9     169.462631  \n",
       "10    178.389450  \n",
       "11    116.107363  \n",
       "12    133.150412  \n",
       "13    262.438155  \n",
       "14    175.871758  \n",
       "15    177.967312  \n",
       "16    180.308199  \n",
       "17     90.697583  \n",
       "18    159.259335  \n",
       "19    330.900337  \n",
       "20    121.985032  \n",
       "21    346.838936  \n",
       "22    305.498832  \n",
       "23    313.435099  \n",
       "24    178.387358  \n",
       "25    292.730619  \n",
       "26    290.890669  \n",
       "27    298.544959  \n",
       "28    285.594264  \n",
       "29    255.023888  \n",
       "30    152.940907  \n",
       "31    321.143084  \n",
       "32    151.727938  \n",
       "33    263.207369  \n",
       "34    202.625156  \n",
       "35    135.275398  \n",
       "36    200.524251  \n",
       "37    236.899373  \n",
       "38    138.414665  \n",
       "39    171.671843  \n",
       "40    192.601780  \n",
       "41    242.099220  \n",
       "42    136.626693  \n",
       "43    216.579907  \n",
       "44    202.619271  \n",
       "45    212.850744  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_ecsdiff.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 155.650 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
