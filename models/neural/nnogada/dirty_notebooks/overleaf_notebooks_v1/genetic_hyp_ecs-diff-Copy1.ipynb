{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from nnogada.elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm for splitting the dataset into training and validation \n",
    "def split(X,Y,porcent): #porcent must be between 0 and 1, it is the asigned porcent to the training dataset.\n",
    "    n=floor(porcent*len(X))\n",
    "    index=random.sample(range(len(X)),n)\n",
    "    X_learn=[]\n",
    "    Y_learn=[]\n",
    "    for i in index:\n",
    "        X_learn.append(X[i])\n",
    "        Y_learn.append(Y[i])\n",
    "    X_val=np.delete(X,index, axis=0)\n",
    "    Y_val=np.delete(Y,index, axis=0)\n",
    "    \n",
    "    X_learn=np.array(X_learn)\n",
    "    Y_learn=np.array(Y_learn)\n",
    "    return X_learn,Y_learn,X_val,Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_m=np.arange(0.1,0.51,0.1)\n",
    "H_0=np.arange(66,81,1)\n",
    "t=np.linspace(0,-12,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RHS(Omega_i, lna, gamma=0):\n",
    "    x, y, z, H = Omega_i\n",
    "    #x, y, z = Omega_i\n",
    "    pi = 3*x + 4*y\n",
    "    return [x*(-3 + pi), y*(-4 + pi), z*pi, -0.5*H*pi]\n",
    "    #return [x*(-3 + pi), y*(-4 + pi), z*pi]\n",
    "\n",
    "def EDO(t,Om,H0):\n",
    "    #t,Or,Om,Ol=X\n",
    "    Or=0.0001\n",
    "    Ol=1-Or-Om\n",
    "    #H0 = 70.\n",
    "    y0 = [Om, Or, Ol, H0]\n",
    "    result = odeint(RHS, y0, t)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets generate the cartesian product between the intervals\n",
    "Y0=[]\n",
    "#este ciclo llena la lista fijando un Om y pasando todos los Or\n",
    "for i in O_m:\n",
    "    for j in H_0:\n",
    "        Y0.extend(EDO(t,i,j))\n",
    "Y0=np.array(Y0)\n",
    "\n",
    "X0=[]\n",
    "for Om in O_m:\n",
    "    for H0 in H_0:\n",
    "        for T in t:\n",
    "            X0.append([T,Om,H0])\n",
    "X0=np.array(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "Y2 = scaler.fit_transform(Y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feactures= \n",
      " [[  0.           0.1         66.        ]\n",
      " [ -0.24489796   0.1         66.        ]\n",
      " [ -0.48979592   0.1         66.        ]\n",
      " ...\n",
      " [-11.51020408   0.5         80.        ]\n",
      " [-11.75510204   0.5         80.        ]\n",
      " [-12.           0.5         80.        ]]\n",
      "\n",
      "\n",
      "labels= \n",
      " [[9.47515373e-02 0.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      " [1.83608702e-01 1.41110961e-04 9.01997379e-01 1.62358493e-10]\n",
      " [3.22333902e-01 4.33937896e-04 7.48914062e-01 4.77144941e-10]\n",
      " ...\n",
      " [4.20164900e-02 9.58104047e-01 5.58400899e-11 3.78984027e-01]\n",
      " [3.19464665e-02 9.68145182e-01 5.73926010e-11 6.15280156e-01]\n",
      " [2.39153274e-02 9.76153281e-01 5.75107557e-11 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Now, here are the datasets\n",
    "print('feactures= \\n',X0)\n",
    "print('\\n')\n",
    "print('labels= \\n',Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = 0.8\n",
    "X_train, Y_train, X_test, Y_test = split(X0, Y2, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1, 2, 3, 4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,1e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=100,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=1)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 500\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces train and val splits.\n",
    "X_test, Y_test, X_val, Y_val = split(X_test, Y_test, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:6])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(4, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 3), (375, 3), (375, 3))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.8        # probability for crossover\n",
    "    P_MUTATION = 0.2         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 9.1352e-04 - mean_squared_error: 9.1352e-04\n",
      "Loss: 0.000913523486815393 , Elapsed time: 283.4972825050354\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 327.\n",
      "Epoch 00427: early stopping\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 2.2760e-04 - mean_squared_error: 2.2760e-04\n",
      "Loss: 0.00022759957937523723 , Elapsed time: 183.7866199016571\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "Loss: 0.0016413835110142827 , Elapsed time: 220.06569814682007\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.2008e-04 - mean_squared_error: 7.2008e-04\n",
      "Loss: 0.0007200794061645865 , Elapsed time: 292.93656277656555\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 184.\n",
      "Epoch 00284: early stopping\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.7028e-05 - mean_squared_error: 7.7028e-05\n",
      "Loss: 7.702817674726248e-05 , Elapsed time: 225.6698136329651\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0038 - mean_squared_error: 0.0038\n",
      "Loss: 0.00381134613417089 , Elapsed time: 205.65184712409973\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 3.1224e-04 - mean_squared_error: 3.1224e-04\n",
      "Loss: 0.00031223573023453355 , Elapsed time: 548.0050094127655\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 3.1626e-04 - mean_squared_error: 3.1626e-04\n",
      "Loss: 0.0003162638167850673 , Elapsed time: 562.8243148326874\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.6652e-05 - mean_squared_error: 7.6652e-05\n",
      "Loss: 7.665200246265158e-05 , Elapsed time: 345.44209241867065\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Loss: 0.004140254110097885 , Elapsed time: 372.1291916370392\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "Loss: 0.0016072392463684082 , Elapsed time: 391.68286418914795\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin       \tavg       \tmax       \n",
      "0  \t11    \t7.6652e-05\t0.00125851\t0.00414025\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0925e-04 - mean_squared_error: 1.0925e-04\n",
      "Loss: 0.00010924926755251363 , Elapsed time: 373.2365963459015\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 2.8051e-04 - mean_squared_error: 2.8051e-04\n",
      "Loss: 0.00028051252593286335 , Elapsed time: 636.3172669410706\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 3.5212e-04 - mean_squared_error: 3.5212e-04\n",
      "Loss: 0.00035211964859627187 , Elapsed time: 434.5663604736328\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 5.1813e-04 - mean_squared_error: 5.1813e-04\n",
      "Loss: 0.0005181254236958921 , Elapsed time: 191.67203783988953\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 3.5318e-04 - mean_squared_error: 3.5318e-04\n",
      "Loss: 0.00035317885340191424 , Elapsed time: 308.87836813926697\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 2.0873e-04 - mean_squared_error: 2.0873e-04\n",
      "Loss: 0.00020873492758255452 , Elapsed time: 301.0720238685608\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 2.3275e-04 - mean_squared_error: 2.3275e-04\n",
      "Loss: 0.0002327527035959065 , Elapsed time: 311.3161165714264\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 3.6721e-04 - mean_squared_error: 3.6721e-04\n",
      "Loss: 0.00036720657953992486 , Elapsed time: 150.08261275291443\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t8     \t7.6652e-05\t0.00060201\t0.00381135\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 6.7201e-05 - mean_squared_error: 6.7201e-05\n",
      "Loss: 6.720051169395447e-05 , Elapsed time: 432.1421720981598\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0017 - mean_squared_error: 0.0017\n",
      "Loss: 0.0017260764725506306 , Elapsed time: 265.4410321712494\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 337.\n",
      "Epoch 00437: early stopping\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 2.5781e-04 - mean_squared_error: 2.5781e-04\n",
      "Loss: 0.0002578137209638953 , Elapsed time: 329.13267397880554\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.4038e-04 - mean_squared_error: 1.4038e-04\n",
      "Loss: 0.00014037781511433423 , Elapsed time: 488.4806354045868\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 3.3565e-04 - mean_squared_error: 3.3565e-04\n",
      "Loss: 0.0003356505767442286 , Elapsed time: 502.6486463546753\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 2.0483e-05 - mean_squared_error: 2.0483e-05\n",
      "Loss: 2.048315764113795e-05 , Elapsed time: 421.0053427219391\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t6     \t2.04832e-05\t0.000327872\t0.00172608\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 262.\n",
      "Epoch 00362: early stopping\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Loss: 0.003946700133383274 , Elapsed time: 254.9750258922577\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 2.1577e-04 - mean_squared_error: 2.1577e-04\n",
      "Loss: 0.00021576977451331913 , Elapsed time: 415.29477429389954\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.9672e-04 - mean_squared_error: 1.9672e-04\n",
      "Loss: 0.00019671775226015598 , Elapsed time: 339.8728325366974\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 7.7092e-05 - mean_squared_error: 7.7092e-05\n",
      "Loss: 7.709213969064876e-05 , Elapsed time: 264.31392335891724\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 337.\n",
      "Epoch 00437: early stopping\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.3071e-05 - mean_squared_error: 8.3071e-05\n",
      "Loss: 8.307085954584181e-05 , Elapsed time: 265.5012710094452\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 8.6509e-05 - mean_squared_error: 8.6509e-05\n",
      "Loss: 8.650901145301759e-05 , Elapsed time: 548.5261371135712\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 4.1178e-04 - mean_squared_error: 4.1178e-04\n",
      "Loss: 0.00041177848470397294 , Elapsed time: 412.3790991306305\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t7     \t2.04832e-05\t0.000468704\t0.0039467 \n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0967e-04 - mean_squared_error: 1.0967e-04\n",
      "Loss: 0.00010966930130962282 , Elapsed time: 503.1159131526947\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.2298e-04 - mean_squared_error: 1.2298e-04\n",
      "Loss: 0.00012297752255108207 , Elapsed time: 429.7158751487732\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 5.1185e-05 - mean_squared_error: 5.1185e-05\n",
      "Loss: 5.118546323501505e-05 , Elapsed time: 357.30993700027466\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.5015e-04 - mean_squared_error: 1.5015e-04\n",
      "Loss: 0.00015014986274763942 , Elapsed time: 380.57908844947815\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.8423e-04 - mean_squared_error: 1.8423e-04\n",
      "Loss: 0.00018423116125632077 , Elapsed time: 362.71887588500977\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 898us/step - loss: 1.1129e-04 - mean_squared_error: 1.1129e-04\n",
      "Loss: 0.00011128801997983828 , Elapsed time: 198.00550770759583\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 363.\n",
      "Epoch 00463: early stopping\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3795e-04 - mean_squared_error: 1.3795e-04\n",
      "Loss: 0.00013794506958220154 , Elapsed time: 295.5468463897705\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch: 376.\n",
      "Epoch 00476: early stopping\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0019 - mean_squared_error: 0.0019\n",
      "Loss: 0.0019051466370001435 , Elapsed time: 247.92839431762695\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.4935e-04 - mean_squared_error: 1.4935e-04\n",
      "Loss: 0.00014935247600078583 , Elapsed time: 211.79827761650085\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t9     \t2.04832e-05\t0.000269356\t0.00190515\n",
      "\n",
      "--------------- Starting trial: 42 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0071e-04 - mean_squared_error: 1.0071e-04\n",
      "Loss: 0.00010071160068036988 , Elapsed time: 239.43681454658508\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 43 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.4524e-04 - mean_squared_error: 1.4524e-04\n",
      "Loss: 0.00014524205471388996 , Elapsed time: 234.26206254959106\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 44 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 4.5008e-05 - mean_squared_error: 4.5008e-05\n",
      "Loss: 4.500833529164083e-05 , Elapsed time: 352.22758197784424\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 45 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.3438e-04 - mean_squared_error: 1.3438e-04\n",
      "Loss: 0.00013438113091979176 , Elapsed time: 400.1475052833557\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 46 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0960e-04 - mean_squared_error: 1.0960e-04\n",
      "Loss: 0.00010959996870951727 , Elapsed time: 402.86985063552856\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 47 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 2.5437e-04 - mean_squared_error: 2.5437e-04\n",
      "Loss: 0.0002543748414609581 , Elapsed time: 283.3383936882019\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 48 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 5.0557e-04 - mean_squared_error: 5.0557e-04\n",
      "Loss: 0.0005055691581219435 , Elapsed time: 258.8237862586975\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 49 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 3.2888e-04 - mean_squared_error: 3.2888e-04\n",
      "Loss: 0.0003288832667749375 , Elapsed time: 253.67270183563232\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 50 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 2.3904e-04 - mean_squared_error: 2.3904e-04\n",
      "Loss: 0.00023903725377749652 , Elapsed time: 316.5923881530762\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 51 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.6370e-04 - mean_squared_error: 1.6370e-04\n",
      "Loss: 0.00016370169760193676 , Elapsed time: 307.77814531326294\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t10    \t2.04832e-05\t0.00018609 \t0.000505569\n",
      "-- Best Individual =  [1, 1, 1, 1, 0, 0]\n",
      "-- Best Fitness =  2.048315764113795e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABbAUlEQVR4nO3dd1gU19fA8e9SRYpiAxXs2Bsqgl0XAQWMvUZsMXaxGyv2kliwt9hiNL/EbhQVFWKJXaMhRmxRIqhAFAvSy7x/7MsqCiwoy1Du53n20d2dcu4O7OHeOXNHIUmShCAIgiBkko7cAQiCIAh5i0gcgiAIQpaIxCEIgiBkiUgcgiAIQpaIxCEIgiBkiUgcgiAIQpaIxFHAPH36FFtbW5KSkuQOBaVSyYULF+QOI0f99NNPNG3aFFtbW16+fImtrS3BwcFyhyVoweDBgzlw4IDcYWiFSBzZRKlUUrt2bSIiIlK93qlTJ6pVq0ZISIhW979//36qVavGwoULU71+6tQpqlWrxpQpUwAoU6YMN27cQFdXV6vxZJfVq1dTrVo1/vzzT7lD+WwJCQksXryYrVu3cuPGDczNzblx4wbW1tYATJkyBW9vb5mjzD3++usvhg4dip2dHY0aNcLV1RVvb29ev34td2gfWb16NRMnTkz12ubNm+ncubNMEWmXSBzZqGzZsvj4+Kif3717l5iYmBzbf7ly5Th27BiJiYnq1w4ePEiFChVyLIbsJEkSBw8epGjRohw8eFAr+8jJnteLFy+Ii4ujSpUqObbPvOD9n9cUf/zxB/369aNBgwYcO3aMa9eusXnzZnR1dblz547s8RV0InFko44dO6b6gjt48CCdOnVKtczp06fp1KkTDRo0oFWrVqxevVr93tGjR1Eqlbx9+xaAM2fO0KxZs496MekpUaIEVatW5ffffwfg1atX3LhxA6VSqV4mJCSEatWqqX8ZPDw8WLFiBb169cLW1pZBgwalu7/Xr18zdOhQHBwcsLOzY+jQoYSGhqrf17StgwcP0qZNG+zt7Vm/fr3G9ly7do3//vuP6dOnc/ToUeLj4wHVEMDOnTtTLfvFF19w4sQJAP755x8GDhxI48aNcXFx4ejRo+rlpkyZwqxZs/j666+pX78+ly9fzvCYfBj32rVrUw2xJScns2nTJtq2bYu9vT1jxozh1atXH7Xl0aNHtGvXDgA7Ozv69esHQLVq1fj333/55ZdfOHz4MFu2bMHW1pZhw4YBqp7sli1b6NChAw0bNmTs2LHExcWpt/vbb7/RsWNHGjVqRK9evVJ9qW7atIkWLVpga2uLi4sLFy9eBCAgIIAuXbrQoEEDmjZtyqJFi9I9Brt378bJyYnGjRszbNgwwsLCAJg1axbffvttqmWHDx/Otm3bAAgLC2P06NE4ODigVCrZsWOHernVq1fj6enJxIkTadCgQZrDOUuWLKFLly4MHTqUEiVKAKresqenJ/b29url9u7dS/v27bGzs+Orr77iyZMn6veqVavG//73P5ydnWnUqBFz5szh/YkyNK27a9cunJ2dcXZ2BmD+/Pm0atWKBg0a0KVLF65duwbA2bNn2bhxI8eOHcPW1pYvvvgCUP0+7NmzB1D9nKxbt442bdrQpEkTJk+eTGRkJPDud/LAgQO0bt36o9+PrByvHCMJ2aJNmzbS+fPnJWdnZ+nBgwdSYmKi1KJFCykkJESqWrWqFBwcLEmSJF26dEm6c+eOlJSUJAUGBkpNmjSRTp48qd7O+PHjpW+++UaKiIiQmjVrJvn7+2dq//v27ZN69eol/frrr9KYMWMkSZKknTt3SjNnzpSWL18uffPNN5IkSVJwcLBUtWpVKSEhQZIkSerbt6/k6OgoPXz4UIqJiZH69u0rLVmyJM19RERESMePH5eio6OlyMhIafTo0dLw4cPV72e0rfv370v169eXrly5IsXFxUkLFy6UatSoIZ0/fz7dNk2dOlXy9PSU4uPjpcaNG0vHjx+XJEmSDhw4IPXs2VO93P3796WGDRtKcXFxUlRUlNSyZUtp7969UkJCgvT3339LjRs3lu7fvy9JkiR98803UoMGDaRr165JSUlJUmxsbIbHJCXuq1evSnFxcdLixYulmjVrquPevn271L17d+nZs2dSXFycNHPmTGncuHFptufDz16SJKlq1apSUFCQOrbly5enWqdNmzZS165dpdDQUOnly5dSu3btpJ9++kmSJEn6+++/JQcHB+nmzZtSYmKitH//fqlNmzZSXFyc9M8//0gtW7aUQkND1fv+999/JUmSpB49ekgHDhyQJEmS3r59K924cSPNeC9cuCA1btxYunXrlhQXFyfNnTtX6tOnjyRJknTlyhWpZcuWUnJysiRJkvTq1SupTp06UmhoqJSUlCR17txZWr16tRQXFyc9fvxYUiqV0tmzZyVJkqRVq1ZJNWvWlE6ePCklJSVJMTExqfYbFRUlVa9eXbp06VKacaU4efKk1LZtW+nBgwdSQkKCtHbt2lQ/F1WrVpWGDBkivX79Wnry5Ilkb28vnTlzJtPrDhgwQHr58qU6voMHD0oRERFSQkKCtGXLFqlp06ZSbGysuk0TJkxIFV/fvn2l3bt3S5IkSXv27JHatm0rPX78WHr79q00cuRIaeLEiepjU7VqVWn69OlSTEyMFBgYKNWqVUt68OBBlo5XThI9jmyW0us4f/48lStXxsLCItX79vb2VKtWDR0dHapXr46bmxtXrlxRvz9r1iwuXbpEv379UCqVtGnTJkv7d3Jy4sqVK0RGRnLo0CE6duyocZ0uXbpQsWJFChUqRLt27QgMDExzOXNzc1xcXDAyMsLExIThw4dz9erVTG3r+PHjtG7dGjs7OwwMDBgzZgw6Oun/+MXExHD8+HE6dOiAvr4+Li4u6t5c27ZtuXPnjvovxMOHD+Pk5ISBgQGnT5+mbNmydO3aFT09PWrWrImLiwvHjx9Xb9vR0ZGGDRuio6ODoaFhhsfk+PHjtGnThkaNGmFgYICnpycKhUK9rZ9//plx48ZhaWmJgYEBo0aNwtfXN1uHNzw8PLCwsKBo0aK0adNG/Zn+8ssv9OzZk3r16qGrq0vnzp3R19fn5s2b6OrqEh8fzz///ENCQgJWVlaUK1cOAD09PR4/fkxERATGxsbUr18/zf0ePnyYrl27UqtWLQwMDBg/fjw3b94kJCSERo0aoVAo1H91+/r6Ur9+fSwsLPjrr7+IiIhg1KhRGBgYYG1tTY8ePVL1/OrXr0/btm3R0dGhUKFCqfb75s0bkpOT1T0NgO+++45GjRpRv3591q1bp/7shwwZQuXKldHT02PYsGEEBgam6jl8/fXXmJmZUaZMGezt7dU9ssysO2TIEIoWLaqOr2PHjpibm6Onp8egQYOIj4/n0aNHmTqGhw8fZsCAAVhbW2NsbMz48eM5evRoqp+TUaNGUahQIapXr0716tXVsWb2eOUkPbkDyG86duxI3759CQkJSfNL+88//2Tp0qXcv3+fhIQE4uPj1UMYAGZmZrRr145t27axatWqLO+/UKFCtGrVinXr1vHq1SsaNmzI2bNnM1ynZMmS6v8bGRkRHR2d5nIxMTEsWrSIc+fOqU9QRkVFkZSUpD7Znt62wsPDsbS0VL9XuHBhihYtmm5MJ0+eRE9Pj5YtWwLQoUMHBg4cSEREBMWKFaNVq1b4+PgwZMgQjhw5wvz58wF48uQJAQEBNGrUSL2tpKQk9fABQOnSpVPtK6Nj8mHcRkZGqeJ++vQpI0eOTJUEdXR0ePHixUd/NHyqDz/T8PBw9b4PHjyYatguISGB8PBwGjduzLRp01i9ejUPHjygefPmTJkyBQsLCxYsWMCqVato3749VlZWjBo1Ks0/UMLDw6lVq5b6ubGxMUWLFiUsLAwrKytcXV05cuQIdnZ2HD58WP0ZP3nyhPDw8I+OwfvP3/9MP2RmZoaOjg7//fcflStXBmDy5MlMnjyZiRMnqs9LPX36lIULF6YaMpMkibCwMMqWLZvmZxcVFZXpdT/8OdmyZQt79+4lPDwchULB27dvefnyZbrteF94eLh6u6A6H5qYmMiLFy/Ur72fKN//3cns8cpJInFks7Jly2JlZcWZM2dYsGDBR+9PmDCBvn37snnzZgwNDVmwYEGqH77AwED27duHu7s78+fPZ8uWLVmOoVOnTvTv359Ro0Z9Vls+tHXrVh49esTu3bspWbIkgYGBdOrUKdW4cXpKlSrFP//8o34eExOT5rmAFAcPHiQ6Olr9CyJJEgkJCRw+fJj+/fvj7u7OmjVrsLOzIy4uTj3uXbp0aezs7NRj7ZmR0TEpVapUqr8qY2NjU8VtaWnJwoULadiwYab3l573ezKZUbp0aYYNG8bw4cPTfL9Dhw506NCBt2/f4uXlxdKlS1myZAkVKlRg+fLlJCcnc+LECTw9Pbl8+TKFCxdOtX6pUqVS/QUeHR3Nq1ev1AnR3d2dQYMGMWTIEAICAli7dq06LisrK/U5p6y2tXDhwtSrV4+TJ0/i4OCgsf3v/1GQWZlZ9/0YU07Ob9++HRsbG3R0dLCzs1P/7Gs6dh9+lk+fPkVPT4/ixYunOk+Ylswer5wkhqq0YMGCBfzwww9pHtioqCiKFCmCoaEhAQEBHDlyRP1eXFwckyZNYty4cSxatIjw8HB27dqlft/Dw+OjE7dpady4Mdu2baNv377Z06D3Yjc0NMTMzIxXr16xZs2aTK/r4uLC6dOnuXbtGvHx8axatYrk5OQ0lw0LC+PixYts2LCBgwcPcvDgQQ4dOsTXX3/NoUOHAGjVqhVPnz5l1apVuLq6qv/ib926NUFBQRw8eJCEhAQSEhIICAhIlbTSald6x8TFxQV/f3/++OMP4uPjWb16dapE2bt3b1asWKH+UoiIiODUqVOZ/lzeV7x48SyVbXfv3p2ff/6ZP//8E0mSiI6O5vTp07x9+5aHDx9y8eJF4uPjMTAwwNDQUP0ZHTp0iIiICHR0dDAzMwNIc9jQ3d2d/fv3ExgYSHx8PMuXL6du3bpYWVkBULNmTczNzZkxYwbNmzdXb6tu3boYGxuzadMmYmNjSUpK4t69ewQEBGS6bRMnTmTfvn1s2rRJ/Vd5aGhoqs+nV69ebNq0ifv37wMQGRnJsWPHMrX9rK4bFRWFrq4uxYoVIzExkTVr1qiLWEB17J48eZLuz7S7uzs//PADwcHBREVF4e3tTfv27dHT0/y3e2aPV04SiUMLypUrR506ddJ8b9asWaxatQpbW1vWrl1L+/bt1e8tW7YMS0tL+vTpg4GBAUuWLGHlypUEBQUB8OzZMxo0aKBx/wqFgiZNmmQ4FPQp+vfvT1xcHA4ODvTs2ZMWLVpkel0bGxu8vLyYOHEiLVq0wMzMLN3hikOHDlGjRg2aN29OyZIl1Q8PDw/u3r3LvXv3MDAwwMnJiQsXLuDu7q5e18TEhC1btnD06FFatGhB8+bNWbp0qboiKy0ZHRMbGxtmzpzJ+PHjadGiBYULF6ZYsWIYGBgAqM9FDRo0CFtbW3r06JGlL8j3devWjQcPHtCoUSNGjBihcfk6deowb9485s6di52dHc7Ozuzfvx+A+Ph4li1bhr29Pc2bNyciIoLx48cDcO7cOdzc3LC1tWXBggV4e3t/dJ4BoGnTpowZM4bRo0fTvHlzgoODP7rOxN3d/aNjoKury4YNG7hz5w6Ojo44ODgwY8aMVF+0mjRq1IgffviBq1ev4uLiQqNGjRg8eDD29vbqP4icnJwYPHgw48ePp0GDBri7u2sclk2R1XWbN29OixYtcHFxQalUYmhomGooK2Vo097ePs1rN7p27coXX3xB3759cXR0xMDAgJkzZ2Yq1swer5ykkDIzziDILjQ0lLFjx/Lzzz/LHUqBFhUVhZ2dHb6+vuoL9wShoBGJQxA08Pf3p0mTJkiSxOLFiwkICODAgQNZPichCPmFGKoSBA38/Pxo0aIFLVq04N9//2X58uUiaQgFmuhxCIIgCFkiehyCIAhClhSI6zhu3ryJoaHhJ60bFxf3yevmVaLN+V9Bay+INn/KuuldpV4gEoehoSE1atT4pHUDAwM/ed28SrQ5/yto7QXR5k9ZNz1iqEoQBEHIEpE4BEEQhCwRiUMQBEHIkgJxjkMQBEGThIQEQkJCiI2NlTuUbJOQkJDhuQpQzahtZWWFvr5+prcrEocgCAKqO/GZmppSoUKFfHOBZ0xMDEZGRum+L0kSL168ICQkhIoVK2Z6u2KoShAEAdWU+cWLF883SSMzFAoFxYsXz3IvSyQOQRCE/1eQkkaKT2mzGKrKwJF7R7jz+A7m1uZYmqR/xzJBEISCRPQ4MrDs4jImXZpE6WWlqb2uNp7HPDl45yCvYl/JHZogCPlQtWrVmDhxovp5YmIiDg4ODB06FFBNuLlp0ya5wlMTPY4MnPI4xYGLB3jIQ/wf+bPlxhZWX1mNjkKHBqUb4FjREWVFJc2sm2FsYCx3uIIg5HGFCxfm/v37xMbGUqhQIc6fP5/q3vWOjo44OjrKGKGKSBwZ0NXRpVaxWnSr0Y3JzSYTnxTP5ZDL+D/yxz/In+UXl/Pt+W/R19GniXUTlBWUKCsqsbeyx0DXQO7wBUHIg1q1asXp06dp164dPj4+uLm5cf36dQD279/PrVu38PLyYsqUKZiYmHDr1i3+++8/Jk2apL4TobaJxJEFBroGtCjfghblWzCLWUTFR3E++Dx+D/3wD/Jnzpk5zD4zm8L6hWlRrgXKikocKzpS37I+ujq6cocvCEIm7dgBW7dm7zYHDYJ+/TQv5+rqyrp162jTpg13796la9eu6sTxofDwcH766ScePnzI8OHD80fiOHv2LAsWLCA5OZnu3bszZMiQVO/Hx8czefJk/v77b4oWLYq3tzdWVlYAbNy4kb1796Kjo8OMGTNS3d86KSmJrl27YmFhwcaNG7XZhAwZGxjjXNkZ58rOALyMecmZf8+oeiSP/Pnm1DcAmBcyp3WF1igrqnokNUrUKJDVG4IgaFa9enVCQkI4cuQIrVq1ynDZtm3boqOjQ5UqVXj+/HkORajFxJGUlMTcuXPZtm0bFhYWdOvWDaVSSZUqVdTL7NmzBzMzM06ePImPjw9Lly5lxYoVPHjwAB8fH3x8fAgLC2PgwIH4+vqiq6v6q33Hjh1UrlyZt2/faiv8T2JuZE6n6p3oVL0TAKFvQ/nt0W/4PfLD/5E/B+4cAMDSxFKVRCoocazkSIWiFeQLWhCEj/Trl7negbYolUq+++47duzYwatXr9JdzsBAniFxrSWOgIAAypcvj7W1NQBubm74+fmlShz+/v6MGjUKABcXF+bOnYskSfj5+eHm5oaBgQHW1taUL1+egIAAbG1tCQ0N5fTp0wwbNozt27drK/xsYWliSe86veldpzcAj14+Up8f8X/kz09//QRAxaIV1cNabSq2EaW/glDAdevWDTMzM6pVq8bly5flDucjWkscYWFhWFq++wK0sLAgICDgo2VKly6tCkRPD1NTU16+fElYWBj16tVLtW5YWBgACxcuZNKkSURFRWU6lri4OI3ztaQnNjb2k9dNS1OjpjSt0ZTp1afzz5t/uBx+mUvhl9hzaw9bbmwBoIpZFexL2WNvYU/jko0xMzDLtv1nRna3OS8oaG0uaO0FzW1OSEggJiYmByP6mCRJxMTEUKRIEbp3705MTAxxcXEkJSURExNDfHw8iYmJxMTEkJiYSHx8vDrmlHXT2p4mmZnT6n156uT4b7/9RrFixahdu3aWsnBuvZFTTWrSgQ4AJCUncTP05rthrX8PsOvBLnXpb8qwVk6U/oob3uR/v9/4HdsatnKHkaM0HePAwMAM53XKCTdv3vzotZYtW9KyZUsAevXqpX596dKlGtfVNFdVCn19/Y8+m4wSidYSh4WFBaGhoernYWFhqeqRU5Z59uwZlpaWJCYmEhkZibm5ebrr+vv74+/vz9mzZ4mLi+Pt27dMnDjxow8wL9LV0aVhmYY0LNPwo9Jfv0d+eF/y5rsL34nSX+GzrbmyBs9jnhw3O64u7BCErNDaleN16tQhKCiI4OBg4uPj8fHxQalUplpGqVRy4IDqhLGvry8ODg4oFAqUSiU+Pj7Ex8cTHBxMUFAQdevWZcKECZw9exZ/f3+WL1+Og4NDvkgaaUkp/Z3VehZnB57l5Tcv8e3ryziHcUQnRDPnzBxabm+J+bfmtNvZju/Of8f1p9dJSk6SO3QhF3se/ZyZv81EQmK4z3BiEuQdmhHyJq31OPT09PDy8mLw4MHq8lkbGxtWrlxJ7dq1cXR0pFu3bkyaNAknJyeKFCmCt7c3ADY2NrRv3x5XV1d0dXXx8vJSV1QVVBmV/vo98lOX/hYtVJTWFVqrr2oXpb/C+2afnk1kXCTTbaez4MYCFpxbwHzlfLnDEvIYhSRJktxBaNvn3rA9L4x9f1j6++jVI+DTSn/zSpuzU0Fo89/hf1NvQz2GNRrGyIojWXRnET/f+pk/h/1JjZL5u+2QuXMc+e1nILPnONJqe0afR546OS6kT5T+ChmRJInxJ8ZjamjKnNZzCP83nKXOSzly7wjDfIZxuv9p0TMVMk0kjnyqonlFvjL/iq8afIUkSQQ+D1QPa+0L3Kcu/a1ZsqZ6WKt1hdYULVRU3sAFrTh6/ygn/jnBCpcVFC9cnHDCKWVciu+cvuPrw1+z/eZ2BtoOlDtMIY8Q06oXAAqFgpolazKq8SgO9DzA80nPufr1Vb5t+y3WZtZsubGFzr90pvh3xWn8fWMCXxas+v78LiEpgfEnxlOteDVG2I1I9d4g20E0L9ecSScn8Tw656asENKmaVr13EIkjgJIV0eXRmUaMbnZZI73Pc7Lb15ydsBZvFp68fj1Y6ZemUpicqLcYQrZZN3Vddx7cY9lzsvQ19VP9Z6OQocNbht4HfeaSScnyRShkOL9adWBj6ZVzy1E4hBSlf6ud1vPvdf3WH15tdxhCdngRfQLZp+ZjUtlF1xtXNNcplapWkxqOontN7dzOuh0zgYofCRlWnVAPa16iujoaKZOnUq3bt3o1KkTp06dAiAkJIQ+ffrQuXNnOnfuzB9//AHA1atX8fDwwNPTk3bt2jFhwgSyox5KnOMQUulUvRMtS7fE67QXPWr1oKxZWblDEj7DrNOziIyLZJnzsgxPfs9oOYOfb/3MsCPD+HPYnxjqGeZglLnPjj93sPVG9s6rPsh2EP3qaZ45MaNp1Tds2ICDgwOLFi3izZs3dO/enaZNm1K8eHG2bduGoaEhQUFBjB8/nv379wNw+/ZtfHx8KFWqFL179+b69es0atTos9oiehxCKgqFgmm200hISmDCiQlyhyN8hr/D/2bDtQ0MazSMWqVqZbhsYf3CrHNbx90Xd/nu/Hc5FKGQloymVf/999/5/vvv6dixIx4eHsTFxfHs2TMSExOZMWMGHTp0YMyYMfzzzz/qderWrYulpSU6OjpUr16dJ0+efHaMoschfKScSTmmtZjGrNOz+Mr2K5wqO8kdkpBF75ffzm49O1PrtKvSjh61erDg3AJ61e6FTXEb7QaZi/Wr1y9TvQNtyWha9VWrVlGpUqVUr61evZoSJUpw6NAhkpOTqVu3rvq996de19XVJSnp82eXED0OIU2Tm02mSrEqjDw6krjEOLnDEbLo2INjnPjnBLNbzaZE4RKZXm+FywoM9QwZcXREtoyFC5+mW7dujBw5kmrVqqV6vXnz5uzcuVN9bG7fvg1AZGQkJUuWREdHh0OHDmVLcsiISBxCmgrpFWKt61ruR9xnyYUlcocjZEFCUgLjfdMuv9WktGlpFioXcurhKf53639ailDQxNLSkn5p3ElqxIgRJCYm8sUXX+Dm5sbKlSsB6NOnDwcOHOCLL77g4cOHFC5cWLsBSgXA7du3ZVk3r3q/zd12d5MKzS8kPYx4KGNE2pefjvOKiyskZiMduXsk3WUyam9iUqJkt8lOKrWklBQRHaGNEGWh6Rjnp5+BFNHR0ZlaLq22Z/R5iB6HkCFvF290FbqMPjZaDF3kASnlt86VndMtv9VEV0eXje4beR79nKl+U7M5QiE/EIlDyJCVmRVzWs/B574Pv979Ve5wBA1Sym+XOy//rLmnbEvbMtZ+LBuvb+Ri8MVsjFDID0TiEDTytPekdqnaeB73JCo+87fsFXJWVspvM2NOmzlYm1kz9MhQEpISsiFCIb8QiUPQSF9Xn3Wu63j8+jELzi2QOxwhDdInlN9qYmJgwur2q/kr/C9WXFqRLdsU8geROIRMaVG+Bf3r9WfphaUE/icmQcxtUspvZ7WalaXyW006Vu9Ix2odmXV6FkGvgrJtu0LeJhKHkGnfOX2HsYExI4+OFCfKc5H3y29H2o3M9u2vbr8aHYUOo46OEsddAETiELKglHEpFjku4reg30SNfy6y7qpqqpC0Zr/NDtZFrJnbZi4+933YH7g/27cvvCOmVRfypa8bfE2jMo2YcGICr2Nfyx1OgZcd5beZ4WnvSX3L+nge9+RN3But7aegy5fTqicnJ/P27VttxSLkAbo6uqx3W0/Y2zC8fvOSO5wCb/bp2dlSfquJno4eG9038izyGTP9Z2ptP0LG06oHBATQs2dPOnXqRK9evXj48CEA27dvZ+pU1TU3d+/exd3dnZiYGK3FqHGSwwkTJjBnzhx0dHTo1q0bb9++pV+/fgwePFhrQQm5W6MyjRjeaDhrrq5hQP0B2Ja2lTukAunv8L9Zf219tpXfatK4bGP1ce9Xrx8NyzTU+j5ls2MHbM3eadUZNAjSmEbkQxlNq16pUiV27dqFnp4eFy5cwNvbm9WrV9OvXz88PDw4efIk69evZ86cORgZGWkteWjscTx48AATExNOnTpFy5Yt8fPz49ChQ1oJRsg75ivnU6JwCYb7DCdZSpY7nAJHG+W3mbHQcSGljEsx9MhQkpK1O5FeQZXRtOqRkZGMGTMGd3d3Fi1axP379wHQ0dFh8eLFTJ48mcaNG9OwoXaTusYeR2JiIgkJCZw6dYq+ffuir6+v1S6xkDeYG5mzxGkJ/Q/2Z+uNrQxuIHqgOSml/NbbxTtby281KVKoCCtcVtBrXy/WXl2Lp71nju07R/Xrl6negbakN636ypUrsbe3Z+3atYSEhKSaCDEoKIjChQsTHh6u9fg09jh69uyJUqkkJiYGOzs7njx5gomJidYDE3I/j7oetCjXgm9OfcPz6Odyh1NgpJTfVi1eNcuz32aHHrV60K5KO2b4z+DJm8+/KZDwsfSmVY+MjFSfLD9w4ECq1+fPn8/OnTt59eoVx48f12p8GhNHv379OHfuHN9//z0KhYKyZcuyY8cOrQYl5A0KhYJ1but4E/eGKaemyB1OgZFSfrvceTkGugaaV8hmCoWCta5rSUhOYMzxMTm+/4IgvWnVBw8ezPLly+nUqROJiYnq1xcuXMiXX35JxYoVWbBgAcuWLePFixfaC1DTdLvbt2+XIiMjpeTkZGnq1KlSp06dpHPnzmVqqt7cQkyrnjVZbfOkE5MkZiOdf3xeSxFpX145zs+jnktFFxeVnH90lpKTkz95O9nR3oVnF0rMRjp89/BnbysniGnV05ft06rv27cPExMTfv/9d968ecN3333HsmXLtJfJhDzHq5UXVmZWjPAZQWJyouYVhE82+/Rs3sS90Xr5bWZMaDqBmiVrMvLoSDH5ZQGjMXFI/z/FwJkzZ+jYsSM2NjZi2gEhFRMDE1a4rODPsD9Ze2Wt3OHkW7f/u60qv22YM+W3mhjoGrDRfSOPXz9mzpk5cocj5CCNiaN27doMGjSIs2fP0rx5c96+fYuOjrjgXEitS40utKvSjpm/zeRp5FO5w8l3JElivK+q/HZOm9zzJd28XHO+sv2K5ReXExAWIHc4n60g/lH8KW3WmAEWLFjAhAkT2Lt3L0ZGRiQkJLBw4cJPClDIvxQKBavbryY+KZ4JJybIHU6+c+zBMXz/8c322W+zw7dtv8XcyJyhR4bm6Wt6ChUqxIsXLwpU8pAkiRcvXlCoUKEsrafxOg6FQsGDBw/47bffGDVqFDExMcTHx39yoEL+VaVYFaY0n8KcM3MYbDsYx0qOcoeUL8hdfqtJ8cLFWea8jP4H+/P99e8Z2ih3TciXWVZWVoSEhPDff//JHUq2SUhIQF8/44kvCxUqhJWVVZa2qzFxzJ49Gx0dHS5dusSoUaMwNjZm9OjR7Nu3L0s7EgqGb5p9w86AnYw8OpI/h/2JoZ6h3CHleeuvrefui7sc7n1YlvLbzPCo68H2m9uZ4jeFTtU7YWGS+ybm00RfX5+KFSvKHUa2CgwMpEaNGtm+XY1DVQEBAcyaNQtDQ9UXQJEiRUhIELeRFNJmpG/E6varVdN8XxTVd5/rRfQLZp9WzX7rZuOmeQWZKBQK1rutJzohmvEnxssdjqBlGhOHnp4eSUlJ6tK/iIgIcXJcyFB7m/Z0rdGV+Wfni7vGfabZp2fzOu51rii/1aRaiWpMaTaFn/76iZP/nJQ7HEGLNGYADw8PRo4cyYsXL/D29qZ379657qYiQu7j7eKNjkJHXFn8GXJb+W1mTG0xFZtiNow4OoLYxFi5wxG0RGPi+OKLL5g0aRJDhw6lZMmSrFu3jvbt2+dEbEIeZl3EmlmtZvHr3V/59e6vcoeT5+TW8ltNCukVYoP7Bh5EPGDhOVF9mV9lasypQoUKtG3bFqVSiZGREU+fijp9QbOxDmOpWbImnsc8iU6IljucPCU3l99qoqyopG/dviz+fTF3nt+ROxxBCzQmjh9//JGmTZsyaNAghg4dqn4Igib6uvqsd1vPv6//ZcHZBXKHk2fk9vLbzFjmvAwTAxOGHRlWoK6LKCg0luPu2LGD48ePY25unhPxCPlMy/It8ajrwZILS+hXrx/VSlTTvFIBlxfKbzUpZVyKb9t+y5AjQ/jhzx8YUH+A3CEJ2Uhjj8PS0hJTU9NP2vjZs2dxcXHBycmJTZs2ffR+fHw8Y8eOxcnJie7duxMSEqJ+b+PGjTg5OeHi4sK5c+cAiIuLo1u3bnzxxRe4ubmxatWqT4pLyFlLnJZQWL8wI4+OFH99apBSfutUySlXl99mxlcNvqKZdTMmnpgo7teSz2hMHNbW1nh4eLBx40a2bdumfmiSlJTE3Llz2bx5Mz4+Phw5coQHDx6kWmbPnj2YmZlx8uRJBgwYwNKlSwHV7Wp9fHzw8fFh8+bNzJkzh6SkJAwMDPjhhx/49ddfOXjwIOfOnePmzZuf1nIhx1iYWLDQcSF+j/z45e9f5A4nV0spv/V28c715bea6Ch02OC+gddxr5l8crLc4QjZSGPiKFOmDM2aNSMhIYGoqCj1Q5OAgADKly+PtbU1BgYGuLm54efnl2oZf39/OnfuDICLiwsXL15EkiT8/Pxwc3PDwMAAa2trypcvT0BAAAqFAmNjY0B1S9vExMQ8/8tVUAxtOJSGpRsy3nc8b+LeyB1OrpQXy281qV2qNhOaTGDbzW2c/fes3OEI2UTjOY7KlSt/VH577NgxjRsOCwvD0tJS/dzCwoKAgICPlildurQqED09TE1NefnyJWFhYdSrVy/VumFhYYCqJ9OlSxceP35Mnz59Ui2Xnri4OAIDAzUul5bY2NhPXjev0labJ9eaTK9TvRi9bzRTbHPXHQNzw3EeenYohfUK82XZL7UeS062t4dFD3Ya72Tg/oHsd9ov23mb3HCMc5q22qwxcWzatOmjxJHWazlFV1eXQ4cO8ebNG0aOHMm9e/eoWrVqhusYGhp+8nwt2prrJTfTVptrUIPfXv7Gpj82MU45jvqW9bN9H59K7uN89P5Rfg/9neXOy2lav6nW95fT7d1UaBNuP7lxJOII01tOz7H9vk/uYyyHz2lzRgkn3cRx5swZzp49S1hYGPPnz1e//vbtW3R1dTXu1MLCgtDQUPXzsLAw9U3W31/m2bNnWFpakpiYSGRkJObm5pla18zMDHt7e86dO6cxcQi5x0LHhewL3McInxH8Puh3dBRi+pr3y29HNh4pdzha4WrjSrea3Zh/bj69aveicrHKcockfIZ0f2stLCyoXbs2hoaG1KpVS/1QKpVs2bJF44br1KlDUFAQwcHBxMfH4+Pjg1KpTLWMUqnkwIEDAPj6+uLg4IBCoUCpVOLj40N8fDzBwcEEBQVRt25dIiIiePNGNT4eGxvLhQsXqFSp0ue0X8hh5kbmLHFawsWQi2y7obnIoiBIKb9d5rwsz5bfZsbKdivR19FnxNERorouj0u3x1G9enWqV69Ohw4d0NPTOKL18Yb19PDy8mLw4MEkJSXRtWtXbGxsWLlyJbVr18bR0ZFu3boxadIknJycKFKkCN7e3gDY2NjQvn17XF1d0dXVxcvLC11dXcLDw5kyZQpJSUlIkkS7du1o06bNp7dekEW/ev3YfGMz35z6hk7VO1G8cHG5Q5JNfiq/1aSMaRkWOi5k9LHR/PL3L/Sq3UvukIRPpJDSSf1jxoxh5cqVdOjQIc0VDx8+rNXAstPnjvOJcdHs91fYX9hutGWQ7SA2dfj4Gp+cJtdxHn10NOuurePPYX9Su1TtHNuvXO1NSk7CYYsDwa+DuTPqDkULFc2xfYvf5exbN92uxJQpqqqXDRs2fNJOBSEjdSzqMNZhLMsuLmOQ7SAcrBzkDinHvV9+m5NJQ066OrpsdN+I3fd2TD01lfXu6+UOSfgE6Z7jGDFCNUdO2bJl2bp1K2XLlk31EITPNavVLMqalmW4z3ASkxPlDifHTTgxARMDkzw1+212aFC6AZ6NPdl4fSOXQi7JHY7wCdJNHO+PYP3xxx85EoxQsJgamuLt4s3N0Jusv1qw/vI8dv8Yxx8cz5Oz32aHuW3mUtasLEOPDCUhSdxRNK9JN3GIK7KFnNCtZjecKzsz47cZPIt8Jnc4OSIhKYHxJ/J3+a0mpoamrGq3ioCwAFZeXil3OEIWpXuO4+HDh+oT448fP/7oJHleOjku5F4KhYI17ddQe31tJp6cyK4uu+QOSevWX1vPned38vTst9mhU/VOdKjagVmnZ9G9ZnfKFy0vd0hCJqWbOI4ePZqTcQgFmE1xG6Y0m8Lcs3P5yvYrlBWVmlfKowpS+a0mCoWC1e1XU3NdTUYfG82hXofESEceke5Q1Ycnw8XJcUGbpjSfQsWiFRl5dCTxSfFyh6M1c87M4XXca5a7LBdfkkD5ouWZ03oOh+8d5uCdg3KHI2SSmO9ByBWM9I1Y47qGO8/vsPzicrnD0Yrb/91m3dV1DG04tMCU32bGGPsx1LOox+hjo4mMi5Q7HCETROIQcg1XG1c6V+/M3DNz+ffVv3KHk+1Sym/ntpkrdyi5ir6uPhvdN/I08ilev3nJHY6QCZlKHLGxsTx8+FDbsQgCK9qtQKFQMOb4GLlDyVYFvfxWE3sre4Y1GsaqK6v445ko/8/tNCYOf39/OnbsyODBgwHVZejDhg3TemBCwVSuSDm8Wnpx6O4hjtw7Inc42SKl/NammE2BLb/NjIWOCyllXIqhR4aSlJwkdzhCBjQmjjVr1rB3717MzMwAqFGjBk+ePNF6YELBNa7JOGqUqMHoY6OJToiWO5zPllJ+m99nv/1cRQsVxdvFm2tPr7Hu6jq5wxEyoDFxpNyZTxByioGuAevc1hH0KohF5xbJHc5neb/81r2qu9zh5Ho9a/XEubIz0/2n8+SN+AM1t9KYOKpUqcLhw4dJSkoiKCiIefPmYWtrmxOxCQVY6wqt6Vu3L99d+I57L+7JHc4nE+W3WaNQKFjnuo6E5ATG+o6VOxwhHRoTx8yZM3nw4AEGBgaMHz8eExMTpk+X59aPQsGyxGkJhfQKMfLoyDx54x9RfvtpKherzIwWM9h7ey9H74sLkXMjjYnDyMiIcePGsW/fPvbv38+4ceMwNDTMidiEAs7SxJIFygWceniKPbf3yB1Olqlnv21dsGa/zQ6Tmk2iRokajDw6Ml+c58pvNN7aL60KKlNTU2rXrk2vXr1EEhG0anij4Wy9sZWxx8fSrko7zAzN5A4pU1LKb5c7L6ekcUm5w8lzDHQN2OC+gVbbWzH3zFwWt10sd0jCezT2OKysrDA2NqZHjx706NEDExMTjI2NCQoKYsaMGTkRo1CA6erost5tPaFvQ5l9erbc4WSKKL/NHi3Lt2Rg/YEsu7iMW+G35A5HeI/GHseNGzfYt2+f+rlSqaRr167s27cPN7eCPUmbkDPsrez5usHXrLq8igH1B1DXoq7cIWUopfz2116/ivLbz7TEaQmH7x1m6JGhnBt4Dh2FmOwiN9B4FKKjo3n69Kn6+dOnT4mOVo056uvray8yQXjPQseFmBuZM9xnOMlSstzhpCul/LZtpbai/DYbFC9cnKVOS7kQfIEtf2yROxzh/2lMHFOmTKFPnz54eHjg4eHBl19+yTfffEN0dDSdOnXKgRAFQfUF8l3b77gQfIEfbv4gdzjpSim/9XbxFuW32aRfvX60rtCayacmE/Y2TO5wBDIxVNWqVStOnDihnquqYsWK6hPiAwYM0GpwgvC+/vX7s+XGFiafmkzH6h0pZlRM7pBSCfwvUJTfaoFCoWC923rqrq/LhBMT2Nllp9whFXiZGjAMCgri4cOH3Llzh2PHjnHw4EEthyUIH9NR6LDObR0vY14yzW+a3OF8ZPyJ8aL8Vkuql6jOlOZT2PXXLk49PCV3OAVepuaqmjdvHvPnz+fy5cssWbIEf3//nIhNED5S16IunvaebLq+icshl+UORy2l/NarlZcov9WSaS2mUaVYFUb4jCA2MVbucAo0jYnD19eXH374gRIlSrBo0SIOHTpEZKS42Yogn9mtZ1PatDTDfYbnillU3y+/HdV4lNzh5FuF9Aqx3m099yPu5/k5zPI6jYnD0NAQHR0d9PT0ePv2LcWLF+fZs2c5EZsgpMnM0AxvF29uhN5g/bX1cofDhmsbxOy3OaRtpbb0qdOHxecXc/f5XbnDKbA0Jo7atWvz5s0bunfvTpcuXejcubOY5FCQXfea3WlbqS0z/GcQ+jZUtjheRL9g1ulZovw2By13Xk5h/cIM8xmWJ+cwyw8yTBySJDF06FDMzMzo3bs3W7duZfHixSxaJLqJgrwUCgVrXdcSkxjDpJOTZItDPfuts5j9NqdYmFiw2HExp4NO82PAj3KHUyBlmDgUCgVDhgxRP7eysqJ69epaD0oQMqNq8apMbjqZnQE7OR10Osf3/375bR2LOjm+/4Ls64Zf08SqCRNOTOBF9Au5wylwNA5V1axZk4CAgJyIRRCybFqLaVQsWpERPiOIT4rP0X2L8lv56Ch02Oi+kVexr/jm1Ddyh1PgaEwcf/75J7169aJt27Z06NBB/RCE3MBI34hV7VcR+DwQ74veObZfUX4rvzoWdRjvMJ4tN7Zw7t9zcodToGi8cnzLFjE/jJC7uVd1p2O1jsw9O5fedXpTrkg5re5PlN/mHl6tvPjl718YemQoN4fdFFVtOURjj6Ns2bI8e/aMS5cuUbZsWYyMjEhOzr2TzAkF08p2K5EkibHHx2p9X6L8NvcwNjBmretaAp8HsvTCUrnDKTAydeX45s2b2bRpEwAJCQlMmiRfFYsgpKV80fJ4tfLiwJ0D+Nzz0dp+RPlt7uNW1Y2uNboy7+w8/on4R+5wCgSNiePkyZOsX78eIyMjACwsLIiKitJ6YIKQVeObjKd6ieqMPjaamIQYrexDlN/mTivbrURfRz/P3p8+r9GYOPT19VEoFOpfkpR7cQhCbmOga8Ba17U8evWIxb9n/61GU8pvhzQYIspvc5myZmWZr5yP7z++7P57t9zh5HsaE0f79u3x8vLizZs37N69m4EDB9KjR4+ciE0QskxZUamekuL+i/vZuu0JJyZgYmDC3DZzs3W7QvYYaTeShqUbMtZ3LK9iX8kdTr6mMXF89dVXuLi44OzszKNHj/D09MTDwyMnYhOET7LUaSmF9Aox6tiobBu2OHb/GMceHBPlt7mYro4uG903Eh4VznS/6XKHk69pLMfdtm0brq6uNGvWLCfiEYTPVtq0NPPazGPM8THsvb2X7rW6f9b2UspvqxSrIspvc7mGZRoyym4Uq6+spn/9/jQu21jukPIljT2OqKgoBg0aRJ8+fdi5cyfPnz/P9MbPnj2Li4sLTk5O6qqs98XHxzN27FicnJzo3r07ISEh6vc2btyIk5MTLi4unDunurjn2bNneHh44OrqipubGz/8kHtvISrIa4TdCOpb1mec7zgi4z7vNgCi/DZvmaecR2nT0gw9MpTE5ES5w8mfpEwKDAyUli9fLrm4uEj9+/fXuHxiYqLk6OgoPX78WIqLi5M6dOgg3b9/P9UyO3fulGbOnClJkiQdOXJEGjNmjCRJknT//n2pQ4cOUlxcnPT48WPJ0dFRSkxMlMLCwqRbt25JkiRJkZGRkrOz80fbTMvt27cz28xsXTevyi9tvhh8UWI20gTfCRqXTa/NL6JfSOaLzSXHHxyl5OTk7A5RNvnlGKdn3+19ErORll1Ypn4tv7c5Ldr67svUrWMBihcvTokSJShatCgvXmieVCwgIIDy5ctjbW2NgYEBbm5u+Pn5pVrG39+fzp07A+Di4sLFixeRJAk/Pz/c3NwwMDDA2tqa8uXLExAQQKlSpahVqxYAJiYmVKpUibAwcfN6IW0OVg583eBrVlxawV9hf33SNmafns3ruNd4u3iL8ts8pHP1zrhXdcfrNy8ev34sdzj5jsZzHLt27eL48eNERETQrl075s+fT5UqVTRuOCwsDEtLS/VzCwuLjyZLDAsLo3Tp0qpA9PQwNTXl5cuXhIWFUa9evVTrfpggQkJCCAwMTLVceuLi4ggMDNS4XFpiY2M/ed28Kj+1eaD1QPbc2sOAvQP4sc2P6X75p9Xmf978w7qr6+heqTt6EXoERuSPzwTy1zFOzxibMfj948eA3QNY23xtgWjzh7TVZo2JIzQ0lGnTplGjRg1A9SV87Ngx2rdvn+3BZFZUVBSenp5MmzYNExMTjcsbGhqq48+qwMDAT143r8pvbV6avJTBhwdzNf4q/ev3T3OZtNo8YdcEjA2MWd15db6rpMpvxzgtNajBnLg5TD41mbuKu1QrVC3ft/lDn3OcM0o4GoeqJkyYQNWqVTlz5gyTJk2iTZs2HDt2TONOLSwsCA19d2e2sLAwLCwsPlom5Ta0iYmJREZGYm5unuG6CQkJeHp60qFDB5ydnTXGIQgDbQfSxKoJk05OIiImIlPrqMtvW4ry27xsrMNY6pSqw+hjo4lKEDNeZJcME8eVK1fw8vJCqVSyd+9eLly4gJ+fH6tWrdK44Tp16hAUFERwcDDx8fH4+PigVCpTLaNUKjlw4AAAvr6+ODg4oFAoUCqV+Pj4EB8fT3BwMEFBQdStWxdJkpg+fTqVKlVi4MCBn9FsoSDRUeiw3m09L2JeZKq+//3y29H2o3MgQkFb9HX12ei+kSdvnrDm7zVyh5NvpJs4WrZsyfLly2nQoAE+Pj6sXr0aQ0ND9ZxVmujp6eHl5cXgwYNxdXWlffv22NjYsHLlSvVJ8m7duvHq1SucnJzYtm0bEydOBMDGxob27dvj6urK4MGD8fLyQldXl+vXr3Po0CEuXbpEx44d6dixI2fOnMmGj0HI7+pZ1mN049FsvL6Rq0+uZrisKL/NX5pYN2Fow6HsuLeD/YH75Q4nX1BIUtqX1i5YsAA/Pz9sbGxwd3fH0dGRDh06fFQZlRd87jifGBfNH97EvaH6muqUMS3D5cGX0dXRVb+X0uaImAiqrKpCg9INOOlxMt9WUuXXY5yeqPgomm1qxp1Xd/Dt60urCq3kDilHaOu7L90ex/Tp0/Hz82PgwIFcuXKFdu3aERERwdGjR8XsuEKeZGZoxnKX5Vx/dp2N1zemucyc03NE+W0+ZGxgzPrm66loXpEvfv6CP0P/lDukPC3DcxwKhQIHBwfmzZuHn58fy5cvx8/P76NzFYKQV/Ss1RPHio5M85tG2NvUJd6B/wWy9upaMfttPlXUsCi+fX0xMzSj3a52PHr5SO6Q8qxMXwCor69PmzZtWLZsmTivIORZCoWCta5riU6IZtLJ1Dckm3BCVX4rZr/Nv8oVKcfxL48TlxiH805nwqPC5Q4pT8p04nhfoUKFsjsOQcgx1UpUY1LTSfwY8CNnglR/BJ19dlaU3xYQtUrV4kifIzx58wS3n9w+ey6zguiTEocg5HXTW06nfJHyjDg6guiEaL67+Z0ovy1Amlo35Zduv3Dj2Q267u5KfFK83CHlKekmjo0bN3L79u2cjEUQckxh/cKsbr+a2//dpvX21jyMfCjKbwuYDtU68H2H7zn58CQDDg4gWUqWO6Q8I90pR6ytrdmxYwd37tyhevXqtGzZkmbNmlGkSJGcjE8QtKZDtQ50qNqBw/cO41DKgQ5VO8gdkpDDBtoOJCwqjKl+UyllXEpU02VSuonD1dUVV1dXAG7fvs25c+cYNWoUycnJNGnShJYtW1K3bt0cC1QQtGF1+9XoKHT4uuLX4gujgPqm2TeEvg1l5eWVlDYpzTfNv5E7pFxP4ySHADVr1qRmzZoMHTqUt2/fcv78efbs2SMSh5DnlS9anoO9Dha4WVOFdxQKBctdlhMeFc4UvymUMi7FQFsxpVFGMpU43mdiYoKLiwsuLi7aiEcQBCHH6Sh02N5pO/9F/8fXh7+mpHFJ3Ku6yx1WriWqqgRBEAADXQP299hPfcv69NjTgwvBF+QOKdcSiUMQBOH/mRqacvTLo5Q1K4v7T+7c/k9UlqYlU4kjLCyMP/74g6tXr6ofgiAI+VEp41Kc6HsCQz1DXHa6EPw6WO6Qch2N5ziWLFnCsWPHqFy5Mrq672YTtbOz02pggiAIcqloXpFjXx6j1fZWuOx04fdBv1PMqJjcYeUaGhPHqVOnOH78OAYG4sIoQRAKjvqW9TnU6xAuO11w/8mdU/1OUVi/sNxh5Qoah6qsra1JSEjIiVgEQRByldYVWvNTl5+4FHKJHnt6kJAkvgshEz0OIyMjOnXqRJMmTVL1OmbMmKHVwARBEHKDrjW7ss5tHcN9hjPkyBC2frG1wF8sqjFxKJVKcf8NQRAKtGGNhhH2NozZZ2ZjaWzJoraL5A5JVhoTR+fOnXMiDkEQhFzNq5UXoW9DWXx+MRYmFox1GCt3SLJJN3GMGTOGlStX0qFD2hO/HT58WGtBCYIg5DYKhYI1rmsIjw5nnO84LIwt6F2nt9xhySLdxDF9+nQANmzYkGPBCIIg5Ga6Orrs6rKLdjvb0f9gf4oXLo5zZWe5w8px6SaOUqVKAVC2bNkcC0YQBCG3K6RXiEO9DtFqeyu6/NKF3/r/hl3ZgnVdW7qJw9bWNlXlgCRJKBQK9b9//PFHjgQoCIKQ2xQpVIRjXx6j6damuP7kyvlB56lavKrcYeWYdBNHkyZNeP78OU5OTri5uVGmTJmcjEsQBCFXK21amhN9T9BsazOcf3TmwlcXKGNaML4n070AcN26dWzZsoVixYoxc+ZM+vbty65du3j16lUOhicIgpB72RS34eiXR3ke/Zz2u9rzKvaV3CHliAyvHDc1NaVr1658//339OzZk1WrVnHgwIGcik0QBCHXa1SmEQd6HiDwv0A6/tyR2MRYuUPSugyv4/jjjz/w8fHh2rVrNGzYkLVr19KoUaOcik0QBCFPcKrsxA+dfqDP/j702deHPd33oKujq3nFPCrdxKFUKjE1NcXNzY158+apZ8b9+++/AahVq1bORCgIgpAH9K7Tm/CocMb6jmWEzwg2uG/It1OTpJs4Uspwz507x++//44kSer3FAoFO3bs0H50cgsPRxEdLXcUgiDkEWMcxqivLi9tWprZrWfLHZJWpJs4fvzxx5yMI3dycqJKcDBMnw4jRoCRkdwRCYKQyy10XEhYVBhzzszBwtiC4XbD5Q4p24lbx2Zk+3Zia9aEiROhcmVYtw7i4+WOShCEXEyhULCpwybcq7oz8uhI9t7eK3dI2U4kjozY2hL8/fdw5owqcYwcCVWrwrZtkJgod3SCIORSejp6/NLtF5pYN+HL/V9yOui03CFlq3QTh7h503tatoSzZ+H4cShZEgYNglq14OefITlZ7ugEQciFCusX5nDvw1QpVoWOP3fkZuhNuUPKNukmjp49ezJixAj+97//ERISkpMx5U4KBbi4wJUrcOAAGBhA795Qvz4cOgTvFQ8IgiAAFDMqhm9fX4oYFqH9rvY8fPlQ7pCyRbqJY//+/UybNg2AhQsX0rVrVxYuXMjvv/9OfEEe51cooFMn+PNP+OkniI1VPbe3hxMnRAIRBCEVKzMrfPv6Ep8Uj8tOF8KjwuUO6bNleI7DysqK3r17s27dOn7++WfatGnDhQsX6NOnD0OGDMmpGHMnHR1Vj+P2bdi6FcLDVT2SVq3g3Dm5oxMEIRepUbIGR3of4cmbJ7juciUyLlLukD5Lpk+O6+vr06RJEyZPnszevXuZN2+eNuPKO/T0YOBAuHsX1qyB+/dV50RcXODqVbmjEwQhl2hi3YQ93fdwM/QmXXZ3IT4p747cfHJVlYWFRXbGkfcZGqqqrv75B5YsgevXoXFj1TDWX3/JHZ0gCLmAW1U3tnyxhVMPT9H/YH+SpbxZXCPKcbNb4cKq6z4ePYJ58+D0aahXTzWsde+e3NEJgiCz/vX7823bb/n51s+MOz4u1awceYXGxBEXF/fRaxEREZna+NmzZ3FxccHJyYlNmzZ99H58fDxjx47FycmJ7t27p6re2rhxI05OTri4uHDuvXMGU6dOpUmTJri7u2cqBtmYmsKMGfDwIUydCocPQ40aqlLeoCC5oxMEQUaTmk5inMM4Vl1Zxbfnv5U7nCzTmDi6devGzZs31c99fX3p3VvzDdqTkpKYO3cumzdvxsfHhyNHjvDgwYNUy+zZswczMzNOnjzJgAEDWLp0KQAPHjzAx8cHHx8fNm/ezJw5c0hKSgKgS5cubN68OSttlFexYrBggSqBjBmjqsSqWlU1rPX0qdzRCYIgA4VCwVLnpfSp04epflPZemOr3CFlicbEsXTpUubNm8e3337LhAkT2L17Nz/88IPGDQcEBFC+fHmsra0xMDDAzc0NPz+/VMv4+/vTuXNnAFxcXLh48SKSJOHn54ebmxsGBgZYW1tTvnx5AgICALCzs6NIkSKf0lZ5lSoFy5fDgwfw1VewaZPqavSJE+G//+SOThCEHKaj0GFbx204V3bm68Nf8+vdX+UOKdMyvB8HQLVq1Rg+fDiTJk3C2NiYXbt2YWlpqXHDYWFhqZazsLBQf/m/v0zp0qVVgejpYWpqysuXLwkLC6NevXqp1g0LC8t0oz4UFxdHYGDgJ60bGxv7yeumy9MT/c6dKbF+PUW8vUlev56X/frxYsAAks3Msndfn0Arbc7lClqbC1p7Ife2eUHdBTx7+Ywee3qwpdUWGpRokG3b1labNSaOadOmERwczK+//kpQUBBDhw7Fw8ODL7/8MtuD0RZDQ0Nq1KjxSesGBgZ+8roZqlEDnJ0hMBDd2bMpsWEDJX7+GSZNAk9PMDHJ/n1mktbanIsVtDYXtPZC7m6zX2U/mm1txqgLo/h94O/UKpU99zv6nDZnlHA0DlVVrVqVHTt2YG1tTYsWLdizZ4/6Zk4ZsbCwIDQ0VP08LCzsoxJeCwsLnj17BkBiYiKRkZGYm5tnat18o0YN+OUXuHEDWrRQTeFeqRJ4e0NMjNzRCYKQA0oal+SExwmM9Ixw2enC49eP5Q4pQxoTx4ABA1LdxcrU1JSFCxdq3HCdOnUICgoiODiY+Ph4fHx8UCqVqZZRKpXqe5j7+vri4OCAQqFAqVTi4+NDfHw8wcHBBAUFUbdu3ay2LW+pXx9+/RUuXlSV744fD1WqwIYNYip3QSgAKhStwPG+x3kb/xaXnS68iH4hd0jp0pg4goKC8PT0xNXVFUdHR/VDEz09Pby8vBg8eDCurq60b98eGxsbVq5cqT5J3q1bN169eoWTkxPbtm1j4sSJANjY2NC+fXtcXV0ZPHgwXl5e6lvXjh8/nl69evHo0SNatmzJnj17Pqf9uY+DA5w8Cb/9BhUqwPDhUL06/PCDmMpdEPK5uhZ1+bX3rzx6+Qi3n9yIio+SO6S0SRr06tVLunDhguTu7i6FhIRIq1atklasWKFptVzl9u3bsqz72ZKTJenoUUlq0ECSQJKqVZOkn3+WpKQkre5W1jbLpKC1uaC1V5LyVpsPBB6QdOboSK67XKX4xPhP3o62vvsydQFgkyZNANV9yEePHs2ZM2e0ntAEVDPxtm8P167B/v2qebF69QJbW9WwVh684lQQBM06Ve/Eerf1HL1/lMGHB+e6q8s1Jg4DAwOSk5MpX748O3fu5OTJk0RF5dLuU36lUEDnzqqp3Hftguho6Njx3bBWLvuhEgTh8w1pOIS5reey488dTDk1Re5wUtGYOKZNm0ZMTAwzZszg77//5tChQ3z7bd67RD5f0NWFPn1UU7lv3gzPnqlKetu0gd9/lzs6QRCy2YyWMxjRaATfXfiO5ReXyx2OmsbrOFKqmYyNjVm0aJHWAxIyQV9fdfV5377w/feqKU1atIB27VQTKzZqJHeEgiBkA4VCwar2qwiPDmfCiQlYGFvwZV35r6FLN3EMGzYswxU3bNiQ7cEIWWRoCKNGqSZOXLsWFi8GOzvVsNbcuVC7ttwRCoLwmXR1dNnZeScvol8w4NAAShQugUsVF1ljSjdx3Lx5k9KlS+Pm5ka9evVy3ckZ4T2FC6uuOB86FFasgGXL4OBB1Yn0OXPAxkbuCAVB+AyGeoYc6HmA1j+0puvurvj396dx2cayxZPuOY7z588zbtw47t+/z4IFCzh//jzm5uY0btyYxo3lC1jIgJkZeHmpZuL95hs4dEh1ZfrgwfDvv3JHJwjCZyhSqAjHvjxGKeNSuP3kxt3nd2WLJd3EoaurS8uWLfn222/ZvXs35cuXx8PDg507d+ZkfMKnKF4cFi1S3Y1w1Cj48UdVr2PUKNUJdUEQ8iRLE0tOeJxAgQKXnS48jZTn1gwZVlXFx8dz4sQJJk6cyK5du/Dw8MDJySmnYhM+l6WlaujqwQPVfdE3blTNgzVpEjx/Lnd0giB8girFqnDsy2O8iHlBu53teBX7KsdjSDdxTJ48mZ49e/L3338zatQo9u3bx8iRI/PvZIP5mbW1KmncuQPdu6vOgVSqBLNmwevXckcnCEIWNSzTkAM9D3Dn+R2++N8XxCTk7ISo6SaOlGnUd+zYQa9evWjQoAENGjTA1taWBg2yb754IQdVrgw7dsCtW+Dioqq8qlhRNawlLuoUhDylbaW2/Nj5R35//Dt99vchKTkpx/adblXVnTt3ciwIIYfVrAl79qimcp85E6ZNUw1pTZ0KGsqwBUHIPXrW7kl4VDiexz0Z7jOcje4bU81mri0arxwX8jFbWzhyBC5cUF3zMW4cVKmCxfz5sHMn3L8vpjMRhFxutP1opreYzvd/fM+s07NyZJ8arxwXCoAmTcDPD/z94bvvKHrgAPz0k+q9YsXA3v7do3Fj1WuCIOQa89rMI/RtKPPOzsPC2IKRjUdqdX8icQjvKJWgVHL31i1qSBJcugSXL6sex4+/631UrapKIg4Oqn/r1lVNgyIIgiwUCgUb3DfwX/R/jD42mlLGpeheq7vW9icSh/AxXV3VhYN16sDXX6tee/NGNb17SiI5cUJ1fQhAoULQsOG7XomDg6qSKwfGWgVBUNHT0ePnrj/jvNOZvgf6UrxwcUpTWjv70spWhfzHzEzdIwFUvY/Hj1VJJKVnsm4dLP//GTwtLVMnkkaNwNRUvvgFoQAw0jfi116/0mJbCzr93IltrbZRgxrZvh+ROIRPo1BA+fKqR48eqtfi4yEg4F2v5NIl1bQnADo6qmqulOEtBwdVr+b/bwksCEL2MDcyx7evL023NmX+H/Pp2rRrtu9DJA4h+xgYqHoWjRrByP8/ORcRAVeuvOuV7NunupcIgImJajbf98+XWFrKF39+FxcHISEQEoJJQAAkJKimojEykjsyIZuVNSvL5cGXufTXJa1sXyQOQbuKFVPdJ6RdO9VzSVKV+b7fK1m6FBITVe+XK/cuidjbQ4MG4ostM2Jj4ckTVWIIDk773//+Uy9unfIfhQIqVIDq1VU9wOrV3z1KlpSjJUI2sTSxpFrRalrZtkgcQs5SKFRVWVWrgoeH6rWYGNXFiO9Xce3erXpPTw/q1UvdK7GxKVgn3lOSQkoC0JAU1IoWVRUpWFmpenZWVqqHtTWPIiKoKEmqaWgCA1X//vabal8pihdPnUhSkkuFCmKIsYATiUOQn5ERNG2qeqQIDU3dK9mxQ3XyHcDcPPW1Jfb2effakg+TQlrJIa2kYG6uTgLqpJCSJKytoWxZ1VBgersNDFQlgfclJ6sKHt5PJnfuwOHDsGXLu+UMDFSJ/8NeSrVqYGycTR+MkJuJxCHkTpaW0LGj6gGQlKT6Mnu/VzJ37rtrS2xsPr62xMBAvvgh/aTwfnLITFJISQiZTAqfTEdH1ZuoUOHd0GKKiIh3iSTlcfMm7N+vSjgpypX7uIdSvTpYWBSsXmI+JxKHkDfo6qqmRaldW3VjKoDIyHfXlly6BKdOqaZKAdVtdT+8tqRcuez78oqNfffln955hbSmrjc3Tz189H4vISU55Ma/2osV+7hXCKoT7g8evEsmKT2VLVtST5xZpEja51EqVRIXj+ZBInEIeZepKbRpo3qAqvcRHJy6V7J+PXh7q963sEjdK7GzS/vakveTQnrnFTQlhcaNU/cScnNS+ByGhlCrlurxPklS9bbeH/K6c0d14ej27e+W09eHKlU+7qVUq6a6dkjIlUTiEPIPhULVqyhX7t21JQkJ764tSUkov/76bvlataB+fayePIFXr9JPCsWKvfvyT0kKH55TyG9J4XMoFO8+rw9v/vb6Ndy9mzqhBAaqzqWkVNcBlCmTdi+lbFkx7CUzkTiE/E1fXzVk1bAhjBihei3l2pKUXsnp0+gbG6v+8m3cOO1zCiIpZJ8iRVSfc+PGqV9PSICHDz8+Of/jj6opb1KYmKRd7VWlivzntQoIkTiEgufDa0uAR4GB1PiwykjIWfr6qiGqatXeFUWAatgrLOzjYa+zZ9+d0wLVebBKldLupZib53x78jGROARByN0UClWVnaXlu/NZKd6+hXv3Ph728vVVTYGTwsKC8mXKqHqP5ubvHkWLpn7+/muFC4shsXSIxCEIQt5lYqKaXeDD21knJUFQUKphL+nWLdUJ+1u34OVL1bmWjOjrZy7BpPWamVm+TjoicQiCkP/o6kLlyqqHmxsAjz8cjkxKUp07efky9ePVq7Rfe/FCVXqc8jwpg3t86+i8SyJZTTpFi+b6K/NF4hAEoWDS1X33pZ1VkqQaJvswwWSUeEJC3j1/fxgtLWZmaSeYzCShHCgQEIlDEAQhqxQK1TVApqaq8u+skCTV/GzpJZi0Xrt//93z6OiMt1+4sDqZlGzeHDZs+LQ2ZkAkDkEQhJykUKi+3AsXVl2rklXx8ZlOOknamJoGkTgEQRDyFgMDKFVK9dAgIjAQCy2EoKOFbQqCIAj5mEgcgiAIQpaIoaoM7NkDJ06UpEQJuSPJWZGRJahYUTXLhrGxqlQ+vf8XLpzrKwcFQchmInFkYNs2OHWqWH6+jucjkgQJCVm7ZWihQhknl4z+n9H7hob5+hoqQciztJo4zp49y4IFC0hOTqZ79+4MGTIk1fvx8fFMnjyZv//+m6JFi+Lt7Y2VlRUAGzduZO/evejo6DBjxgxatGiRqW1mp6NHITDwboGbw+jWrUDKlatBVJTqlgpv36b+N7P/Dwn5+PWMrpn6kI5O1hNRZv+vJ/5kEoRPprVfn6SkJObOncu2bduwsLCgW7duKJVKqlSpol5mz549mJmZcfLkSXx8fFi6dCkrVqzgwYMH+Pj44OPjQ1hYGAMHDsTX1xdA4zaFz6erq7r+KLtvhyBJqkrCrCagD///+jU8fZr6dU2l7R8yNHyXRIyNISGhEoaG2dve3CwurmC1Fwpmm+3sSqW6/Ul20VriCAgIoHz58lhbWwPg5uaGn59fqi95f39/Ro0aBYCLiwtz585FkiT8/Pxwc3PDwMAAa2trypcvT0BAAIDGbQq5l0Kh+sI2NITixbN328nJquTxKckoKgpev47DzKzgfKu8eVOw2gsFs83W1gla2a7WEkdYWBiWlpbq5xYWFuov//eXKV26tCoQPT1MTU15+fIlYWFh1KtXL9W6YWFhABq3mZa4uDgCAwM/qR2xsbGfvG5elR/arK+vuni2aNHMLR8bG0uhQoW0GVKuUtDaCwW3zYGBL7N9uwVipNfQ0PCTz1MEFsD7NIg2538Frb0g2vwp66ZHa9dxWFhYEBoaqn4eFhaGhYXFR8s8e/YMgMTERCIjIzE3N0933cxsUxAEQdAurSWOOnXqEBQURHBwMPHx8fj4+KBUKlMto1QqOXDgAAC+vr44ODigUChQKpX4+PgQHx9PcHAwQUFB1K1bN1PbFARBELRLa0NVenp6eHl5MXjwYJKSkujatSs2NjasXLmS2rVr4+joSLdu3Zg0aRJOTk4UKVIEb29vAGxsbGjfvj2urq7o6uri5eWF7v9fZZbWNgVBEISco5AkSZI7CG373HE+MS6a/xW0Nhe09oJoc3auK+aqEgRBELJEJA5BEAQhS0TiEARBELKkQJzjuHnzJoYFba4BQRCEzxAXF0f9+vXTfK9AJA5BEAQh+4ihKkEQBCFLROIQBEEQskQkDkEQBCFLROIQBEEQskQkDkEQBCFLROIQBEEQsqRA3I/jU+Tkvc1zi6lTp3L69GmKFy/OkSNH5A5H6549e8bkyZN58eIFCoWCHj160L9/f7nD0qq4uDi+/PJL4uPjSUpKwsXFBU9PT7nDyhEpE6NaWFiwceNGucPROqVSibGxMTo6Oujq6rJ///7s27gkfCQxMVFydHSUHj9+LMXFxUkdOnSQ7t+/L3dYWnflyhXp1q1bkpubm9yh5IiwsDDp1q1bkiRJUmRkpOTs7Jzvj3NycrL09u1bSZIkKT4+XurWrZt048YNeYPKIVu3bpXGjx8vDRkyRO5QckSbNm2kFy9eaGXbYqgqDe/fL93AwEB9b/P8zs7OjiJFisgdRo4pVaoUtWrVAsDExIRKlSqpb1GcXykUCoyNjQHVzdMSExNRKBQyR6V9oaGhnD59mm7duskdSr4gEkca0rpfen7/QinoQkJCCAwMTHWv+/wqKSmJjh070rRpU5o2bVog2rxw4UImTZqEjk7B+sr76quv6NKlC7/88ku2brdgfYqCkIaoqCg8PT2ZNm0aJiYmcoejdbq6uhw6dIgzZ84QEBDAvXv35A5Jq3777TeKFStG7dq15Q4lR/3vf//jwIEDfP/99+zatYurV69m27ZF4kiDuLd5wZGQkICnpycdOnTA2dlZ7nBylJmZGfb29pw7d07uULTqjz/+wN/fH6VSyfjx47l06RITJ06UOyytS/nOKl68OE5OTgQEBGTbtkXiSIO4t3nBIEkS06dPp1KlSgwcOFDucHJEREQEb968ASA2NpYLFy5QqVIlmaPSrgkTJnD27Fn8/f1Zvnw5Dg4OLF26VO6wtCo6Opq3b9+q/3/+/Plsvc22KMdNQ3r3S8/vxo8fz5UrV3j58iUtW7Zk9OjRdO/eXe6wtOb69escOnSIqlWr0rFjR0D1GbRq1UrmyLQnPDycKVOmkJSUhCRJtGvXjjZt2sgdlpDNXrx4wciRIwHVOS13d3datmyZbdsX06oLgiAIWSKGqgRBEIQsEYlDEARByBKROARBEIQsEYlDEARByBKROARBEIQsEeW4gpCG58+fs2jRIm7evEmRIkXQ19dn8ODBODk55Xgsly9fRl9fnwYNGgCqK4KNjIzo1KlTjsciCCAShyB8RJIkRo4cSadOnVi2bBkAT548wd/fX2v7TExMRE8v7V/HK1euULhwYXXi6N27t9biEITMENdxCMIHLl68yNq1a9m5c+dH7yUlJbF06VKuXLlCfHw8X375Jb169eLy5cusWbMGc3Nz7t27R61atVi6dCkKhYJbt26xePFioqOjMTc3Z9GiRZQqVQoPDw+qV6/O9evXcXd3p0KFCqxfv56EhASKFi3K0qVLiY2NpWfPnujo6FCsWDFmzpzJxYsXKVy4MF999RWBgYHMmjWLmJgYypUrx8KFCylSpAgeHh7UrVuXy5cvExkZyYIFC2jUqJEMn6aQH4lzHILwgfv371OzZs0039u7dy+mpqbs27ePffv2sXv3boKDgwG4ffs206ZN4+jRo4SEhHD9+nUSEhKYP38+q1atYv/+/XTt2hVvb2/19hISEti/fz+DBg2iYcOG7N69m4MHD+Lm5sbmzZuxsrKiV69eDBgwgEOHDn305T958mQmTpzI4cOHqVq1KmvWrFG/l5SUxN69e5k2bVqq1wXhc4mhKkHQYM6cOVy/fh19fX3Kli3L3bt38fX1BSAyMpJ///0XfX196tatq56Ov3r16jx58gQzMzPu3bunngsrOTmZkiVLqrft6uqq/n9oaCjjxo3jv//+Iz4+HisrqwzjioyMJDIyksaNGwPQuXNnxowZo34/5XxMrVq1ePLkSTZ8EoKgIhKHIHzAxsaGEydOqJ/PmjWLiIgIunXrRpkyZZgxYwYtWrRItc7ly5cxMDBQP9fV1VXPB2VjY5Pu/RCMjIzU/58/fz4DBgzA0dFRPfT1OVLi0dHRISkp6bO2JQjvE0NVgvABBwcH4uLi+Omnn9SvxcbGAtC8eXP+97//kZCQAMCjR4+Ijo5Od1sVK1YkIiKCGzduAKqhqfv376e5bGRkpHoq7IMHD6pfNzY2Jioq6qPlTU1NMTMz49q1awAcOnQIOzu7LLRUED6N6HEIwgcUCgVr165l0aJFbN68mWLFimFkZMTEiRNp164dT548oUuXLkiShLm5OevWrUt3WwYGBqxatYr58+cTGRlJUlIS/fv3T3O25VGjRjFmzBiKFCmCvb09ISEhALRp0wZPT0/8/PyYOXNmqnW+/fZb9clxa2trFi1alL0fhiCkQVRVCYIgCFkihqoEQRCELBGJQxAEQcgSkTgEQRCELBGJQxAEQcgSkTgEQRCELBGJQxAEQcgSkTgEQRCELPk/53YmJFCqg3MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 288.5776114543279 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 11   # max of individuals per generation\n",
    "max_generations = 5    # number of generations\n",
    "gene_length = 6      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 4\n",
      "Number of neurons: 200 , Batch size 8 , Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:2])    # (8)\n",
    "    num_units_bits     = BitArray(bi[2:4])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[4:5])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[5:6])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    \n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1])\n",
    "    print('Number of neurons:', best_num_units[-1], ', Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>421.005343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>352.227582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>357.309937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>432.142172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>345.442092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>225.669814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>264.313923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>265.501271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>548.526137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>239.436815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>373.236596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>402.869851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>503.115913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>198.005508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>429.715875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>400.147505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>295.546846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>488.480635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>234.262063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>211.798278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>380.579088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>307.778145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>362.718876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>339.872833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>301.072024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>415.294774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>183.786620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>311.316117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>316.592388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>283.338394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>329.132674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>636.317267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>548.005009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>562.824315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>253.672702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>502.648646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>434.566360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>308.878368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>150.082613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>412.379099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>258.823786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>191.672038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>292.936563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>283.497283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>391.682864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>220.065698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>265.441032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>247.928394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.003811</td>\n",
       "      <td>0.003811</td>\n",
       "      <td>205.651847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>254.975026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>372.129192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             4        200         0.0001           8  0.000020  0.000020   \n",
       "1             3        200         0.0010           8  0.000045  0.000045   \n",
       "2             4        200         0.0001           8  0.000051  0.000051   \n",
       "3             4        200         0.0010           8  0.000067  0.000067   \n",
       "4             1        150         0.0010           8  0.000077  0.000077   \n",
       "5             2        100         0.0010           8  0.000077  0.000077   \n",
       "6             1        200         0.0010           8  0.000077  0.000077   \n",
       "7             1        200         0.0010           8  0.000083  0.000083   \n",
       "8             4        200         0.0001           8  0.000087  0.000087   \n",
       "9             1        200         0.0010           8  0.000101  0.000101   \n",
       "10            1        150         0.0010           8  0.000109  0.000109   \n",
       "11            4        200         0.0001           8  0.000110  0.000110   \n",
       "12            4        200         0.0001           8  0.000110  0.000110   \n",
       "13            1        200         0.0010           8  0.000111  0.000111   \n",
       "14            4        200         0.0001           8  0.000123  0.000123   \n",
       "15            4        200         0.0001           8  0.000134  0.000134   \n",
       "16            3        200         0.0010           8  0.000138  0.000138   \n",
       "17            4        150         0.0001           8  0.000140  0.000140   \n",
       "18            1        200         0.0010           8  0.000145  0.000145   \n",
       "19            1        200         0.0010           8  0.000149  0.000149   \n",
       "20            4        200         0.0001           8  0.000150  0.000150   \n",
       "21            4        200         0.0001           8  0.000164  0.000164   \n",
       "22            1        150         0.0010           8  0.000184  0.000184   \n",
       "23            4        150         0.0010           8  0.000197  0.000197   \n",
       "24            4        200         0.0001           8  0.000209  0.000209   \n",
       "25            4        200         0.0001           8  0.000216  0.000216   \n",
       "26            2        150         0.0010          16  0.000228  0.000228   \n",
       "27            4        150         0.0001           8  0.000233  0.000233   \n",
       "28            4        200         0.0001           8  0.000239  0.000239   \n",
       "29            4        200         0.0001           8  0.000254  0.000254   \n",
       "30            1        150         0.0010           8  0.000258  0.000258   \n",
       "31            4        200         0.0010           8  0.000281  0.000281   \n",
       "32            4        200         0.0001           8  0.000312  0.000312   \n",
       "33            3        200         0.0001           8  0.000316  0.000316   \n",
       "34            4        200         0.0001           8  0.000329  0.000329   \n",
       "35            4        150         0.0001           8  0.000336  0.000336   \n",
       "36            4        200         0.0001           8  0.000352  0.000352   \n",
       "37            4        200         0.0001           8  0.000353  0.000353   \n",
       "38            1        200         0.0010          16  0.000367  0.000367   \n",
       "39            4        150         0.0001           8  0.000412  0.000412   \n",
       "40            4        200         0.0001           8  0.000506  0.000506   \n",
       "41            4        200         0.0001          16  0.000518  0.000518   \n",
       "42            4        200         0.0001          16  0.000720  0.000720   \n",
       "43            4        200         0.0001          16  0.000914  0.000914   \n",
       "44            1        150         0.0001           8  0.001607  0.001607   \n",
       "45            4         50         0.0001          16  0.001641  0.001641   \n",
       "46            1        150         0.0001           8  0.001726  0.001726   \n",
       "47            2        200         0.0001           8  0.001905  0.001905   \n",
       "48            2        100         0.0001          16  0.003811  0.003811   \n",
       "49            4        200         0.0010           8  0.003947  0.003947   \n",
       "50            1        150         0.0001           8  0.004140  0.004140   \n",
       "\n",
       "    Elapsed time  \n",
       "0     421.005343  \n",
       "1     352.227582  \n",
       "2     357.309937  \n",
       "3     432.142172  \n",
       "4     345.442092  \n",
       "5     225.669814  \n",
       "6     264.313923  \n",
       "7     265.501271  \n",
       "8     548.526137  \n",
       "9     239.436815  \n",
       "10    373.236596  \n",
       "11    402.869851  \n",
       "12    503.115913  \n",
       "13    198.005508  \n",
       "14    429.715875  \n",
       "15    400.147505  \n",
       "16    295.546846  \n",
       "17    488.480635  \n",
       "18    234.262063  \n",
       "19    211.798278  \n",
       "20    380.579088  \n",
       "21    307.778145  \n",
       "22    362.718876  \n",
       "23    339.872833  \n",
       "24    301.072024  \n",
       "25    415.294774  \n",
       "26    183.786620  \n",
       "27    311.316117  \n",
       "28    316.592388  \n",
       "29    283.338394  \n",
       "30    329.132674  \n",
       "31    636.317267  \n",
       "32    548.005009  \n",
       "33    562.824315  \n",
       "34    253.672702  \n",
       "35    502.648646  \n",
       "36    434.566360  \n",
       "37    308.878368  \n",
       "38    150.082613  \n",
       "39    412.379099  \n",
       "40    258.823786  \n",
       "41    191.672038  \n",
       "42    292.936563  \n",
       "43    283.497283  \n",
       "44    391.682864  \n",
       "45    220.065698  \n",
       "46    265.441032  \n",
       "47    247.928394  \n",
       "48    205.651847  \n",
       "49    254.975026  \n",
       "50    372.129192  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_ecsdiff.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 288.574 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
