{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 16:43:44.904691: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-16 16:43:45.028242: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:45.028262: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-16 16:43:45.887056: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:45.887127: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:45.887134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,5e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.5         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 16:43:46.938998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-16 16:43:46.939282: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:46.939366: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:46.939421: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:46.939474: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:46.939526: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:46.939577: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:46.939627: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:46.939678: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:46.939687: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-16 16:43:46.939925: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0679 - mean_squared_error: 0.0679\n",
      "Loss: 0.06786428391933441 , Elapsed time: 143.06047916412354\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0577 - mean_squared_error: 0.0577\n",
      "Loss: 0.057705748826265335 , Elapsed time: 22.452398538589478\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0530 - mean_squared_error: 0.0530\n",
      "Loss: 0.052950408309698105 , Elapsed time: 26.733599424362183\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0756 - mean_squared_error: 0.0756\n",
      "Loss: 0.07558683305978775 , Elapsed time: 46.82421064376831\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Loss: 0.0358257032930851 , Elapsed time: 52.19677495956421\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax      \n",
      "0  \t5     \t0.0358257\t0.0579866\t0.0755868\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Loss: 0.03658382222056389 , Elapsed time: 63.73570108413696\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0649 - mean_squared_error: 0.0649\n",
      "Loss: 0.06492967158555984 , Elapsed time: 22.51244068145752\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0486 - mean_squared_error: 0.0486\n",
      "Loss: 0.04860348999500275 , Elapsed time: 24.705320596694946\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Loss: 0.03453017398715019 , Elapsed time: 119.22306299209595\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.0345302\t0.0440946\t0.0649297\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Loss: 0.03889775648713112 , Elapsed time: 19.793076038360596\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Loss: 0.037410248070955276 , Elapsed time: 115.57441592216492\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Loss: 0.03873357176780701 , Elapsed time: 56.35945725440979\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t3     \t0.0345302\t0.0370795\t0.0388978\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Loss: 0.03533736616373062 , Elapsed time: 118.36797976493835\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Loss: 0.03628211468458176 , Elapsed time: 83.17493343353271\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 1 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3714 - mean_squared_error: 0.3714\n",
      "Loss: 0.3714022934436798 , Elapsed time: 56.60303711891174\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Loss: 0.03552532568573952 , Elapsed time: 126.82304549217224\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t4     \t0.0345302\t0.102615 \t0.371402 \n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0408 - mean_squared_error: 0.0408\n",
      "Loss: 0.040830135345458984 , Elapsed time: 35.55097579956055\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Loss: 0.03566659241914749 , Elapsed time: 126.09017753601074\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Loss: 0.03585764020681381 , Elapsed time: 123.7982075214386\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t3     \t0.0345302\t0.0362829\t0.0408301\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Loss: 0.03125663101673126 , Elapsed time: 104.11381316184998\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Loss: 0.03139365836977959 , Elapsed time: 115.98263335227966\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Loss: 0.03626713156700134 , Elapsed time: 143.48165678977966\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t3     \t0.0312566\t0.0338228\t0.0362671\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Loss: 0.032503604888916016 , Elapsed time: 111.56908798217773\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Loss: 0.037150945514440536 , Elapsed time: 124.16360759735107\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0488 - mean_squared_error: 0.0488\n",
      "Loss: 0.04883132874965668 , Elapsed time: 128.3630862236023\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0547 - mean_squared_error: 0.0547\n",
      "Loss: 0.05469782277941704 , Elapsed time: 95.95190978050232\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t4     \t0.0312566\t0.0408881\t0.0546978\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Loss: 0.032288115471601486 , Elapsed time: 97.01867508888245\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Loss: 0.032259296625852585 , Elapsed time: 103.98507761955261\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Loss: 0.0319267176091671 , Elapsed time: 95.93560433387756\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t3     \t0.0312566\t0.0320469\t0.0325036\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0880 - mean_squared_error: 0.0880\n",
      "Loss: 0.0879889503121376 , Elapsed time: 28.43432879447937\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Loss: 0.03786096349358559 , Elapsed time: 96.52617907524109\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Loss: 0.03425077348947525 , Elapsed time: 108.5397138595581\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t3     \t0.0312566\t0.0447233\t0.087989 \n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Loss: 0.03244296461343765 , Elapsed time: 76.10416793823242\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Loss: 0.032966602593660355 , Elapsed time: 84.7047278881073\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Loss: 0.03202762082219124 , Elapsed time: 77.39819717407227\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0612 - mean_squared_error: 0.0612\n",
      "Loss: 0.061189472675323486 , Elapsed time: 75.588613986969\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t4     \t0.0312566\t0.0379767\t0.0611895\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Loss: 0.03210652619600296 , Elapsed time: 78.70832180976868\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Loss: 0.03185496851801872 , Elapsed time: 74.353600025177\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0999 - mean_squared_error: 0.0999\n",
      "Loss: 0.09989152103662491 , Elapsed time: 83.12963271141052\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t3     \t0.0312566\t0.0455105\t0.0998915\n",
      "-- Best Individual =  [0, 1, 0, 1, 0, 0, 0]\n",
      "-- Best Fitness =  0.03125663101673126\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 54.798545054594676 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 1 , Number of neurons: 100\n",
      "Batch size 4 , Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031257</td>\n",
       "      <td>0.031257</td>\n",
       "      <td>104.113813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031394</td>\n",
       "      <td>0.031394</td>\n",
       "      <td>115.982633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031855</td>\n",
       "      <td>0.031855</td>\n",
       "      <td>74.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031927</td>\n",
       "      <td>0.031927</td>\n",
       "      <td>95.935604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>77.398197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032107</td>\n",
       "      <td>0.032107</td>\n",
       "      <td>78.708322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032259</td>\n",
       "      <td>0.032259</td>\n",
       "      <td>103.985078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032288</td>\n",
       "      <td>0.032288</td>\n",
       "      <td>97.018675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032443</td>\n",
       "      <td>0.032443</td>\n",
       "      <td>76.104168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032504</td>\n",
       "      <td>0.032504</td>\n",
       "      <td>111.569088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032967</td>\n",
       "      <td>0.032967</td>\n",
       "      <td>84.704728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034251</td>\n",
       "      <td>0.034251</td>\n",
       "      <td>108.539714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034530</td>\n",
       "      <td>0.034530</td>\n",
       "      <td>119.223063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035337</td>\n",
       "      <td>0.035337</td>\n",
       "      <td>118.367980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035525</td>\n",
       "      <td>0.035525</td>\n",
       "      <td>126.823045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035667</td>\n",
       "      <td>0.035667</td>\n",
       "      <td>126.090178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035826</td>\n",
       "      <td>0.035826</td>\n",
       "      <td>52.196775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035858</td>\n",
       "      <td>0.035858</td>\n",
       "      <td>123.798208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.036267</td>\n",
       "      <td>0.036267</td>\n",
       "      <td>143.481657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036282</td>\n",
       "      <td>0.036282</td>\n",
       "      <td>83.174933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036584</td>\n",
       "      <td>0.036584</td>\n",
       "      <td>63.735701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.037151</td>\n",
       "      <td>0.037151</td>\n",
       "      <td>124.163608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.037410</td>\n",
       "      <td>0.037410</td>\n",
       "      <td>115.574416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.037861</td>\n",
       "      <td>0.037861</td>\n",
       "      <td>96.526179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038734</td>\n",
       "      <td>0.038734</td>\n",
       "      <td>56.359457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.038898</td>\n",
       "      <td>0.038898</td>\n",
       "      <td>19.793076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.040830</td>\n",
       "      <td>0.040830</td>\n",
       "      <td>35.550976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.048603</td>\n",
       "      <td>0.048603</td>\n",
       "      <td>24.705321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.048831</td>\n",
       "      <td>0.048831</td>\n",
       "      <td>128.363086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.052950</td>\n",
       "      <td>0.052950</td>\n",
       "      <td>26.733599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.054698</td>\n",
       "      <td>0.054698</td>\n",
       "      <td>95.951910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.057706</td>\n",
       "      <td>0.057706</td>\n",
       "      <td>22.452399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.061189</td>\n",
       "      <td>0.061189</td>\n",
       "      <td>75.588614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>22.512441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.067864</td>\n",
       "      <td>0.067864</td>\n",
       "      <td>143.060479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.075587</td>\n",
       "      <td>0.075587</td>\n",
       "      <td>46.824211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.087989</td>\n",
       "      <td>0.087989</td>\n",
       "      <td>28.434329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.099892</td>\n",
       "      <td>0.099892</td>\n",
       "      <td>83.129633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.371402</td>\n",
       "      <td>0.371402</td>\n",
       "      <td>56.603037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             2        100         0.0001           2  0.031257  0.031257   \n",
       "1             4         50         0.0001           2  0.031394  0.031394   \n",
       "2             2        100         0.0001           2  0.031855  0.031855   \n",
       "3             2        100         0.0001           2  0.031927  0.031927   \n",
       "4             2        100         0.0001           2  0.032028  0.032028   \n",
       "5             2        100         0.0001           2  0.032107  0.032107   \n",
       "6             2        100         0.0001           2  0.032259  0.032259   \n",
       "7             2        100         0.0001           2  0.032288  0.032288   \n",
       "8             2        100         0.0001           2  0.032443  0.032443   \n",
       "9             2        100         0.0001           2  0.032504  0.032504   \n",
       "10            3        100         0.0001           2  0.032967  0.032967   \n",
       "11            4        100         0.0001           2  0.034251  0.034251   \n",
       "12            4        100         0.0001           2  0.034530  0.034530   \n",
       "13            4        100         0.0001           2  0.035337  0.035337   \n",
       "14            4        100         0.0001           2  0.035525  0.035525   \n",
       "15            4        100         0.0001           2  0.035667  0.035667   \n",
       "16            2        100         0.0001           4  0.035826  0.035826   \n",
       "17            4        100         0.0001           2  0.035858  0.035858   \n",
       "18            4        100         0.0001           2  0.036267  0.036267   \n",
       "19            2        100         0.0001           4  0.036282  0.036282   \n",
       "20            2        200         0.0001           4  0.036584  0.036584   \n",
       "21            4        100         0.0001           2  0.037151  0.037151   \n",
       "22            4        100         0.0001           2  0.037410  0.037410   \n",
       "23            2        100         0.0001           2  0.037861  0.037861   \n",
       "24            2        100         0.0001           4  0.038734  0.038734   \n",
       "25            2        100         0.0050          16  0.038898  0.038898   \n",
       "26            2        100         0.0001           8  0.040830  0.040830   \n",
       "27            2        200         0.0001          16  0.048603  0.048603   \n",
       "28            4        100         0.0050           2  0.048831  0.048831   \n",
       "29            4        200         0.0050          16  0.052950  0.052950   \n",
       "30            1        100         0.0001           2  0.054698  0.054698   \n",
       "31            2        200         0.0001          16  0.057706  0.057706   \n",
       "32            1        100         0.0001           2  0.061189  0.061189   \n",
       "33            2        100         0.0001          16  0.064930  0.064930   \n",
       "34            3        100         0.0050           2  0.067864  0.067864   \n",
       "35            1        100         0.0001           4  0.075587  0.075587   \n",
       "36            1        200         0.0001           8  0.087989  0.087989   \n",
       "37            4        100         0.0050           2  0.099892  0.099892   \n",
       "38            1         50         0.0001           4  0.371402  0.371402   \n",
       "\n",
       "    Elapsed time  \n",
       "0     104.113813  \n",
       "1     115.982633  \n",
       "2      74.353600  \n",
       "3      95.935604  \n",
       "4      77.398197  \n",
       "5      78.708322  \n",
       "6     103.985078  \n",
       "7      97.018675  \n",
       "8      76.104168  \n",
       "9     111.569088  \n",
       "10     84.704728  \n",
       "11    108.539714  \n",
       "12    119.223063  \n",
       "13    118.367980  \n",
       "14    126.823045  \n",
       "15    126.090178  \n",
       "16     52.196775  \n",
       "17    123.798208  \n",
       "18    143.481657  \n",
       "19     83.174933  \n",
       "20     63.735701  \n",
       "21    124.163608  \n",
       "22    115.574416  \n",
       "23     96.526179  \n",
       "24     56.359457  \n",
       "25     19.793076  \n",
       "26     35.550976  \n",
       "27     24.705321  \n",
       "28    128.363086  \n",
       "29     26.733599  \n",
       "30     95.951910  \n",
       "31     22.452399  \n",
       "32     75.588614  \n",
       "33     22.512441  \n",
       "34    143.060479  \n",
       "35     46.824211  \n",
       "36     28.434329  \n",
       "37     83.129633  \n",
       "38     56.603037  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla1.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 54.794 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
