{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 18:54:47.887433: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-16 18:54:48.011208: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:48.011227: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-16 18:54:48.903270: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:48.903367: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:48.903378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,5e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.5         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 18:54:49.981896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-16 18:54:49.982087: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:49.982148: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:49.982211: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:49.982262: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:49.982308: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:49.982367: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:49.982421: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:49.982479: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:49.982486: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-16 18:54:49.982676: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1470 - mean_squared_error: 0.1470\n",
      "Loss: 0.14704643189907074 , Elapsed time: 116.83144164085388\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0512 - mean_squared_error: 0.0512\n",
      "Loss: 0.05121162161231041 , Elapsed time: 19.654233932495117\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0515 - mean_squared_error: 0.0515\n",
      "Loss: 0.05150337517261505 , Elapsed time: 42.30494976043701\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0828 - mean_squared_error: 0.0828\n",
      "Loss: 0.08280253410339355 , Elapsed time: 83.16278147697449\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Loss: 0.03735656291246414 , Elapsed time: 83.07009315490723\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax     \n",
      "0  \t5     \t0.0373566\t0.0739841\t0.147046\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0409 - mean_squared_error: 0.0409\n",
      "Loss: 0.040936198085546494 , Elapsed time: 26.304829597473145\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1831 - mean_squared_error: 0.1831\n",
      "Loss: 0.1830572634935379 , Elapsed time: 56.00268888473511\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0949 - mean_squared_error: 0.0949\n",
      "Loss: 0.09494080394506454 , Elapsed time: 22.116179704666138\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Loss: 0.034682098776102066 , Elapsed time: 66.31651425361633\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.0346821\t0.0781946\t0.183057\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Loss: 0.03562094643712044 , Elapsed time: 42.0069682598114\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0735 - mean_squared_error: 0.0735\n",
      "Loss: 0.0735398679971695 , Elapsed time: 83.3651213645935\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0723 - mean_squared_error: 0.0723\n",
      "Loss: 0.0723222941160202 , Elapsed time: 143.6028335094452\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t3     \t0.0346821\t0.0507044\t0.0735399\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0474 - mean_squared_error: 0.0474\n",
      "Loss: 0.04739008843898773 , Elapsed time: 44.717018127441406\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0935 - mean_squared_error: 0.0935\n",
      "Loss: 0.09350772947072983 , Elapsed time: 105.65029096603394\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Loss: 0.03039962612092495 , Elapsed time: 32.94785189628601\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Loss: 0.031387437134981155 , Elapsed time: 143.19119906425476\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t4     \t0.0303996\t0.0474734\t0.0935077\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0423 - mean_squared_error: 0.0423\n",
      "Loss: 0.04225583374500275 , Elapsed time: 21.461265563964844\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0571 - mean_squared_error: 0.0571\n",
      "Loss: 0.0571051724255085 , Elapsed time: 22.121341943740845\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0453 - mean_squared_error: 0.0453\n",
      "Loss: 0.04534508287906647 , Elapsed time: 142.94623398780823\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t3     \t0.0303996\t0.0419576\t0.0571052\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0601 - mean_squared_error: 0.0601\n",
      "Loss: 0.06011895835399628 , Elapsed time: 21.326356410980225\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Loss: 0.031824879348278046 , Elapsed time: 34.99046230316162\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0795 - mean_squared_error: 0.0795\n",
      "Loss: 0.07954556494951248 , Elapsed time: 68.5368800163269\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t3     \t0.0303996\t0.048829 \t0.0795456\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0483 - mean_squared_error: 0.0483\n",
      "Loss: 0.04828431084752083 , Elapsed time: 36.94820499420166\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Loss: 0.032177623361349106 , Elapsed time: 35.783910274505615\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t2     \t0.0303996\t0.0405611\t0.060119 \n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0606 - mean_squared_error: 0.0606\n",
      "Loss: 0.06058772653341293 , Elapsed time: 33.684762716293335\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Loss: 0.031100524589419365 , Elapsed time: 34.35664987564087\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0419 - mean_squared_error: 0.0419\n",
      "Loss: 0.041897911578416824 , Elapsed time: 21.035857915878296\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0839 - mean_squared_error: 0.0839\n",
      "Loss: 0.08391761779785156 , Elapsed time: 142.98022389411926\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.0303996\t0.0495807\t0.0839176\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0996 - mean_squared_error: 0.0996\n",
      "Loss: 0.09960757941007614 , Elapsed time: 35.46846389770508\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Loss: 0.03626199811697006 , Elapsed time: 82.24155735969543\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0414 - mean_squared_error: 0.0414\n",
      "Loss: 0.04140843451023102 , Elapsed time: 34.29656386375427\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0686 - mean_squared_error: 0.0686\n",
      "Loss: 0.06860850006341934 , Elapsed time: 37.43644142150879\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t4     \t0.0303996\t0.0552572\t0.0996076\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0414 - mean_squared_error: 0.0414\n",
      "Loss: 0.04144977778196335 , Elapsed time: 36.641578674316406\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Loss: 0.03534841910004616 , Elapsed time: 42.09113097190857\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t2     \t0.0303996\t0.0358012\t0.0414498\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0402 - mean_squared_error: 0.0402\n",
      "Loss: 0.040233511477708817 , Elapsed time: 34.649334192276\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0468 - mean_squared_error: 0.0468\n",
      "Loss: 0.04682765156030655 , Elapsed time: 35.59595847129822\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Loss: 0.0330062098801136 , Elapsed time: 35.94743371009827\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t3     \t0.0303996\t0.0361733\t0.0468277\n",
      "-- Best Individual =  [0, 0, 1, 1, 1, 1, 0]\n",
      "-- Best Fitness =  0.03039962612092495\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 35.034829998016356 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 1 , Number of neurons: 50\n",
      "Batch size 4 , Learning rate: 0.005\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>32.947852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.031101</td>\n",
       "      <td>0.031101</td>\n",
       "      <td>34.356650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031387</td>\n",
       "      <td>0.031387</td>\n",
       "      <td>143.191199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.031825</td>\n",
       "      <td>0.031825</td>\n",
       "      <td>34.990462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.032178</td>\n",
       "      <td>0.032178</td>\n",
       "      <td>35.783910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.033006</td>\n",
       "      <td>0.033006</td>\n",
       "      <td>35.947434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034682</td>\n",
       "      <td>0.034682</td>\n",
       "      <td>66.316514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.035348</td>\n",
       "      <td>0.035348</td>\n",
       "      <td>42.091131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.035621</td>\n",
       "      <td>0.035621</td>\n",
       "      <td>42.006968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036262</td>\n",
       "      <td>0.036262</td>\n",
       "      <td>82.241557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037357</td>\n",
       "      <td>0.037357</td>\n",
       "      <td>83.070093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.040234</td>\n",
       "      <td>0.040234</td>\n",
       "      <td>34.649334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.040936</td>\n",
       "      <td>0.040936</td>\n",
       "      <td>26.304830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.041408</td>\n",
       "      <td>0.041408</td>\n",
       "      <td>34.296564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.041450</td>\n",
       "      <td>0.041450</td>\n",
       "      <td>36.641579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.041898</td>\n",
       "      <td>0.041898</td>\n",
       "      <td>21.035858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.042256</td>\n",
       "      <td>0.042256</td>\n",
       "      <td>21.461266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.045345</td>\n",
       "      <td>0.045345</td>\n",
       "      <td>142.946234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.046828</td>\n",
       "      <td>0.046828</td>\n",
       "      <td>35.595958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.047390</td>\n",
       "      <td>0.047390</td>\n",
       "      <td>44.717018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.048284</td>\n",
       "      <td>0.048284</td>\n",
       "      <td>36.948205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.051212</td>\n",
       "      <td>0.051212</td>\n",
       "      <td>19.654234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.051503</td>\n",
       "      <td>0.051503</td>\n",
       "      <td>42.304950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.057105</td>\n",
       "      <td>0.057105</td>\n",
       "      <td>22.121342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.060119</td>\n",
       "      <td>0.060119</td>\n",
       "      <td>21.326356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.060588</td>\n",
       "      <td>0.060588</td>\n",
       "      <td>33.684763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.068609</td>\n",
       "      <td>0.068609</td>\n",
       "      <td>37.436441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.072322</td>\n",
       "      <td>0.072322</td>\n",
       "      <td>143.602834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.073540</td>\n",
       "      <td>0.073540</td>\n",
       "      <td>83.365121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.079546</td>\n",
       "      <td>0.079546</td>\n",
       "      <td>68.536880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.082803</td>\n",
       "      <td>0.082803</td>\n",
       "      <td>83.162781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.083918</td>\n",
       "      <td>0.083918</td>\n",
       "      <td>142.980224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.093508</td>\n",
       "      <td>0.093508</td>\n",
       "      <td>105.650291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.094941</td>\n",
       "      <td>0.094941</td>\n",
       "      <td>22.116180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.099608</td>\n",
       "      <td>0.099608</td>\n",
       "      <td>35.468464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.147046</td>\n",
       "      <td>0.147046</td>\n",
       "      <td>116.831442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.183057</td>\n",
       "      <td>0.183057</td>\n",
       "      <td>56.002689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             1        200         0.0050           8  0.030400  0.030400   \n",
       "1             1        200         0.0050           8  0.031101  0.031101   \n",
       "2             2        200         0.0001           2  0.031387  0.031387   \n",
       "3             1        150         0.0050           8  0.031825  0.031825   \n",
       "4             1        150         0.0050           8  0.032178  0.032178   \n",
       "5             1        150         0.0050           8  0.033006  0.033006   \n",
       "6             2        200         0.0001           4  0.034682  0.034682   \n",
       "7             1        200         0.0050           8  0.035348  0.035348   \n",
       "8             2        200         0.0050           8  0.035621  0.035621   \n",
       "9             3        200         0.0050           4  0.036262  0.036262   \n",
       "10            2        100         0.0001           4  0.037357  0.037357   \n",
       "11            1        200         0.0050           8  0.040234  0.040234   \n",
       "12            2        200         0.0050          16  0.040936  0.040936   \n",
       "13            1        200         0.0050           8  0.041408  0.041408   \n",
       "14            1        150         0.0050           8  0.041450  0.041450   \n",
       "15            1        200         0.0050          16  0.041898  0.041898   \n",
       "16            1        100         0.0050          16  0.042256  0.042256   \n",
       "17            1        200         0.0001           2  0.045345  0.045345   \n",
       "18            1        200         0.0050           8  0.046828  0.046828   \n",
       "19            2        200         0.0050           8  0.047390  0.047390   \n",
       "20            1        200         0.0050           8  0.048284  0.048284   \n",
       "21            2        200         0.0001          16  0.051212  0.051212   \n",
       "22            4        200         0.0050          16  0.051503  0.051503   \n",
       "23            2        200         0.0001          16  0.057105  0.057105   \n",
       "24            1        200         0.0050          16  0.060119  0.060119   \n",
       "25            1        150         0.0050           8  0.060588  0.060588   \n",
       "26            1        200         0.0050           8  0.068609  0.068609   \n",
       "27            4        150         0.0050           2  0.072322  0.072322   \n",
       "28            1        100         0.0001           4  0.073540  0.073540   \n",
       "29            2        200         0.0050           4  0.079546  0.079546   \n",
       "30            1        100         0.0001           4  0.082803  0.082803   \n",
       "31            1        200         0.0050           2  0.083918  0.083918   \n",
       "32            2        100         0.0050           2  0.093508  0.093508   \n",
       "33            2        100         0.0001          16  0.094941  0.094941   \n",
       "34            1        200         0.0001           8  0.099608  0.099608   \n",
       "35            3        100         0.0050           2  0.147046  0.147046   \n",
       "36            1        100         0.0001           4  0.183057  0.183057   \n",
       "\n",
       "    Elapsed time  \n",
       "0      32.947852  \n",
       "1      34.356650  \n",
       "2     143.191199  \n",
       "3      34.990462  \n",
       "4      35.783910  \n",
       "5      35.947434  \n",
       "6      66.316514  \n",
       "7      42.091131  \n",
       "8      42.006968  \n",
       "9      82.241557  \n",
       "10     83.070093  \n",
       "11     34.649334  \n",
       "12     26.304830  \n",
       "13     34.296564  \n",
       "14     36.641579  \n",
       "15     21.035858  \n",
       "16     21.461266  \n",
       "17    142.946234  \n",
       "18     35.595958  \n",
       "19     44.717018  \n",
       "20     36.948205  \n",
       "21     19.654234  \n",
       "22     42.304950  \n",
       "23     22.121342  \n",
       "24     21.326356  \n",
       "25     33.684763  \n",
       "26     37.436441  \n",
       "27    143.602834  \n",
       "28     83.365121  \n",
       "29     68.536880  \n",
       "30     83.162781  \n",
       "31    142.980224  \n",
       "32    105.650291  \n",
       "33     22.116180  \n",
       "34     35.468464  \n",
       "35    116.831442  \n",
       "36     56.002689  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla1.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 35.030 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
