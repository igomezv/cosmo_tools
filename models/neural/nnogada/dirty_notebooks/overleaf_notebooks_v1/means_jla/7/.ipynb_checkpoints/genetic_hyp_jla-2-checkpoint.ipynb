{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 20:41:01.464171: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-16 20:41:01.645859: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:41:01.645901: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-16 20:41:02.876465: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:41:02.876623: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:41:02.876638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,5e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.8         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 20:41:04.258388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-16 20:41:04.258718: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:41:04.258821: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:41:04.258901: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:41:04.259009: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:41:04.259086: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:41:04.259163: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:41:04.259235: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:41:04.259312: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-16 20:41:04.259326: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-16 20:41:04.260435: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1673 - mean_squared_error: 0.1673\n",
      "Loss: 0.167262464761734 , Elapsed time: 203.63684105873108\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0524 - mean_squared_error: 0.0524\n",
      "Loss: 0.05243479087948799 , Elapsed time: 42.20518255233765\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Loss: 0.03211690112948418 , Elapsed time: 42.45891356468201\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0741 - mean_squared_error: 0.0741\n",
      "Loss: 0.07409936189651489 , Elapsed time: 67.30302023887634\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Loss: 0.037674322724342346 , Elapsed time: 81.6662266254425\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax     \n",
      "0  \t5     \t0.0321169\t0.0727176\t0.167262\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Loss: 0.03886004909873009 , Elapsed time: 42.37861609458923\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0456 - mean_squared_error: 0.0456\n",
      "Loss: 0.04562770202755928 , Elapsed time: 42.525039196014404\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Loss: 0.03572890907526016 , Elapsed time: 83.24502992630005\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Loss: 0.03812139481306076 , Elapsed time: 73.5003433227539\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.0321169\t0.038091 \t0.0456277\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0499 - mean_squared_error: 0.0499\n",
      "Loss: 0.04985319823026657 , Elapsed time: 38.96246027946472\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0393 - mean_squared_error: 0.0393\n",
      "Loss: 0.0393332913517952 , Elapsed time: 42.615108489990234\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Loss: 0.03808402270078659 , Elapsed time: 83.22020506858826\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0611 - mean_squared_error: 0.0611\n",
      "Loss: 0.0611078217625618 , Elapsed time: 35.13384747505188\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t4     \t0.0321169\t0.044099 \t0.0611078\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0586 - mean_squared_error: 0.0586\n",
      "Loss: 0.05863169580698013 , Elapsed time: 42.44841551780701\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Loss: 0.03488121181726456 , Elapsed time: 78.21037077903748\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0502 - mean_squared_error: 0.0502\n",
      "Loss: 0.050233397632837296 , Elapsed time: 29.967852115631104\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0572 - mean_squared_error: 0.0572\n",
      "Loss: 0.05715208500623703 , Elapsed time: 34.99561262130737\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t4     \t0.0321169\t0.0466031\t0.0586317\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0439 - mean_squared_error: 0.0439\n",
      "Loss: 0.04390943795442581 , Elapsed time: 41.53292179107666\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Loss: 0.03507593646645546 , Elapsed time: 143.7970130443573\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0453 - mean_squared_error: 0.0453\n",
      "Loss: 0.045268092304468155 , Elapsed time: 42.53217625617981\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0474 - mean_squared_error: 0.0474\n",
      "Loss: 0.04736146330833435 , Elapsed time: 102.24872064590454\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t4     \t0.0321169\t0.0407464\t0.0473615\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0546 - mean_squared_error: 0.0546\n",
      "Loss: 0.054610613733530045 , Elapsed time: 39.12802004814148\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Loss: 0.0330321379005909 , Elapsed time: 108.6620728969574\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0778 - mean_squared_error: 0.0778\n",
      "Loss: 0.07783467322587967 , Elapsed time: 43.46394991874695\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t3     \t0.0321169\t0.0459422\t0.0778347\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Loss: 0.03849499672651291 , Elapsed time: 36.953242778778076\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0656 - mean_squared_error: 0.0656\n",
      "Loss: 0.06559383869171143 , Elapsed time: 143.42980527877808\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Loss: 0.03314190357923508 , Elapsed time: 143.779305934906\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0710 - mean_squared_error: 0.0710\n",
      "Loss: 0.07102902233600616 , Elapsed time: 42.60757350921631\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t4     \t0.0321169\t0.0480753\t0.071029 \n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0851 - mean_squared_error: 0.0851\n",
      "Loss: 0.08511833846569061 , Elapsed time: 143.3116466999054\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0528 - mean_squared_error: 0.0528\n",
      "Loss: 0.052765484899282455 , Elapsed time: 42.079655170440674\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 1s 3ms/step - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Loss: 0.03383679315447807 , Elapsed time: 144.3974540233612\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1010 - mean_squared_error: 0.1010\n",
      "Loss: 0.10100317001342773 , Elapsed time: 35.136449098587036\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.0321169\t0.0609681\t0.101003 \n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0665 - mean_squared_error: 0.0665\n",
      "Loss: 0.06652222573757172 , Elapsed time: 82.45798921585083\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0532 - mean_squared_error: 0.0532\n",
      "Loss: 0.05316604673862457 , Elapsed time: 34.65398335456848\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Loss: 0.03609954193234444 , Elapsed time: 121.48196792602539\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1506 - mean_squared_error: 0.1506\n",
      "Loss: 0.15062102675437927 , Elapsed time: 188.74604725837708\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t4     \t0.0321169\t0.0677051\t0.150621 \n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0682 - mean_squared_error: 0.0682\n",
      "Loss: 0.06817720085382462 , Elapsed time: 83.54658031463623\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0642 - mean_squared_error: 0.0642\n",
      "Loss: 0.06424243748188019 , Elapsed time: 43.2689208984375\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Loss: 0.03369096666574478 , Elapsed time: 143.8767535686493\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Loss: 0.03197594732046127 , Elapsed time: 144.18769526481628\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t4     \t0.0319759\t0.0460407\t0.0681772\n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Loss: 0.03229418769478798 , Elapsed time: 40.00685930252075\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 42 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0485 - mean_squared_error: 0.0485\n",
      "Loss: 0.04853101819753647 , Elapsed time: 42.864362955093384\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 43 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0470 - mean_squared_error: 0.0470\n",
      "Loss: 0.047004684805870056 , Elapsed time: 42.692293882369995\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 44 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Loss: 0.03726731985807419 , Elapsed time: 66.73775053024292\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t4     \t0.0319759\t0.0394146\t0.048531 \n",
      "-- Best Individual =  [1, 1, 1, 0, 0, 0, 1]\n",
      "-- Best Fitness =  0.03197594732046127\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 55.87273411750793 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 2 , Number of neurons: 100\n",
      "Batch size 2 , Learning rate: 0.005\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031976</td>\n",
       "      <td>0.031976</td>\n",
       "      <td>144.187695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.032117</td>\n",
       "      <td>0.032117</td>\n",
       "      <td>42.458914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.032294</td>\n",
       "      <td>0.032294</td>\n",
       "      <td>40.006859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033032</td>\n",
       "      <td>0.033032</td>\n",
       "      <td>108.662073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033142</td>\n",
       "      <td>0.033142</td>\n",
       "      <td>143.779306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033691</td>\n",
       "      <td>0.033691</td>\n",
       "      <td>143.876754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033837</td>\n",
       "      <td>0.033837</td>\n",
       "      <td>144.397454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034881</td>\n",
       "      <td>0.034881</td>\n",
       "      <td>78.210371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035076</td>\n",
       "      <td>0.035076</td>\n",
       "      <td>143.797013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035729</td>\n",
       "      <td>0.035729</td>\n",
       "      <td>83.245030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>121.481968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037267</td>\n",
       "      <td>0.037267</td>\n",
       "      <td>66.737751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037674</td>\n",
       "      <td>0.037674</td>\n",
       "      <td>81.666227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038084</td>\n",
       "      <td>0.038084</td>\n",
       "      <td>83.220205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038121</td>\n",
       "      <td>0.038121</td>\n",
       "      <td>73.500343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.038495</td>\n",
       "      <td>0.038495</td>\n",
       "      <td>36.953243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>42.378616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.039333</td>\n",
       "      <td>0.039333</td>\n",
       "      <td>42.615108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.043909</td>\n",
       "      <td>0.043909</td>\n",
       "      <td>41.532922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.045268</td>\n",
       "      <td>0.045268</td>\n",
       "      <td>42.532176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.045628</td>\n",
       "      <td>0.045628</td>\n",
       "      <td>42.525039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.047005</td>\n",
       "      <td>0.047005</td>\n",
       "      <td>42.692294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.047361</td>\n",
       "      <td>0.047361</td>\n",
       "      <td>102.248721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.048531</td>\n",
       "      <td>0.048531</td>\n",
       "      <td>42.864363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.049853</td>\n",
       "      <td>0.049853</td>\n",
       "      <td>38.962460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.050233</td>\n",
       "      <td>0.050233</td>\n",
       "      <td>29.967852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.052435</td>\n",
       "      <td>0.052435</td>\n",
       "      <td>42.205183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.052765</td>\n",
       "      <td>0.052765</td>\n",
       "      <td>42.079655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.053166</td>\n",
       "      <td>0.053166</td>\n",
       "      <td>34.653983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.054611</td>\n",
       "      <td>0.054611</td>\n",
       "      <td>39.128020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.057152</td>\n",
       "      <td>0.057152</td>\n",
       "      <td>34.995613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.058632</td>\n",
       "      <td>0.058632</td>\n",
       "      <td>42.448416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.061108</td>\n",
       "      <td>0.061108</td>\n",
       "      <td>35.133847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.064242</td>\n",
       "      <td>0.064242</td>\n",
       "      <td>43.268921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>143.429805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.066522</td>\n",
       "      <td>0.066522</td>\n",
       "      <td>82.457989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.068177</td>\n",
       "      <td>0.068177</td>\n",
       "      <td>83.546580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.071029</td>\n",
       "      <td>0.071029</td>\n",
       "      <td>42.607574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.074099</td>\n",
       "      <td>0.074099</td>\n",
       "      <td>67.303020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.077835</td>\n",
       "      <td>0.077835</td>\n",
       "      <td>43.463950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.085118</td>\n",
       "      <td>0.085118</td>\n",
       "      <td>143.311647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.101003</td>\n",
       "      <td>0.101003</td>\n",
       "      <td>35.136449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.150621</td>\n",
       "      <td>0.150621</td>\n",
       "      <td>188.746047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.167262</td>\n",
       "      <td>0.167262</td>\n",
       "      <td>203.636841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             4        150         0.0001           4  0.031976  0.031976   \n",
       "1             4        200         0.0050          16  0.032117  0.032117   \n",
       "2             4        200         0.0001          16  0.032294  0.032294   \n",
       "3             4        200         0.0001           4  0.033032  0.033032   \n",
       "4             4        200         0.0001           4  0.033142  0.033142   \n",
       "5             3        200         0.0001           4  0.033691  0.033691   \n",
       "6             4        200         0.0001           4  0.033837  0.033837   \n",
       "7             2        100         0.0001           4  0.034881  0.034881   \n",
       "8             4        100         0.0001           4  0.035076  0.035076   \n",
       "9             2        100         0.0001           4  0.035729  0.035729   \n",
       "10            4        200         0.0001           4  0.036100  0.036100   \n",
       "11            2        150         0.0001           4  0.037267  0.037267   \n",
       "12            2        100         0.0001           4  0.037674  0.037674   \n",
       "13            2        100         0.0001           4  0.038084  0.038084   \n",
       "14            2        100         0.0001           4  0.038121  0.038121   \n",
       "15            4        200         0.0001          16  0.038495  0.038495   \n",
       "16            3        200         0.0001          16  0.038860  0.038860   \n",
       "17            4        200         0.0050          16  0.039333  0.039333   \n",
       "18            4        200         0.0050          16  0.043909  0.043909   \n",
       "19            4        200         0.0050          16  0.045268  0.045268   \n",
       "20            4        200         0.0050          16  0.045628  0.045628   \n",
       "21            4        200         0.0050          16  0.047005  0.047005   \n",
       "22            4        200         0.0050           4  0.047361  0.047361   \n",
       "23            4        200         0.0050          16  0.048531  0.048531   \n",
       "24            4        200         0.0050          16  0.049853  0.049853   \n",
       "25            4        200         0.0050          16  0.050233  0.050233   \n",
       "26            2        200         0.0001          16  0.052435  0.052435   \n",
       "27            2        150         0.0001          16  0.052765  0.052765   \n",
       "28            4        100         0.0050          16  0.053166  0.053166   \n",
       "29            4        200         0.0050          16  0.054611  0.054611   \n",
       "30            4        200         0.0050          16  0.057152  0.057152   \n",
       "31            4        100         0.0050          16  0.058632  0.058632   \n",
       "32            4        150         0.0050          16  0.061108  0.061108   \n",
       "33            4        100         0.0050          16  0.064242  0.064242   \n",
       "34            2        200         0.0050           4  0.065594  0.065594   \n",
       "35            2         50         0.0050           4  0.066522  0.066522   \n",
       "36            4        200         0.0050           8  0.068177  0.068177   \n",
       "37            4        150         0.0050          16  0.071029  0.071029   \n",
       "38            1        100         0.0001           4  0.074099  0.074099   \n",
       "39            4        200         0.0050          16  0.077835  0.077835   \n",
       "40            2        200         0.0050           4  0.085118  0.085118   \n",
       "41            4         50         0.0050          16  0.101003  0.101003   \n",
       "42            3        200         0.0050           2  0.150621  0.150621   \n",
       "43            3        100         0.0050           2  0.167262  0.167262   \n",
       "\n",
       "    Elapsed time  \n",
       "0     144.187695  \n",
       "1      42.458914  \n",
       "2      40.006859  \n",
       "3     108.662073  \n",
       "4     143.779306  \n",
       "5     143.876754  \n",
       "6     144.397454  \n",
       "7      78.210371  \n",
       "8     143.797013  \n",
       "9      83.245030  \n",
       "10    121.481968  \n",
       "11     66.737751  \n",
       "12     81.666227  \n",
       "13     83.220205  \n",
       "14     73.500343  \n",
       "15     36.953243  \n",
       "16     42.378616  \n",
       "17     42.615108  \n",
       "18     41.532922  \n",
       "19     42.532176  \n",
       "20     42.525039  \n",
       "21     42.692294  \n",
       "22    102.248721  \n",
       "23     42.864363  \n",
       "24     38.962460  \n",
       "25     29.967852  \n",
       "26     42.205183  \n",
       "27     42.079655  \n",
       "28     34.653983  \n",
       "29     39.128020  \n",
       "30     34.995613  \n",
       "31     42.448416  \n",
       "32     35.133847  \n",
       "33     43.268921  \n",
       "34    143.429805  \n",
       "35     82.457989  \n",
       "36     83.546580  \n",
       "37     42.607574  \n",
       "38     67.303020  \n",
       "39     43.463950  \n",
       "40    143.311647  \n",
       "41     35.136449  \n",
       "42    188.746047  \n",
       "43    203.636841  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla2.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 55.868 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
