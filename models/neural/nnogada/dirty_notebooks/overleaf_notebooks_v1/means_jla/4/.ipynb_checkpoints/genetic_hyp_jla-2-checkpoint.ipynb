{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 16:43:54.181594: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-16 16:43:54.321955: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:54.321973: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-16 16:43:55.337967: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:55.338113: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:55.338125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,5e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.8         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 16:43:56.495616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-16 16:43:56.495874: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:56.495957: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:56.496030: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:56.496106: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:56.496181: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:56.496248: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:56.496306: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:56.496375: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-16 16:43:56.496388: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-16 16:43:56.497444: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0913 - mean_squared_error: 0.0913\n",
      "Loss: 0.0912751778960228 , Elapsed time: 113.24654054641724\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0567 - mean_squared_error: 0.0567\n",
      "Loss: 0.05669412016868591 , Elapsed time: 22.349292755126953\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0472 - mean_squared_error: 0.0472\n",
      "Loss: 0.04723990708589554 , Elapsed time: 42.4021999835968\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1927 - mean_squared_error: 0.1927\n",
      "Loss: 0.19273295998573303 , Elapsed time: 47.44456434249878\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Loss: 0.03868740051984787 , Elapsed time: 52.32404589653015\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax     \n",
      "0  \t5     \t0.0386874\t0.0853259\t0.192733\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Loss: 0.03572923317551613 , Elapsed time: 22.49745011329651\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Loss: 0.03446820005774498 , Elapsed time: 59.276809215545654\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0488 - mean_squared_error: 0.0488\n",
      "Loss: 0.04879177361726761 , Elapsed time: 29.3854079246521\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t3     \t0.0344682\t0.0497904\t0.0912752\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Loss: 0.03603595495223999 , Elapsed time: 59.43494009971619\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0545 - mean_squared_error: 0.0545\n",
      "Loss: 0.05446227639913559 , Elapsed time: 23.3591890335083\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0698 - mean_squared_error: 0.0698\n",
      "Loss: 0.06975428014993668 , Elapsed time: 25.380202293395996\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0773 - mean_squared_error: 0.0773\n",
      "Loss: 0.07726841419935226 , Elapsed time: 52.652085065841675\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t4     \t0.0344682\t0.0543978\t0.0772684\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Loss: 0.03513601794838905 , Elapsed time: 21.91679620742798\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0785 - mean_squared_error: 0.0785\n",
      "Loss: 0.0785117968916893 , Elapsed time: 54.8149516582489\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t2     \t0.0344682\t0.0514081\t0.0785118\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Loss: 0.03639987111091614 , Elapsed time: 63.4497344493866\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0763 - mean_squared_error: 0.0763\n",
      "Loss: 0.0763142928481102 , Elapsed time: 20.058188438415527\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0585 - mean_squared_error: 0.0585\n",
      "Loss: 0.05853571742773056 , Elapsed time: 21.470094680786133\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t3     \t0.0344682\t0.0520361\t0.0763143\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Loss: 0.03175947442650795 , Elapsed time: 84.20396542549133\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Loss: 0.03857024386525154 , Elapsed time: 83.3154628276825\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Loss: 0.03536459058523178 , Elapsed time: 65.94522166252136\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0498 - mean_squared_error: 0.0498\n",
      "Loss: 0.04977837949991226 , Elapsed time: 83.84675168991089\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.0317595\t0.0379882\t0.0497784\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0596 - mean_squared_error: 0.0596\n",
      "Loss: 0.05958932638168335 , Elapsed time: 67.58829808235168\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Loss: 0.03690316155552864 , Elapsed time: 60.07032108306885\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0454 - mean_squared_error: 0.0454\n",
      "Loss: 0.045408107340335846 , Elapsed time: 83.4826488494873\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Loss: 0.035003721714019775 , Elapsed time: 70.70940256118774\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t4     \t0.0317595\t0.0417328\t0.0595893\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Loss: 0.03007643297314644 , Elapsed time: 83.6533727645874\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Loss: 0.03639769181609154 , Elapsed time: 66.87145590782166\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t2     \t0.0300764\t0.0336482\t0.0363977\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Loss: 0.03590410202741623 , Elapsed time: 64.78009390830994\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0503 - mean_squared_error: 0.0503\n",
      "Loss: 0.05027676001191139 , Elapsed time: 82.95840859413147\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0621 - mean_squared_error: 0.0621\n",
      "Loss: 0.062095481902360916 , Elapsed time: 19.909109354019165\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Loss: 0.03311270475387573 , Elapsed time: 78.35475897789001\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t4     \t0.0300764\t0.0422931\t0.0620955\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Loss: 0.031089195981621742 , Elapsed time: 84.32959365844727\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Loss: 0.02981656603515148 , Elapsed time: 83.67005181312561\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Loss: 0.04632432013750076 , Elapsed time: 59.540165424346924\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Loss: 0.033142250031232834 , Elapsed time: 81.28092527389526\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t4     \t0.0298166\t0.0340898\t0.0463243\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0735 - mean_squared_error: 0.0735\n",
      "Loss: 0.07347477227449417 , Elapsed time: 83.259024143219\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0466 - mean_squared_error: 0.0466\n",
      "Loss: 0.04662860557436943 , Elapsed time: 83.31589078903198\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1099 - mean_squared_error: 0.1099\n",
      "Loss: 0.10985833406448364 , Elapsed time: 142.92509841918945\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Loss: 0.037013281136751175 , Elapsed time: 61.521015882492065\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t4     \t0.0298166\t0.0593583\t0.109858 \n",
      "-- Best Individual =  [1, 0, 1, 1, 0, 0, 1]\n",
      "-- Best Fitness =  0.02981656603515148\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 40.13123496373495 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 2 , Number of neurons: 50\n",
      "Batch size 4 , Learning rate: 0.005\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.029817</td>\n",
       "      <td>0.029817</td>\n",
       "      <td>83.670052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030076</td>\n",
       "      <td>0.030076</td>\n",
       "      <td>83.653373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031089</td>\n",
       "      <td>0.031089</td>\n",
       "      <td>84.329594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031759</td>\n",
       "      <td>0.031759</td>\n",
       "      <td>84.203965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033113</td>\n",
       "      <td>0.033113</td>\n",
       "      <td>78.354759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033142</td>\n",
       "      <td>0.033142</td>\n",
       "      <td>81.280925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034468</td>\n",
       "      <td>0.034468</td>\n",
       "      <td>59.276809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035004</td>\n",
       "      <td>0.035004</td>\n",
       "      <td>70.709403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.035136</td>\n",
       "      <td>0.035136</td>\n",
       "      <td>21.916796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035365</td>\n",
       "      <td>0.035365</td>\n",
       "      <td>65.945222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.035729</td>\n",
       "      <td>0.035729</td>\n",
       "      <td>22.497450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035904</td>\n",
       "      <td>0.035904</td>\n",
       "      <td>64.780094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036036</td>\n",
       "      <td>0.036036</td>\n",
       "      <td>59.434940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036398</td>\n",
       "      <td>0.036398</td>\n",
       "      <td>66.871456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>63.449734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036903</td>\n",
       "      <td>0.036903</td>\n",
       "      <td>60.070321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037013</td>\n",
       "      <td>0.037013</td>\n",
       "      <td>61.521016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038570</td>\n",
       "      <td>0.038570</td>\n",
       "      <td>83.315463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038687</td>\n",
       "      <td>0.038687</td>\n",
       "      <td>52.324046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.045408</td>\n",
       "      <td>0.045408</td>\n",
       "      <td>83.482649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.046324</td>\n",
       "      <td>0.046324</td>\n",
       "      <td>59.540165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.046629</td>\n",
       "      <td>0.046629</td>\n",
       "      <td>83.315891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.047240</td>\n",
       "      <td>0.047240</td>\n",
       "      <td>42.402200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.048792</td>\n",
       "      <td>0.048792</td>\n",
       "      <td>29.385408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.049778</td>\n",
       "      <td>0.049778</td>\n",
       "      <td>83.846752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.050277</td>\n",
       "      <td>0.050277</td>\n",
       "      <td>82.958409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.054462</td>\n",
       "      <td>0.054462</td>\n",
       "      <td>23.359189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.056694</td>\n",
       "      <td>0.056694</td>\n",
       "      <td>22.349293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.058536</td>\n",
       "      <td>0.058536</td>\n",
       "      <td>21.470095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.059589</td>\n",
       "      <td>0.059589</td>\n",
       "      <td>67.588298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.062095</td>\n",
       "      <td>0.062095</td>\n",
       "      <td>19.909109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.069754</td>\n",
       "      <td>0.069754</td>\n",
       "      <td>25.380202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.073475</td>\n",
       "      <td>0.073475</td>\n",
       "      <td>83.259024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.076314</td>\n",
       "      <td>0.076314</td>\n",
       "      <td>20.058188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.077268</td>\n",
       "      <td>0.077268</td>\n",
       "      <td>52.652085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.078512</td>\n",
       "      <td>0.078512</td>\n",
       "      <td>54.814952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.091275</td>\n",
       "      <td>0.091275</td>\n",
       "      <td>113.246541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.109858</td>\n",
       "      <td>0.109858</td>\n",
       "      <td>142.925098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.192733</td>\n",
       "      <td>0.192733</td>\n",
       "      <td>47.444564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             3        200         0.0001           4  0.029817  0.029817   \n",
       "1             3        100         0.0050           4  0.030076  0.030076   \n",
       "2             4        100         0.0050           4  0.031089  0.031089   \n",
       "3             4        100         0.0050           4  0.031759  0.031759   \n",
       "4             4        200         0.0050           4  0.033113  0.033113   \n",
       "5             3        200         0.0050           4  0.033142  0.033142   \n",
       "6             2        100         0.0001           4  0.034468  0.034468   \n",
       "7             4        100         0.0001           4  0.035004  0.035004   \n",
       "8             4        100         0.0001          16  0.035136  0.035136   \n",
       "9             2        200         0.0001           4  0.035365  0.035365   \n",
       "10            4        100         0.0050          16  0.035729  0.035729   \n",
       "11            3        100         0.0050           4  0.035904  0.035904   \n",
       "12            2        100         0.0001           4  0.036036  0.036036   \n",
       "13            4        100         0.0001           4  0.036398  0.036398   \n",
       "14            4        100         0.0050           4  0.036400  0.036400   \n",
       "15            2        100         0.0001           4  0.036903  0.036903   \n",
       "16            3        100         0.0050           4  0.037013  0.037013   \n",
       "17            2         50         0.0001           4  0.038570  0.038570   \n",
       "18            2        100         0.0001           4  0.038687  0.038687   \n",
       "19            4        100         0.0050           4  0.045408  0.045408   \n",
       "20            2        100         0.0050           4  0.046324  0.046324   \n",
       "21            3        200         0.0050           4  0.046629  0.046629   \n",
       "22            4        200         0.0050          16  0.047240  0.047240   \n",
       "23            4        200         0.0050          16  0.048792  0.048792   \n",
       "24            4        200         0.0050           4  0.049778  0.049778   \n",
       "25            1        100         0.0050           4  0.050277  0.050277   \n",
       "26            4        100         0.0050          16  0.054462  0.054462   \n",
       "27            2        200         0.0001          16  0.056694  0.056694   \n",
       "28            4        100         0.0050          16  0.058536  0.058536   \n",
       "29            4        100         0.0050           4  0.059589  0.059589   \n",
       "30            2        100         0.0001          16  0.062095  0.062095   \n",
       "31            4        150         0.0050          16  0.069754  0.069754   \n",
       "32            2        100         0.0050           4  0.073475  0.073475   \n",
       "33            2        100         0.0001          16  0.076314  0.076314   \n",
       "34            2         50         0.0050           4  0.077268  0.077268   \n",
       "35            2        150         0.0050           4  0.078512  0.078512   \n",
       "36            3        100         0.0050           2  0.091275  0.091275   \n",
       "37            1        200         0.0050           2  0.109858  0.109858   \n",
       "38            1        100         0.0001           4  0.192733  0.192733   \n",
       "\n",
       "    Elapsed time  \n",
       "0      83.670052  \n",
       "1      83.653373  \n",
       "2      84.329594  \n",
       "3      84.203965  \n",
       "4      78.354759  \n",
       "5      81.280925  \n",
       "6      59.276809  \n",
       "7      70.709403  \n",
       "8      21.916796  \n",
       "9      65.945222  \n",
       "10     22.497450  \n",
       "11     64.780094  \n",
       "12     59.434940  \n",
       "13     66.871456  \n",
       "14     63.449734  \n",
       "15     60.070321  \n",
       "16     61.521016  \n",
       "17     83.315463  \n",
       "18     52.324046  \n",
       "19     83.482649  \n",
       "20     59.540165  \n",
       "21     83.315891  \n",
       "22     42.402200  \n",
       "23     29.385408  \n",
       "24     83.846752  \n",
       "25     82.958409  \n",
       "26     23.359189  \n",
       "27     22.349293  \n",
       "28     21.470095  \n",
       "29     67.588298  \n",
       "30     19.909109  \n",
       "31     25.380202  \n",
       "32     83.259024  \n",
       "33     20.058188  \n",
       "34     52.652085  \n",
       "35     54.814952  \n",
       "36    113.246541  \n",
       "37    142.925098  \n",
       "38     47.444564  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla2.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 40.117 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
