{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 18:54:55.931936: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-16 18:54:56.068075: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:56.068095: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-16 18:54:57.111241: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:57.111353: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:57.111364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,5e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.8         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 18:54:58.339850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-16 18:54:58.340138: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:58.340229: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:58.340309: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:58.340382: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:58.340452: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:58.340525: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:58.340597: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:58.340666: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-16 18:54:58.340678: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-16 18:54:58.340915: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1402 - mean_squared_error: 0.1402\n",
      "Loss: 0.14020921289920807 , Elapsed time: 117.90852999687195\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0539 - mean_squared_error: 0.0539\n",
      "Loss: 0.05393391102552414 , Elapsed time: 22.16800093650818\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0466 - mean_squared_error: 0.0466\n",
      "Loss: 0.046556659042835236 , Elapsed time: 40.47647547721863\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0732 - mean_squared_error: 0.0732\n",
      "Loss: 0.07320014387369156 , Elapsed time: 59.752352237701416\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Loss: 0.037421975284814835 , Elapsed time: 53.330567359924316\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin     \tavg      \tmax     \n",
      "0  \t5     \t0.037422\t0.0702644\t0.140209\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Loss: 0.03266243636608124 , Elapsed time: 82.55513381958008\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0603 - mean_squared_error: 0.0603\n",
      "Loss: 0.06025472283363342 , Elapsed time: 21.187556266784668\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 1 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1165 - mean_squared_error: 0.1165\n",
      "Loss: 0.11648785322904587 , Elapsed time: 58.19053936004639\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t3     \t0.0326624\t0.0568498\t0.116488\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Loss: 0.03647367283701897 , Elapsed time: 83.15917587280273\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0588 - mean_squared_error: 0.0588\n",
      "Loss: 0.058758728206157684 , Elapsed time: 54.75286030769348\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0871 - mean_squared_error: 0.0871\n",
      "Loss: 0.08707332611083984 , Elapsed time: 82.99125075340271\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Loss: 0.035617370158433914 , Elapsed time: 64.11960029602051\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t4     \t0.0326624\t0.0501171\t0.0870733\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Loss: 0.03179461508989334 , Elapsed time: 143.64444541931152\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Loss: 0.03170035034418106 , Elapsed time: 143.13221287727356\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Loss: 0.03387705609202385 , Elapsed time: 82.21962094306946\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0406 - mean_squared_error: 0.0406\n",
      "Loss: 0.040566906332969666 , Elapsed time: 59.42308855056763\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t4     \t0.0317004\t0.0341203\t0.0405669\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 1s 3ms/step - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Loss: 0.03197555989027023 , Elapsed time: 86.3814344406128\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Loss: 0.037682145833969116 , Elapsed time: 77.12712907791138\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Loss: 0.03582461178302765 , Elapsed time: 84.24599647521973\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Loss: 0.03153851628303528 , Elapsed time: 49.90961503982544\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t4     \t0.0315385\t0.0337442\t0.0376821\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Loss: 0.033320873975753784 , Elapsed time: 135.1502685546875\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Loss: 0.039529312402009964 , Elapsed time: 27.267658472061157\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Loss: 0.0324026457965374 , Elapsed time: 131.46214365959167\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Loss: 0.034244753420352936 , Elapsed time: 54.75064182281494\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.0315385\t0.0342072\t0.0395293\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Loss: 0.033474694937467575 , Elapsed time: 143.25493693351746\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0462 - mean_squared_error: 0.0462\n",
      "Loss: 0.04616028070449829 , Elapsed time: 39.77544355392456\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Loss: 0.037177637219429016 , Elapsed time: 42.7326397895813\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t3     \t0.0315385\t0.0359779\t0.0461603\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Loss: 0.03398611769080162 , Elapsed time: 131.35401368141174\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Loss: 0.0386539101600647 , Elapsed time: 33.319185733795166\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Loss: 0.03209531307220459 , Elapsed time: 45.78451609611511\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Loss: 0.03327540308237076 , Elapsed time: 158.35711026191711\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.0315385\t0.0339099\t0.0386539\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Loss: 0.031116962432861328 , Elapsed time: 49.070491313934326\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Loss: 0.033955417573451996 , Elapsed time: 39.497525215148926\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0517 - mean_squared_error: 0.0517\n",
      "Loss: 0.05165983363986015 , Elapsed time: 34.19550442695618\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t3     \t0.031117 \t0.0373849\t0.0516598\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0392 - mean_squared_error: 0.0392\n",
      "Loss: 0.03924911841750145 , Elapsed time: 33.41473078727722\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Loss: 0.031075187027454376 , Elapsed time: 43.39440369606018\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Loss: 0.03255072608590126 , Elapsed time: 42.23715901374817\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t3     \t0.0310752\t0.0331061\t0.0392491\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Loss: 0.03484787046909332 , Elapsed time: 112.2288806438446\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Loss: 0.03082403913140297 , Elapsed time: 44.2651731967926\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Loss: 0.03350134938955307 , Elapsed time: 39.97267389297485\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Loss: 0.03731966391205788 , Elapsed time: 34.33766579627991\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t4     \t0.030824 \t0.0336063\t0.0373197\n",
      "-- Best Individual =  [1, 1, 1, 1, 0, 1, 0]\n",
      "-- Best Fitness =  0.03153851628303528\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 48.045942322413126 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 2 , Number of neurons: 100\n",
      "Batch size 4 , Learning rate: 0.005\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.030824</td>\n",
       "      <td>0.030824</td>\n",
       "      <td>44.265173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.031075</td>\n",
       "      <td>0.031075</td>\n",
       "      <td>43.394404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.031117</td>\n",
       "      <td>0.031117</td>\n",
       "      <td>49.070491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.031539</td>\n",
       "      <td>0.031539</td>\n",
       "      <td>49.909615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>143.132213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031795</td>\n",
       "      <td>0.031795</td>\n",
       "      <td>143.644445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031976</td>\n",
       "      <td>0.031976</td>\n",
       "      <td>86.381434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.032095</td>\n",
       "      <td>0.032095</td>\n",
       "      <td>45.784516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032403</td>\n",
       "      <td>0.032403</td>\n",
       "      <td>131.462144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.032551</td>\n",
       "      <td>0.032551</td>\n",
       "      <td>42.237159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032662</td>\n",
       "      <td>0.032662</td>\n",
       "      <td>82.555134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033275</td>\n",
       "      <td>0.033275</td>\n",
       "      <td>158.357110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033321</td>\n",
       "      <td>0.033321</td>\n",
       "      <td>135.150269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033475</td>\n",
       "      <td>0.033475</td>\n",
       "      <td>143.254937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.033501</td>\n",
       "      <td>0.033501</td>\n",
       "      <td>39.972674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033877</td>\n",
       "      <td>0.033877</td>\n",
       "      <td>82.219621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.033955</td>\n",
       "      <td>0.033955</td>\n",
       "      <td>39.497525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033986</td>\n",
       "      <td>0.033986</td>\n",
       "      <td>131.354014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.034245</td>\n",
       "      <td>0.034245</td>\n",
       "      <td>54.750642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034848</td>\n",
       "      <td>0.034848</td>\n",
       "      <td>112.228881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035617</td>\n",
       "      <td>0.035617</td>\n",
       "      <td>64.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035825</td>\n",
       "      <td>0.035825</td>\n",
       "      <td>84.245996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036474</td>\n",
       "      <td>0.036474</td>\n",
       "      <td>83.159176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.037178</td>\n",
       "      <td>0.037178</td>\n",
       "      <td>42.732640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.037320</td>\n",
       "      <td>0.037320</td>\n",
       "      <td>34.337666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037422</td>\n",
       "      <td>0.037422</td>\n",
       "      <td>53.330567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.037682</td>\n",
       "      <td>0.037682</td>\n",
       "      <td>77.127129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.038654</td>\n",
       "      <td>0.038654</td>\n",
       "      <td>33.319186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.039249</td>\n",
       "      <td>0.039249</td>\n",
       "      <td>33.414731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.039529</td>\n",
       "      <td>0.039529</td>\n",
       "      <td>27.267658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.040567</td>\n",
       "      <td>0.040567</td>\n",
       "      <td>59.423089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.046160</td>\n",
       "      <td>0.046160</td>\n",
       "      <td>39.775444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.046557</td>\n",
       "      <td>0.046557</td>\n",
       "      <td>40.476475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.051660</td>\n",
       "      <td>0.051660</td>\n",
       "      <td>34.195504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.053934</td>\n",
       "      <td>0.053934</td>\n",
       "      <td>22.168001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.058759</td>\n",
       "      <td>0.058759</td>\n",
       "      <td>54.752860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.060255</td>\n",
       "      <td>0.060255</td>\n",
       "      <td>21.187556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>59.752352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.087073</td>\n",
       "      <td>0.087073</td>\n",
       "      <td>82.991251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.116488</td>\n",
       "      <td>0.116488</td>\n",
       "      <td>58.190539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.140209</td>\n",
       "      <td>0.140209</td>\n",
       "      <td>117.908530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             4        200         0.0001           8  0.030824  0.030824   \n",
       "1             4        200         0.0001           8  0.031075  0.031075   \n",
       "2             4        200         0.0001           8  0.031117  0.031117   \n",
       "3             4        200         0.0001           8  0.031539  0.031539   \n",
       "4             2        150         0.0001           2  0.031700  0.031700   \n",
       "5             4        200         0.0001           4  0.031795  0.031795   \n",
       "6             4        200         0.0001           4  0.031976  0.031976   \n",
       "7             4        200         0.0001           8  0.032095  0.032095   \n",
       "8             4        150         0.0001           2  0.032403  0.032403   \n",
       "9             4        200         0.0001           8  0.032551  0.032551   \n",
       "10            4        200         0.0001           4  0.032662  0.032662   \n",
       "11            4        200         0.0001           2  0.033275  0.033275   \n",
       "12            4        150         0.0001           2  0.033321  0.033321   \n",
       "13            4        150         0.0001           2  0.033475  0.033475   \n",
       "14            4        200         0.0001           8  0.033501  0.033501   \n",
       "15            4        200         0.0001           4  0.033877  0.033877   \n",
       "16            4        200         0.0001           8  0.033955  0.033955   \n",
       "17            4        150         0.0001           2  0.033986  0.033986   \n",
       "18            4        200         0.0001           8  0.034245  0.034245   \n",
       "19            4        100         0.0001           2  0.034848  0.034848   \n",
       "20            2         50         0.0001           4  0.035617  0.035617   \n",
       "21            4        200         0.0001           4  0.035825  0.035825   \n",
       "22            2        100         0.0001           4  0.036474  0.036474   \n",
       "23            4         50         0.0001           8  0.037178  0.037178   \n",
       "24            4        100         0.0001           8  0.037320  0.037320   \n",
       "25            2        100         0.0001           4  0.037422  0.037422   \n",
       "26            4        200         0.0001           4  0.037682  0.037682   \n",
       "27            4         50         0.0001           8  0.038654  0.038654   \n",
       "28            4         50         0.0001           8  0.039249  0.039249   \n",
       "29            4        200         0.0001          16  0.039529  0.039529   \n",
       "30            2        100         0.0001           4  0.040567  0.040567   \n",
       "31            4        100         0.0001           8  0.046160  0.046160   \n",
       "32            4        200         0.0050          16  0.046557  0.046557   \n",
       "33            3        200         0.0001           8  0.051660  0.051660   \n",
       "34            2        200         0.0001          16  0.053934  0.053934   \n",
       "35            4        100         0.0050           4  0.058759  0.058759   \n",
       "36            2        100         0.0050          16  0.060255  0.060255   \n",
       "37            1        100         0.0001           4  0.073200  0.073200   \n",
       "38            1        100         0.0001           4  0.087073  0.087073   \n",
       "39            1         50         0.0001           4  0.116488  0.116488   \n",
       "40            3        100         0.0050           2  0.140209  0.140209   \n",
       "\n",
       "    Elapsed time  \n",
       "0      44.265173  \n",
       "1      43.394404  \n",
       "2      49.070491  \n",
       "3      49.909615  \n",
       "4     143.132213  \n",
       "5     143.644445  \n",
       "6      86.381434  \n",
       "7      45.784516  \n",
       "8     131.462144  \n",
       "9      42.237159  \n",
       "10     82.555134  \n",
       "11    158.357110  \n",
       "12    135.150269  \n",
       "13    143.254937  \n",
       "14     39.972674  \n",
       "15     82.219621  \n",
       "16     39.497525  \n",
       "17    131.354014  \n",
       "18     54.750642  \n",
       "19    112.228881  \n",
       "20     64.119600  \n",
       "21     84.245996  \n",
       "22     83.159176  \n",
       "23     42.732640  \n",
       "24     34.337666  \n",
       "25     53.330567  \n",
       "26     77.127129  \n",
       "27     33.319186  \n",
       "28     33.414731  \n",
       "29     27.267658  \n",
       "30     59.423089  \n",
       "31     39.775444  \n",
       "32     40.476475  \n",
       "33     34.195504  \n",
       "34     22.168001  \n",
       "35     54.752860  \n",
       "36     21.187556  \n",
       "37     59.752352  \n",
       "38     82.991251  \n",
       "39     58.190539  \n",
       "40    117.908530  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla2.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 48.042 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
