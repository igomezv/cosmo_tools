{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gS9bn7bhqpCG",
    "outputId": "38bcc44c-8019-449a-e1ee-2de1e086c68e"
   },
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iD0I8HwdqpCb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "import scipy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aFY-8_lrqpCg"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fsRv8kuNqpCw",
    "outputId": "773d68d4-3c4e-42e4-c9ca-be5c60ac5412"
   },
   "outputs": [],
   "source": [
    "#algorithm for splitting the dataset into training and validation \n",
    "def split(X,Y,porcent): #porcent must be between 0 and 1, it is the asigned porcent to the training dataset.\n",
    "    n=floor(porcent*len(X))\n",
    "    index=random.sample(range(len(X)),n)\n",
    "    X_learn=[]\n",
    "    Y_learn=[]\n",
    "    for i in index:\n",
    "        X_learn.append(X[i])\n",
    "        Y_learn.append(Y[i])\n",
    "    X_val=np.delete(X,index, axis=0)\n",
    "    Y_val=np.delete(Y,index, axis=0)\n",
    "    \n",
    "    X_learn=np.array(X_learn)\n",
    "    Y_learn=np.array(Y_learn)\n",
    "    return X_learn,Y_learn,X_val,Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_m=np.arange(0.1,0.51,0.1)\n",
    "H_0=np.arange(66,81,1)\n",
    "t=np.linspace(0,-12,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RHS(Omega_i, lna, gamma=0):\n",
    "    x, y, z, H = Omega_i\n",
    "    #x, y, z = Omega_i\n",
    "    pi = 3*x + 4*y\n",
    "    return [x*(-3 + pi), y*(-4 + pi), z*pi, -0.5*H*pi]\n",
    "    #return [x*(-3 + pi), y*(-4 + pi), z*pi]\n",
    "\n",
    "def EDO(t,Om,H0):\n",
    "    #t,Or,Om,Ol=X\n",
    "    Or=0.0001\n",
    "    Ol=1-Or-Om\n",
    "    #H0 = 70.\n",
    "    y0 = [Om, Or, Ol, H0]\n",
    "    result = odeint(RHS, y0, t)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets generate the cartesian product between the intervals\n",
    "Y0=[]\n",
    "#este ciclo llena la lista fijando un Om y pasando todos los Or\n",
    "for i in O_m:\n",
    "    for j in H_0:\n",
    "        Y0.extend(EDO(t,i,j))\n",
    "Y0=np.array(Y0)\n",
    "\n",
    "X0=[]\n",
    "for Om in O_m:\n",
    "    for H0 in H_0:\n",
    "        for T in t:\n",
    "            X0.append([T,Om,H0])\n",
    "X0=np.array(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "Y2 = scaler.fit_transform(Y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feactures= \n",
      " [[  0.           0.1         66.        ]\n",
      " [ -0.24489796   0.1         66.        ]\n",
      " [ -0.48979592   0.1         66.        ]\n",
      " ...\n",
      " [-11.51020408   0.5         80.        ]\n",
      " [-11.75510204   0.5         80.        ]\n",
      " [-12.           0.5         80.        ]]\n",
      "\n",
      "\n",
      "labels= \n",
      " [[9.47515373e-02 0.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      " [1.83608702e-01 1.41110961e-04 9.01997379e-01 1.62358493e-10]\n",
      " [3.22333902e-01 4.33937896e-04 7.48914062e-01 4.77144941e-10]\n",
      " ...\n",
      " [4.20164900e-02 9.58104047e-01 5.58400899e-11 3.78984027e-01]\n",
      " [3.19464665e-02 9.68145182e-01 5.73926010e-11 6.15280156e-01]\n",
      " [2.39153274e-02 9.76153281e-01 5.75107557e-11 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Now, here are the datasets\n",
    "print('feactures= \\n',X0)\n",
    "print('\\n')\n",
    "print('labels= \\n',Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = 0.8\n",
    "X_train, Y_train, X_test, Y_test = split(X0, Y2, split_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgjOFB8kqpC_"
   },
   "source": [
    "### Hiperpar√°metros del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ABViOAgTqpDI"
   },
   "outputs": [],
   "source": [
    "# HP_NUM_UNITS3 = hp.HParam('num_units3', hp.Discrete([50, 100, 150, 200]))\n",
    "# HP_NUM_UNITS4 = hp.HParam('num_units4', hp.Discrete([2, 5, 10]))\n",
    "# HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.0, 0.2))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'Adadelta']))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
    "HP_BATCHSIZE = hp.HParam('batch_size', hp.Discrete([4, 8, 16]))\n",
    "\n",
    "\n",
    "HP_LAYERS =    hp.HParam('layers', hp.Discrete([3, 4]))\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([50, 100, 200]))\n",
    "HP_LEARNING  = hp.HParam('learning_rate', hp.Discrete([1e-5,1e-4,1e-3]))\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n",
    "                                   min_delta=0,\n",
    "                                   patience=100,\n",
    "                                   restore_best_weights=True)]\n",
    "# batch_size = 128\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lXRw_6G8qpDL"
   },
   "outputs": [],
   "source": [
    "# METRIC_ACCURACY = 'accuracy'\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning2').as_default():\n",
    "# with tf.summary.FileWriter('logs/hparam_tuning', sess.graph):\n",
    "#     init = tf.initialize_all_variables()\n",
    "#     sess.run(init)\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_LAYERS,\n",
    "                 HP_NUM_UNITS,\n",
    "                 HP_LEARNING, \n",
    "                 HP_BATCHSIZE],\n",
    "        metrics=[hp.Metric('loss', display_name=\"Loss\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6JG3WeEOqpDY"
   },
   "outputs": [],
   "source": [
    "def train_test_model(hparams):    \n",
    "    \n",
    "    # Train LSTM model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(hparams[HP_LAYERS]):        \n",
    "        model.add(Dense(hparams[HP_NUM_UNITS], activation='relu'))\n",
    "    model.add(Dense(4, activation='linear'))\n",
    "     \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING], beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse', \n",
    "            metrics=['mean_squared_error'])\n",
    "    \n",
    "    # Run with 1 epoch to speed things up for demo purposes\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_test, Y_test),\n",
    "              callbacks=callbacks, batch_size=hparams[HP_BATCHSIZE], shuffle=False, verbose=0)\n",
    "\n",
    "    _, loss = model.evaluate(X_test, Y_test)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fnLlAKGuqpDp"
   },
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(\"loss\", loss, step=1)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tu8q13lqpDs",
    "outputId": "80f9eb4f-dc3a-445e-ff6d-3d42dcb085c3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting trial: run-0\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 572us/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Loss: 0.004773477092385292 Tiempo transcurrido: 300.8620891571045\n",
      "\n",
      "--- Starting trial: run-1\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 591us/step - loss: 0.0076 - mean_squared_error: 0.0076\n",
      "Loss: 0.007636039052158594 Tiempo transcurrido: 155.80036306381226\n",
      "\n",
      "--- Starting trial: run-2\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 581us/step - loss: 0.0074 - mean_squared_error: 0.0074\n",
      "Loss: 0.0074049802497029305 Tiempo transcurrido: 84.41967463493347\n",
      "\n",
      "--- Starting trial: run-3\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 637us/step - loss: 1.8913e-04 - mean_squared_error: 1.8913e-04\n",
      "Loss: 0.00018912542145699263 Tiempo transcurrido: 305.47969341278076\n",
      "\n",
      "--- Starting trial: run-4\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 646us/step - loss: 0.0024 - mean_squared_error: 0.0024\n",
      "Loss: 0.002379420679062605 Tiempo transcurrido: 157.03536653518677\n",
      "\n",
      "--- Starting trial: run-5\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 599us/step - loss: 0.0089 - mean_squared_error: 0.0089\n",
      "Loss: 0.008858210407197475 Tiempo transcurrido: 84.02468681335449\n",
      "\n",
      "--- Starting trial: run-6\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 620us/step - loss: 2.0172e-04 - mean_squared_error: 2.0172e-04\n",
      "Loss: 0.00020172004587948322 Tiempo transcurrido: 163.64964032173157\n",
      "\n",
      "--- Starting trial: run-7\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 623us/step - loss: 2.3684e-04 - mean_squared_error: 2.3684e-04\n",
      "Loss: 0.00023683748440816998 Tiempo transcurrido: 117.31831073760986\n",
      "\n",
      "--- Starting trial: run-8\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 603us/step - loss: 5.3156e-04 - mean_squared_error: 5.3156e-04\n",
      "Loss: 0.0005315645830705762 Tiempo transcurrido: 56.70892143249512\n",
      "\n",
      "--- Starting trial: run-9\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 640us/step - loss: 0.0040 - mean_squared_error: 0.0040\n",
      "Loss: 0.00402867142111063 Tiempo transcurrido: 285.54686403274536\n",
      "\n",
      "--- Starting trial: run-10\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 636us/step - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Loss: 0.00691572017967701 Tiempo transcurrido: 149.02964878082275\n",
      "\n",
      "--- Starting trial: run-11\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 642us/step - loss: 0.0108 - mean_squared_error: 0.0108\n",
      "Loss: 0.010765467770397663 Tiempo transcurrido: 83.5677809715271\n",
      "\n",
      "--- Starting trial: run-12\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 595us/step - loss: 1.9368e-04 - mean_squared_error: 1.9368e-04\n",
      "Loss: 0.0001936786575242877 Tiempo transcurrido: 287.0903089046478\n",
      "\n",
      "--- Starting trial: run-13\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 669us/step - loss: 3.6024e-04 - mean_squared_error: 3.6024e-04\n",
      "Loss: 0.00036024002474732697 Tiempo transcurrido: 150.46125531196594\n",
      "\n",
      "--- Starting trial: run-14\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 678us/step - loss: 0.0028 - mean_squared_error: 0.0028\n",
      "Loss: 0.0027593972627073526 Tiempo transcurrido: 83.75408816337585\n",
      "\n",
      "--- Starting trial: run-15\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 580us/step - loss: 9.7459e-05 - mean_squared_error: 9.7459e-05\n",
      "Loss: 9.745927673066035e-05 Tiempo transcurrido: 238.6674530506134\n",
      "\n",
      "--- Starting trial: run-16\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 636us/step - loss: 1.0473e-04 - mean_squared_error: 1.0473e-04\n",
      "Loss: 0.00010473110160091892 Tiempo transcurrido: 96.17328953742981\n",
      "\n",
      "--- Starting trial: run-17\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 644us/step - loss: 2.9503e-04 - mean_squared_error: 2.9503e-04\n",
      "Loss: 0.0002950335619971156 Tiempo transcurrido: 46.05085229873657\n",
      "\n",
      "--- Starting trial: run-18\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 717us/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Loss: 0.0038771992549300194 Tiempo transcurrido: 326.36333751678467\n",
      "\n",
      "--- Starting trial: run-19\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 732us/step - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Loss: 0.007200188934803009 Tiempo transcurrido: 171.5658688545227\n",
      "\n",
      "--- Starting trial: run-20\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 653us/step - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Loss: 0.005777901969850063 Tiempo transcurrido: 98.80572867393494\n",
      "\n",
      "--- Starting trial: run-21\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 718us/step - loss: 1.8489e-04 - mean_squared_error: 1.8489e-04\n",
      "Loss: 0.00018489384092390537 Tiempo transcurrido: 323.5334966182709\n",
      "\n",
      "--- Starting trial: run-22\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 706us/step - loss: 1.9905e-04 - mean_squared_error: 1.9905e-04\n",
      "Loss: 0.00019904629152733833 Tiempo transcurrido: 170.80649209022522\n",
      "\n",
      "--- Starting trial: run-23\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 620us/step - loss: 2.2498e-04 - mean_squared_error: 2.2498e-04\n",
      "Loss: 0.00022497570898849517 Tiempo transcurrido: 98.75160598754883\n",
      "\n",
      "--- Starting trial: run-24\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 671us/step - loss: 1.4709e-04 - mean_squared_error: 1.4709e-04\n",
      "Loss: 0.00014709097740706056 Tiempo transcurrido: 213.22478103637695\n",
      "\n",
      "--- Starting trial: run-25\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 676us/step - loss: 3.4892e-04 - mean_squared_error: 3.4892e-04\n",
      "Loss: 0.0003489243099465966 Tiempo transcurrido: 63.77119851112366\n",
      "\n",
      "--- Starting trial: run-26\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 746us/step - loss: 0.0020 - mean_squared_error: 0.0020\n",
      "Loss: 0.001977683277800679 Tiempo transcurrido: 99.4697949886322\n",
      "\n",
      "--- Starting trial: run-27\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 618us/step - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Loss: 0.005798382218927145 Tiempo transcurrido: 303.08828997612\n",
      "\n",
      "--- Starting trial: run-28\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 660us/step - loss: 0.0099 - mean_squared_error: 0.0099\n",
      "Loss: 0.009869464673101902 Tiempo transcurrido: 156.27028727531433\n",
      "\n",
      "--- Starting trial: run-29\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 651us/step - loss: 0.0098 - mean_squared_error: 0.0098\n",
      "Loss: 0.009791441261768341 Tiempo transcurrido: 85.67473292350769\n",
      "\n",
      "--- Starting trial: run-30\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 576us/step - loss: 2.2697e-04 - mean_squared_error: 2.2697e-04\n",
      "Loss: 0.00022697009262628853 Tiempo transcurrido: 301.2512493133545\n",
      "\n",
      "--- Starting trial: run-31\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 670us/step - loss: 4.7734e-04 - mean_squared_error: 4.7734e-04\n",
      "Loss: 0.0004773368709720671 Tiempo transcurrido: 155.54998779296875\n",
      "\n",
      "--- Starting trial: run-32\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 670us/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "Loss: 0.0015666859690099955 Tiempo transcurrido: 85.25781893730164\n",
      "\n",
      "--- Starting trial: run-33\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 599us/step - loss: 2.2217e-04 - mean_squared_error: 2.2217e-04\n",
      "Loss: 0.00022216928482521325 Tiempo transcurrido: 302.28452372550964\n",
      "\n",
      "--- Starting trial: run-34\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 656us/step - loss: 1.6340e-04 - mean_squared_error: 1.6340e-04\n",
      "Loss: 0.00016339897410944104 Tiempo transcurrido: 77.95213007926941\n",
      "\n",
      "--- Starting trial: run-35\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 676us/step - loss: 2.5935e-04 - mean_squared_error: 2.5935e-04\n",
      "Loss: 0.0002593545359559357 Tiempo transcurrido: 51.246023654937744\n",
      "\n",
      "--- Starting trial: run-36\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 759us/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Loss: 0.005121372640132904 Tiempo transcurrido: 302.9917800426483\n",
      "\n",
      "--- Starting trial: run-37\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 672us/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Loss: 0.004956631455570459 Tiempo transcurrido: 157.77012467384338\n",
      "\n",
      "--- Starting trial: run-38\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 628us/step - loss: 0.0083 - mean_squared_error: 0.0083\n",
      "Loss: 0.008332641795277596 Tiempo transcurrido: 88.75557136535645\n",
      "\n",
      "--- Starting trial: run-39\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 693us/step - loss: 1.6340e-04 - mean_squared_error: 1.6340e-04\n",
      "Loss: 0.00016339975991286337 Tiempo transcurrido: 271.87248063087463\n",
      "\n",
      "--- Starting trial: run-40\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 661us/step - loss: 3.2692e-04 - mean_squared_error: 3.2692e-04\n",
      "Loss: 0.000326924113323912 Tiempo transcurrido: 158.54304552078247\n",
      "\n",
      "--- Starting trial: run-41\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 630us/step - loss: 9.3940e-04 - mean_squared_error: 9.3940e-04\n",
      "Loss: 0.0009394006920047104 Tiempo transcurrido: 89.06720018386841\n",
      "\n",
      "--- Starting trial: run-42\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 667us/step - loss: 7.7286e-05 - mean_squared_error: 7.7286e-05\n",
      "Loss: 7.728646596660838e-05 Tiempo transcurrido: 297.9256663322449\n",
      "\n",
      "--- Starting trial: run-43\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 665us/step - loss: 1.0357e-04 - mean_squared_error: 1.0357e-04\n",
      "Loss: 0.0001035682435031049 Tiempo transcurrido: 131.0763201713562\n",
      "\n",
      "--- Starting trial: run-44\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 673us/step - loss: 2.4837e-04 - mean_squared_error: 2.4837e-04\n",
      "Loss: 0.0002483733114786446 Tiempo transcurrido: 47.56417775154114\n",
      "\n",
      "--- Starting trial: run-45\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 730us/step - loss: 0.0026 - mean_squared_error: 0.0026\n",
      "Loss: 0.0025665967259556055 Tiempo transcurrido: 355.9526057243347\n",
      "\n",
      "--- Starting trial: run-46\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 757us/step - loss: 0.0038 - mean_squared_error: 0.0038\n",
      "Loss: 0.0038084888365119696 Tiempo transcurrido: 192.99774599075317\n",
      "\n",
      "--- Starting trial: run-47\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 732us/step - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Loss: 0.006138915196061134 Tiempo transcurrido: 110.46041965484619\n",
      "\n",
      "--- Starting trial: run-48\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 731us/step - loss: 1.6335e-04 - mean_squared_error: 1.6335e-04\n",
      "Loss: 0.0001633481151657179 Tiempo transcurrido: 362.1110408306122\n",
      "\n",
      "--- Starting trial: run-49\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 709us/step - loss: 2.6968e-04 - mean_squared_error: 2.6968e-04\n",
      "Loss: 0.0002696763549465686 Tiempo transcurrido: 193.60442519187927\n",
      "\n",
      "--- Starting trial: run-50\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 789us/step - loss: 0.0013 - mean_squared_error: 0.0013\n",
      "Loss: 0.001288837636820972 Tiempo transcurrido: 99.06406164169312\n",
      "\n",
      "--- Starting trial: run-51\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "24/24 [==============================] - 0s 798us/step - loss: 1.6664e-04 - mean_squared_error: 1.6664e-04\n",
      "Loss: 0.00016663850692566484 Tiempo transcurrido: 364.4622781276703\n",
      "\n",
      "--- Starting trial: run-52\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "24/24 [==============================] - 0s 716us/step - loss: 1.3160e-04 - mean_squared_error: 1.3160e-04\n",
      "Loss: 0.00013160184607841074 Tiempo transcurrido: 193.11871457099915\n",
      "\n",
      "--- Starting trial: run-53\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "24/24 [==============================] - 0s 708us/step - loss: 2.4436e-04 - mean_squared_error: 2.4436e-04\n",
      "Loss: 0.0002443617267999798 Tiempo transcurrido: 102.9963960647583\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "datos = []\n",
    "t0 = time.time()\n",
    "for deep_layers in HP_LAYERS.domain.values:\n",
    "    for num_units in HP_NUM_UNITS.domain.values:\n",
    "        for learning_rate in HP_LEARNING.domain.values:\n",
    "            for batch_size in HP_BATCHSIZE.domain.values:\n",
    "                t = time.time()\n",
    "                hparams = {\n",
    "\n",
    "                    HP_LAYERS: deep_layers,\n",
    "                    HP_NUM_UNITS: num_units,\n",
    "                    HP_LEARNING: learning_rate,\n",
    "                    HP_BATCHSIZE: batch_size,\n",
    "                }\n",
    "                run_name = \"run-%d\" % session_num\n",
    "                print('\\n--- Starting trial: %s' % run_name)\n",
    "                print({h.name: hparams[h] for h in hparams})\n",
    "                score = run('logs/hparam_tuning2/' + run_name, hparams)\n",
    "                t = time.time()-t\n",
    "                session_num += 1\n",
    "                print(\"Loss:\", score, \"Tiempo transcurrido:\", t)\n",
    "            \n",
    "            datos.append([deep_layers, num_units, learning_rate, batch_size, score, t])\n",
    "\n",
    "print(session_num)\n",
    "tf = time.time() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1ct8OIfqpD3"
   },
   "source": [
    "### Guardar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_6xZZqEhqpD5"
   },
   "outputs": [],
   "source": [
    "filename = \"historial_ecsdiff_tunning.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep size\", \"Num units\", \"Learning rate\", \"Batch size\", \"MSE\", \"Tiempo de ejecuci√≥n\"])\n",
    "\n",
    "df.sort_values(by=[\"MSE\", \"Tiempo de ejecuci√≥n\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "z9BerX2yqpD-",
    "outputId": "992e08c9-0adb-4f57-e50e-df65c91fe142",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep size</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Tiempo de ejecuci√≥n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>98.751606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>102.996396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>47.564178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>51.246024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>46.050852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>56.708921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>89.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>99.064062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>85.257819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>99.469795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>83.754088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.005778</td>\n",
       "      <td>98.805729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.006139</td>\n",
       "      <td>110.460420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>84.419675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>88.755571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.008858</td>\n",
       "      <td>84.024687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.009791</td>\n",
       "      <td>85.674733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.010765</td>\n",
       "      <td>83.567781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep size  Num units  Learning rate  Batch size       MSE  \\\n",
       "0           3        200        0.00010          16  0.000225   \n",
       "1           4        200        0.00100          16  0.000244   \n",
       "2           4        100        0.00100          16  0.000248   \n",
       "3           4         50        0.00100          16  0.000259   \n",
       "4           3        100        0.00100          16  0.000295   \n",
       "5           3         50        0.00100          16  0.000532   \n",
       "6           4        100        0.00010          16  0.000939   \n",
       "7           4        200        0.00010          16  0.001289   \n",
       "8           4         50        0.00010          16  0.001567   \n",
       "9           3        200        0.00100          16  0.001978   \n",
       "10          3        100        0.00010          16  0.002759   \n",
       "11          3        200        0.00001          16  0.005778   \n",
       "12          4        200        0.00001          16  0.006139   \n",
       "13          3         50        0.00001          16  0.007405   \n",
       "14          4        100        0.00001          16  0.008333   \n",
       "15          3         50        0.00010          16  0.008858   \n",
       "16          4         50        0.00001          16  0.009791   \n",
       "17          3        100        0.00001          16  0.010765   \n",
       "\n",
       "    Tiempo de ejecuci√≥n  \n",
       "0             98.751606  \n",
       "1            102.996396  \n",
       "2             47.564178  \n",
       "3             51.246024  \n",
       "4             46.050852  \n",
       "5             56.708921  \n",
       "6             89.067200  \n",
       "7             99.064062  \n",
       "8             85.257819  \n",
       "9             99.469795  \n",
       "10            83.754088  \n",
       "11            98.805729  \n",
       "12           110.460420  \n",
       "13            84.419675  \n",
       "14            88.755571  \n",
       "15            84.024687  \n",
       "16            85.674733  \n",
       "17            83.567781  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo total: 157.51 min\n"
     ]
    }
   ],
   "source": [
    "print(\"Tiempo total: {:.2f} min\".format(tf/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xh9rbb8kqpED"
   },
   "outputs": [],
   "source": [
    "# rm -rf /tmp/tb_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm044iKXqpEM"
   },
   "source": [
    "### Now in terminal:\n",
    "`python3 -m tensorboard.main --logdir='/home/isidro/Documents/github/neurapprox/logs/hparam_tuning'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: kill: No such process\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "!kill 7439\n",
    "%tensorboard --logdir logs/hparam_tuning2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "tunning_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
