{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# Clear any logs from previous runs\n",
    "# !rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "import scipy\n",
    "import pandas as pd\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('data/jla.csv')\n",
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]\n",
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_test = np.split(z, indx)\n",
    "Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_test), Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss -> val_loss\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n",
    "                                   min_delta=0.0,\n",
    "                                   patience=20,\n",
    "                                   restore_best_weights=True, verbose=True)\n",
    "                                   ]\n",
    "\n",
    "n_cols = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_BATCHSIZE = hp.HParam('batch_size', hp.Discrete([4, 8, 16]))\n",
    "HP_LAYERS =    hp.HParam('layers', hp.Discrete([2, 3, 4]))\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([100,200]))\n",
    "HP_LEARNING  = hp.HParam('learning_rate', hp.Discrete([1e-5,1e-4,1e-3]))\n",
    "# HP_NUM_UNITS3 = hp.HParam('num_units3', hp.Discrete([50, 100, 150, 200]))\n",
    "# HP_NUM_UNITS4 = hp.HParam('num_units4', hp.Discrete([2, 5, 10]))\n",
    "# HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.0, 0.2))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'Adadelta']))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.Session()\n",
    "# writer = tf.summary.FileWriter(\"/tmp/tfvgg\", sess.graph)\n",
    "# init = tf.initialize_all_variables()\n",
    "# sess.run(init)\n",
    "# with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "#     hp.hparams_config(\n",
    "#         hparams=[HP_NUM_UNITS1, HP_NUM_UNITS2, HP_NUM_UNITS3, HP_NUM_UNITS4,\n",
    "#                  HP_OPTIMIZER, HP_BATCHSIZE],\n",
    "#         metrics=[hp.Metric('loss', display_name=\"Loss\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC_ACCURACY = 'accuracy'\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning3').as_default():\n",
    "# with tf.summary.FileWriter('logs/hparam_tuning', sess.graph):\n",
    "#     init = tf.initialize_all_variables()\n",
    "#     sess.run(init)\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_LAYERS,\n",
    "                 HP_NUM_UNITS,\n",
    "                 HP_LEARNING, \n",
    "                 HP_BATCHSIZE],\n",
    "        metrics=[hp.Metric('loss', display_name=\"Loss\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):    \n",
    "    \n",
    "    # Train LSTM model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "#     model.add(Dense(hparams[HP_NUM_UNITS], input_shape=(int(X_train.shape[1]),)))\n",
    "    \n",
    "    for i in range(hparams[HP_LAYERS]):        \n",
    "        model.add(Dense(hparams[HP_NUM_UNITS], activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "     \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING], beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse', \n",
    "            metrics=['mean_squared_error'])\n",
    "    \n",
    "    # Run with 1 epoch to speed things up for demo purposes\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_test, Y_test),\n",
    "              callbacks=callbacks, batch_size=hparams[HP_BATCHSIZE], shuffle=False, verbose=0)\n",
    "\n",
    "    _, loss = model.evaluate(X_test, Y_test)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(\"loss\", loss, step=1)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting trial: run-0\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0793 - mean_squared_error: 0.0793\n",
      "Loss: 0.07932641357183456 Tiempo transcurrido: 127.65052461624146\n",
      "\n",
      "--- Starting trial: run-1\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1924 - mean_squared_error: 0.1924\n",
      "Loss: 0.1924332082271576 Tiempo transcurrido: 70.56019949913025\n",
      "\n",
      "--- Starting trial: run-2\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 16}\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.7910 - mean_squared_error: 2.7910\n",
      "Loss: 2.790975570678711 Tiempo transcurrido: 48.25539946556091\n",
      "\n",
      "--- Starting trial: run-3\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00311: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Loss: 0.03505986928939819 Tiempo transcurrido: 89.8285825252533\n",
      "\n",
      "--- Starting trial: run-4\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Loss: 0.031384989619255066 Tiempo transcurrido: 79.18626356124878\n",
      "\n",
      "--- Starting trial: run-5\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Loss: 0.034521620720624924 Tiempo transcurrido: 46.584476709365845\n",
      "\n",
      "--- Starting trial: run-6\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00075: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Loss: 0.03165954351425171 Tiempo transcurrido: 21.603556394577026\n",
      "\n",
      "--- Starting trial: run-7\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00103: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Loss: 0.031509969383478165 Tiempo transcurrido: 16.76666259765625\n",
      "\n",
      "--- Starting trial: run-8\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00126: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Loss: 0.03890568017959595 Tiempo transcurrido: 12.35003137588501\n",
      "\n",
      "--- Starting trial: run-9\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0480 - mean_squared_error: 0.0480\n",
      "Loss: 0.047967907041311264 Tiempo transcurrido: 140.19015073776245\n",
      "\n",
      "--- Starting trial: run-10\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1002 - mean_squared_error: 0.1002\n",
      "Loss: 0.10024335235357285 Tiempo transcurrido: 78.69869875907898\n",
      "\n",
      "--- Starting trial: run-11\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 16}\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5984 - mean_squared_error: 0.5984\n",
      "Loss: 0.5983596444129944 Tiempo transcurrido: 48.46231174468994\n",
      "\n",
      "--- Starting trial: run-12\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00324: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Loss: 0.03510195016860962 Tiempo transcurrido: 94.9573085308075\n",
      "\n",
      "--- Starting trial: run-13\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00373: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Loss: 0.031141990795731544 Tiempo transcurrido: 61.06962013244629\n",
      "\n",
      "--- Starting trial: run-14\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Loss: 0.032118964940309525 Tiempo transcurrido: 48.28004169464111\n",
      "\n",
      "--- Starting trial: run-15\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00055: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0499 - mean_squared_error: 0.0499\n",
      "Loss: 0.04985011741518974 Tiempo transcurrido: 16.59892988204956\n",
      "\n",
      "--- Starting trial: run-16\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00109: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Loss: 0.03207802027463913 Tiempo transcurrido: 18.092222929000854\n",
      "\n",
      "--- Starting trial: run-17\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00092: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0403 - mean_squared_error: 0.0403\n",
      "Loss: 0.040337879210710526 Tiempo transcurrido: 9.379066705703735\n",
      "\n",
      "--- Starting trial: run-18\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Loss: 0.03874960541725159 Tiempo transcurrido: 144.87251019477844\n",
      "\n",
      "--- Starting trial: run-19\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1377 - mean_squared_error: 0.1377\n",
      "Loss: 0.13768303394317627 Tiempo transcurrido: 82.63213348388672\n",
      "\n",
      "--- Starting trial: run-20\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 16}\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1735 - mean_squared_error: 0.1735\n",
      "Loss: 0.17351463437080383 Tiempo transcurrido: 45.62894821166992\n",
      "\n",
      "--- Starting trial: run-21\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00274: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Loss: 0.030957138165831566 Tiempo transcurrido: 93.96923995018005\n",
      "\n",
      "--- Starting trial: run-22\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00239: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Loss: 0.03460477665066719 Tiempo transcurrido: 46.66007661819458\n",
      "\n",
      "--- Starting trial: run-23\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00399: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Loss: 0.03403804823756218 Tiempo transcurrido: 45.363802671432495\n",
      "\n",
      "--- Starting trial: run-24\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00037: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0436 - mean_squared_error: 0.0436\n",
      "Loss: 0.04355167597532272 Tiempo transcurrido: 13.513280630111694\n",
      "\n",
      "--- Starting trial: run-25\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00063: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Loss: 0.03813659027218819 Tiempo transcurrido: 12.606405019760132\n",
      "\n",
      "--- Starting trial: run-26\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00068: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0465 - mean_squared_error: 0.0465\n",
      "Loss: 0.04650065675377846 Tiempo transcurrido: 8.372406721115112\n",
      "\n",
      "--- Starting trial: run-27\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0424 - mean_squared_error: 0.0424\n",
      "Loss: 0.04238049313426018 Tiempo transcurrido: 184.20871472358704\n",
      "\n",
      "--- Starting trial: run-28\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0433 - mean_squared_error: 0.0433\n",
      "Loss: 0.04328763112425804 Tiempo transcurrido: 108.46244144439697\n",
      "\n",
      "--- Starting trial: run-29\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 16}\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0862 - mean_squared_error: 0.0862\n",
      "Loss: 0.08618701249361038 Tiempo transcurrido: 70.97081065177917\n",
      "\n",
      "--- Starting trial: run-30\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00156: early stopping\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Loss: 0.033616792410612106 Tiempo transcurrido: 68.79901099205017\n",
      "\n",
      "--- Starting trial: run-31\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00232: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0419 - mean_squared_error: 0.0419\n",
      "Loss: 0.04192880541086197 Tiempo transcurrido: 57.1885347366333\n",
      "\n",
      "--- Starting trial: run-32\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00450: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Loss: 0.03134153410792351 Tiempo transcurrido: 82.79127907752991\n",
      "\n",
      "--- Starting trial: run-33\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00039: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Loss: 0.03878674656152725 Tiempo transcurrido: 17.969987392425537\n",
      "\n",
      "--- Starting trial: run-34\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00059: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Loss: 0.03184348717331886 Tiempo transcurrido: 12.749454736709595\n",
      "\n",
      "--- Starting trial: run-35\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00122: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0473 - mean_squared_error: 0.0473\n",
      "Loss: 0.047348324209451675 Tiempo transcurrido: 15.42040228843689\n",
      "\n",
      "--- Starting trial: run-36\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Loss: 0.03575265780091286 Tiempo transcurrido: 211.6512541770935\n",
      "\n",
      "--- Starting trial: run-37\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0455 - mean_squared_error: 0.0455\n",
      "Loss: 0.045523688197135925 Tiempo transcurrido: 108.52836227416992\n",
      "\n",
      "--- Starting trial: run-38\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 16}\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0761 - mean_squared_error: 0.0761\n",
      "Loss: 0.07610070705413818 Tiempo transcurrido: 67.1832823753357\n",
      "\n",
      "--- Starting trial: run-39\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00126: early stopping\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Loss: 0.03217129409313202 Tiempo transcurrido: 83.06011819839478\n",
      "\n",
      "--- Starting trial: run-40\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00089: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0645 - mean_squared_error: 0.0645\n",
      "Loss: 0.06454215198755264 Tiempo transcurrido: 22.204223155975342\n",
      "\n",
      "--- Starting trial: run-41\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00334: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Loss: 0.032300639897584915 Tiempo transcurrido: 45.84954237937927\n",
      "\n",
      "--- Starting trial: run-42\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00040: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0419 - mean_squared_error: 0.0419\n",
      "Loss: 0.04185826703906059 Tiempo transcurrido: 18.587400674819946\n",
      "\n",
      "--- Starting trial: run-43\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00069: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0432 - mean_squared_error: 0.0432\n",
      "Loss: 0.04320980980992317 Tiempo transcurrido: 17.402968168258667\n",
      "\n",
      "--- Starting trial: run-44\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00042: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0645 - mean_squared_error: 0.0645\n",
      "Loss: 0.06448283791542053 Tiempo transcurrido: 11.175946235656738\n",
      "\n",
      "--- Starting trial: run-45\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Loss: 0.03351152315735817 Tiempo transcurrido: 200.3425362110138\n",
      "\n",
      "--- Starting trial: run-46\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Loss: 0.03731463849544525 Tiempo transcurrido: 109.73465514183044\n",
      "\n",
      "--- Starting trial: run-47\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 16}\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0492 - mean_squared_error: 0.0492\n",
      "Loss: 0.049241602420806885 Tiempo transcurrido: 61.93728423118591\n",
      "\n",
      "--- Starting trial: run-48\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00117: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Loss: 0.03218847140669823 Tiempo transcurrido: 82.83660578727722\n",
      "\n",
      "--- Starting trial: run-49\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00200: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Loss: 0.03384045511484146 Tiempo transcurrido: 44.40141153335571\n",
      "\n",
      "--- Starting trial: run-50\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 16}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00159: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Loss: 0.039516013115644455 Tiempo transcurrido: 21.249553680419922\n",
      "\n",
      "--- Starting trial: run-51\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0548 - mean_squared_error: 0.0548\n",
      "Loss: 0.05484551191329956 Tiempo transcurrido: 11.71580195426941\n",
      "\n",
      "--- Starting trial: run-52\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00058: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Loss: 0.03626811504364014 Tiempo transcurrido: 14.025613784790039\n",
      "\n",
      "--- Starting trial: run-53\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 16}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00085: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Loss: 0.036285657435655594 Tiempo transcurrido: 21.304758310317993\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "datos = []\n",
    "\n",
    "for deep_layers in HP_LAYERS.domain.values:\n",
    "    for num_units in HP_NUM_UNITS.domain.values:\n",
    "        for learning_rate in HP_LEARNING.domain.values:\n",
    "            for batch_size in HP_BATCHSIZE.domain.values:\n",
    "                t = time.time()\n",
    "                hparams = {\n",
    "\n",
    "                    HP_LAYERS: deep_layers,\n",
    "                    HP_NUM_UNITS: num_units,\n",
    "                    HP_LEARNING: learning_rate,\n",
    "                    HP_BATCHSIZE: batch_size,\n",
    "                }\n",
    "                run_name = \"run-%d\" % session_num\n",
    "                print('\\n--- Starting trial: %s' % run_name)\n",
    "                print({h.name: hparams[h] for h in hparams})\n",
    "                score = run('logs/hparam_tuning3/' + run_name, hparams)\n",
    "                t = time.time()-t\n",
    "                session_num += 1\n",
    "                print(\"Loss:\", score, \"Tiempo transcurrido:\", t)\n",
    "            \n",
    "            datos.append([deep_layers, num_units, learning_rate, batch_size, score, t])\n",
    "\n",
    "print(session_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"historial_jla_tunning.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep size\", \"Num units\", \"Learning rate\", \"Batch size\", \"MSE\", \"Tiempo de ejecución\"])\n",
    "\n",
    "df.sort_values(by=[\"MSE\", \"Tiempo de ejecución\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep size</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Tiempo de ejecución</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.031342</td>\n",
       "      <td>82.791279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.032119</td>\n",
       "      <td>48.280042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Deep size  Num units  Learning rate  Batch size       MSE  \\\n",
       "0          3        200         0.0001          16  0.031342   \n",
       "1          2        200         0.0001          16  0.032119   \n",
       "\n",
       "   Tiempo de ejecución  \n",
       "0            82.791279  \n",
       "1            48.280042  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 11.843 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Tiempo de ejecución\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     82.791279\n",
       "1     48.280042\n",
       "2     45.849542\n",
       "3     45.363803\n",
       "4     46.584477\n",
       "5     21.304758\n",
       "6     12.350031\n",
       "7     21.249554\n",
       "8      9.379067\n",
       "9      8.372407\n",
       "10    15.420402\n",
       "11    61.937284\n",
       "12    11.175946\n",
       "13    67.183282\n",
       "14    70.970811\n",
       "15    45.628948\n",
       "16    48.462312\n",
       "17    48.255399\n",
       "Name: Tiempo de ejecución, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Tiempo de ejecución\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
