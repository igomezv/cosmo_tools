{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>delta</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135.689107</td>\n",
       "      <td>32.494632</td>\n",
       "      <td>23.87882</td>\n",
       "      <td>22.27530</td>\n",
       "      <td>20.39501</td>\n",
       "      <td>19.16573</td>\n",
       "      <td>18.79371</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144.826101</td>\n",
       "      <td>31.274185</td>\n",
       "      <td>24.77759</td>\n",
       "      <td>22.83188</td>\n",
       "      <td>22.58444</td>\n",
       "      <td>21.16812</td>\n",
       "      <td>21.61427</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142.188790</td>\n",
       "      <td>35.582444</td>\n",
       "      <td>25.26307</td>\n",
       "      <td>22.66389</td>\n",
       "      <td>20.60976</td>\n",
       "      <td>19.34857</td>\n",
       "      <td>18.94827</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>338.741038</td>\n",
       "      <td>-0.402828</td>\n",
       "      <td>22.13682</td>\n",
       "      <td>23.77656</td>\n",
       "      <td>21.61162</td>\n",
       "      <td>20.50454</td>\n",
       "      <td>19.25010</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345.282593</td>\n",
       "      <td>21.183866</td>\n",
       "      <td>19.43718</td>\n",
       "      <td>17.58028</td>\n",
       "      <td>16.49747</td>\n",
       "      <td>15.97711</td>\n",
       "      <td>15.54461</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        alpha      delta         u         g         r         i         z  \\\n",
       "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
       "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
       "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
       "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
       "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
       "\n",
       "    class  \n",
       "0  GALAXY  \n",
       "1  GALAXY  \n",
       "2  GALAXY  \n",
       "3  GALAXY  \n",
       "4  GALAXY  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/igomezv/nnogada/main/data/star_classification.csv\"\n",
    "data = pd.read_csv(url)\n",
    "cols = ['alpha','delta','u','g','r','i','z','class']\n",
    "data = data[cols]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        alpha      delta         u         g         r         i         z  \\\n",
      "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
      "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
      "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
      "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
      "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
      "\n",
      "   class  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n"
     ]
    }
   ],
   "source": [
    "data[\"class\"]=[0 if i == \"GALAXY\" else 1 if i == \"STAR\" else 2 for i in data[\"class\"]]\n",
    "print(data.head())\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function layers\n",
    "\n",
    "# f1 = lambda x: Dense(x, activation='relu')      #ReLU\n",
    "# f2 = lambda x: Dense(x, activation='elu')       #ELU\n",
    "# f3 = lambda x: keras.layers.LeakyReLU(0.3)      #LReLU\n",
    "# f4 = lambda x: Dense(x, kernel_initializer='lecun_normal', activation='selu')   #SELU\n",
    "\n",
    "# f_names = [\"ReLU\", \"ELU\", \"LReLU\", \"SELU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([3, 4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([200, 100]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-5,1e-4,1e-3, 0.01])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([128,256])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=50,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=1)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 50\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into X and Y and implement hot_ones in Y\n",
    "def prepare_dataset(data):\n",
    "    X, Y = np.empty((0)), np.empty((0))\n",
    "    X = data[:,0:7]\n",
    "    Y = data[:,7]\n",
    "    Y = to_categorical(Y, num_classes=3)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation and test sets\n",
    "X,Y = prepare_dataset(data)\n",
    "\n",
    "# Defines ratios, w.r.t. whole dataset.\n",
    "ratio_train = 0.8\n",
    "ratio_val = 0.1\n",
    "ratio_test = 0.1\n",
    "\n",
    "# Produces test split.\n",
    "x_, X_test, y_, Y_test = split(X, Y, test_size = ratio_test, random_state=0)\n",
    "\n",
    "# Adjusts val ratio, w.r.t. remaining dataset.\n",
    "ratio_remaining = 1 - ratio_test\n",
    "ratio_val_adjusted = ratio_val / ratio_remaining\n",
    "\n",
    "# Produces train and val splits.\n",
    "X_train, X_val, Y_train, Y_val = split(x_, y_, test_size=ratio_val_adjusted, random_state=0)\n",
    "\n",
    "# Normalize and scale the input sets.\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "X_val   = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:1])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[1:2])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[2:4])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[4:5])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(num_units, input_shape=(int(X_train.shape[1]),)))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(3, activation=tf.nn.softmax))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=1, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Accuracy:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.2      # probability for crossover\n",
    "    P_MUTATION = 0.5         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1  # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3392 - accuracy: 0.8738\n",
      "Accuracy: 0.8737999796867371 , Elapsed time: 92.76707792282104\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3452 - accuracy: 0.8786\n",
      "Accuracy: 0.878600001335144 , Elapsed time: 171.91798067092896\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3535 - accuracy: 0.8683\n",
      "Accuracy: 0.8683000206947327 , Elapsed time: 98.86318111419678\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3586 - accuracy: 0.8681\n",
      "Accuracy: 0.8680999875068665 , Elapsed time: 86.61473870277405\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 0.3654 - accuracy: 0.8677\n",
      "Accuracy: 0.8676999807357788 , Elapsed time: 101.60740518569946\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3668 - accuracy: 0.8690\n",
      "Accuracy: 0.8690000176429749 , Elapsed time: 152.8181025981903\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3757 - accuracy: 0.8638\n",
      "Accuracy: 0.8637999892234802 , Elapsed time: 94.19921779632568\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3269 - accuracy: 0.8787\n",
      "Accuracy: 0.8787000179290771 , Elapsed time: 66.33406233787537\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin     \tavg     \tmax     \n",
      "0  \t8     \t0.326869\t0.353901\t0.375668\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3515 - accuracy: 0.8730\n",
      "Accuracy: 0.8730000257492065 , Elapsed time: 163.99112844467163\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3414 - accuracy: 0.8752\n",
      "Accuracy: 0.8751999735832214 , Elapsed time: 127.53065490722656\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3262 - accuracy: 0.8788\n",
      "Accuracy: 0.8787999749183655 , Elapsed time: 65.03994345664978\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3431 - accuracy: 0.8713\n",
      "Accuracy: 0.8712999820709229 , Elapsed time: 127.20990633964539\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3223 - accuracy: 0.8836\n",
      "Accuracy: 0.8835999965667725 , Elapsed time: 57.0923969745636\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t5     \t0.322291\t0.338134\t0.366778\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3438 - accuracy: 0.8713\n",
      "Accuracy: 0.8712999820709229 , Elapsed time: 133.05114269256592\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3390 - accuracy: 0.8767\n",
      "Accuracy: 0.8766999840736389 , Elapsed time: 70.54426288604736\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3380 - accuracy: 0.8736\n",
      "Accuracy: 0.8736000061035156 , Elapsed time: 71.20755934715271\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3476 - accuracy: 0.8751\n",
      "Accuracy: 0.8751000165939331 , Elapsed time: 64.00270676612854\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3268 - accuracy: 0.8802\n",
      "Accuracy: 0.8802000284194946 , Elapsed time: 73.05279874801636\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3199 - accuracy: 0.8824\n",
      "Accuracy: 0.8823999762535095 , Elapsed time: 73.3400285243988\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3312 - accuracy: 0.8781\n",
      "Accuracy: 0.8780999779701233 , Elapsed time: 83.38652467727661\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t7     \t0.319948\t0.333584\t0.34758 \n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3284 - accuracy: 0.8808\n",
      "Accuracy: 0.8808000087738037 , Elapsed time: 76.30180072784424\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3502 - accuracy: 0.8721\n",
      "Accuracy: 0.8720999956130981 , Elapsed time: 65.87167596817017\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t2     \t0.319948\t0.326522\t0.350247\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3200 - accuracy: 0.8822\n",
      "Accuracy: 0.8822000026702881 , Elapsed time: 85.8351457118988\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3430 - accuracy: 0.8753\n",
      "Accuracy: 0.8752999901771545 , Elapsed time: 74.438791513443\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3365 - accuracy: 0.8789\n",
      "Accuracy: 0.8788999915122986 , Elapsed time: 143.46771454811096\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.5505 - accuracy: 0.7894\n",
      "Accuracy: 0.7893999814987183 , Elapsed time: 72.24866962432861\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3260 - accuracy: 0.8805\n",
      "Accuracy: 0.8805000185966492 , Elapsed time: 57.15156149864197\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t5     \t0.319948\t0.355535\t0.550519\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3490 - accuracy: 0.8755\n",
      "Accuracy: 0.8755000233650208 , Elapsed time: 112.25765204429626\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3159 - accuracy: 0.8843\n",
      "Accuracy: 0.8842999935150146 , Elapsed time: 143.67027187347412\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8797\n",
      "Accuracy: 0.8797000050544739 , Elapsed time: 61.37342357635498\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t3     \t0.315929\t0.326887\t0.349005\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.5282 - accuracy: 0.7968\n",
      "Accuracy: 0.7968000173568726 , Elapsed time: 84.79557871818542\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3396 - accuracy: 0.8755\n",
      "Accuracy: 0.8755000233650208 , Elapsed time: 58.17303395271301\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3349 - accuracy: 0.8754\n",
      "Accuracy: 0.8754000067710876 , Elapsed time: 128.49021339416504\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3258 - accuracy: 0.8793\n",
      "Accuracy: 0.8792999982833862 , Elapsed time: 75.53325700759888\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t4     \t0.315929\t0.350037\t0.528203\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3295 - accuracy: 0.8768\n",
      "Accuracy: 0.876800000667572 , Elapsed time: 76.69055342674255\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3370 - accuracy: 0.8740\n",
      "Accuracy: 0.8740000128746033 , Elapsed time: 69.5280601978302\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3328 - accuracy: 0.8792\n",
      "Accuracy: 0.8791999816894531 , Elapsed time: 80.84275555610657\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3244 - accuracy: 0.8809\n",
      "Accuracy: 0.8809000253677368 , Elapsed time: 143.4969403743744\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.315929\t0.323435\t0.337025\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3343 - accuracy: 0.8783\n",
      "Accuracy: 0.8783000111579895 , Elapsed time: 78.19048810005188\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4939 - accuracy: 0.8115\n",
      "Accuracy: 0.8115000128746033 , Elapsed time: 160.78047251701355\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3204 - accuracy: 0.8856\n",
      "Accuracy: 0.8855999708175659 , Elapsed time: 111.45352268218994\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t3     \t0.315929\t0.341031\t0.493854\n",
      "\n",
      "--------------- Starting trial: 42 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8802\n",
      "Accuracy: 0.8802000284194946 , Elapsed time: 143.70930314064026\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 43 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3228 - accuracy: 0.8838\n",
      "Accuracy: 0.8838000297546387 , Elapsed time: 110.86671948432922\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 44 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3308 - accuracy: 0.8793\n",
      "Accuracy: 0.8792999982833862 , Elapsed time: 103.79693531990051\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t3     \t0.315929\t0.32063 \t0.331786\n",
      "\n",
      "--------------- Starting trial: 45 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3169 - accuracy: 0.8863\n",
      "Accuracy: 0.8863000273704529 , Elapsed time: 162.43839073181152\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 46 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3133 - accuracy: 0.8886\n",
      "Accuracy: 0.8885999917984009 , Elapsed time: 93.5187554359436\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 47 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3231 - accuracy: 0.8809\n",
      "Accuracy: 0.8809000253677368 , Elapsed time: 94.99594306945801\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 48 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3290 - accuracy: 0.8797\n",
      "Accuracy: 0.8797000050544739 , Elapsed time: 67.06326341629028\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 49 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.5205 - accuracy: 0.8002\n",
      "Accuracy: 0.8001999855041504 , Elapsed time: 110.24073052406311\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 50 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3231 - accuracy: 0.8863\n",
      "Accuracy: 0.8863000273704529 , Elapsed time: 89.80550837516785\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t6     \t0.313251\t0.344701\t0.52045 \n",
      "-- Best Individual =  [1, 0, 1, 0, 1]\n",
      "-- Best Fitness =  0.3159293830394745\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABz5klEQVR4nO2dd1iTV/vHvwkh7C0kTCciMqOggqvgwIpWXFVb9VVfq7VabdW21lbbWkdb7XDV1p/W1u7WWYvVOtpacGKQqKDiQGYCgiAzhOT8/njeRJCRANmcz3XlgjzjnPvkSZ77Off3nPuwCCEEFAqFQqE8AdvQBlAoFArFOKEOgkKhUChNQh0EhUKhUJqEOggKhUKhNAl1EBQKhUJpEuogKBQKhdIk1EGYKfn5+RAIBJDL5YY2BbGxsTh79qyhzdArP/zwA6KjoyEQCPDw4UMIBALk5OQY2iyKDpg7dy4OHjxoaDN0AnUQrSQ2NhbBwcEoKSlpsD0hIQEBAQHIzc3Vaf0HDhxAQEAA1q9f32D7yZMnERAQgBUrVgAAvLy8kJqaCgsLC53aoy22bt2KgIAApKWlGdqUdiOTyfDBBx/gq6++QmpqKlxcXJCamgpfX18AwIoVK/Dpp58a2Erj4erVq5g/fz4iIyMRERGB0aNH49NPP0VZWZmhTWvE1q1bsXz58gbbdu3ahfHjxxvIIt1CHUQb8Pb2RmJiour9zZs3UV1drbf6/fz88Mcff6Curk617dChQ+jSpYvebNAmhBAcOnQIzs7OOHTokE7q0GdPqri4GFKpFD169NBbnaZA/e+rEqFQiJkzZ6JPnz74448/kJKSgl27dsHCwgI3btwwuH0dHeog2sC4ceMa3MgOHTqEhISEBsf8/fffSEhIQJ8+fTB06FBs3bpVte/o0aOIjY1FRUUFAOCff/7BwIEDG/VKmqNTp07o2bMnkpKSAAClpaVITU1FbGys6pjc3FwEBASovvQzZszAZ599hqlTp0IgEGDOnDnN1ldWVob58+djwIABiIyMxPz58yEWi1X71ZV16NAhxMTEoH///tixY4fa9qSkpKCoqAhvvfUWjh49itraWgBM1/27775rcOwzzzyDP//8EwBw584dzJ49G/369UNcXByOHj2qOm7FihV455138MILLyA8PBwXLlxo8Zo8aff27dsbhMYUCgV27tyJ4cOHo3///liyZAlKS0sbteXevXsYNWoUACAyMhIzZ84EAAQEBOD+/fv4+eefceTIEezevRsCgQAvvvgiAKZnunv3bowdOxZ9+/bFK6+8AqlUqir3r7/+wrhx4xAREYGpU6c2uHnu3LkTgwcPhkAgQFxcHM6dOwcAEIlEmDBhAvr06YPo6Ghs2LCh2Wvwyy+/YMSIEejXrx9efPFFSCQSAMA777yDDz/8sMGxCxYswJ49ewAAEokEL7/8MgYMGIDY2Fjs3btXddzWrVuxePFiLF++HH369GkyDLNx40ZMmDAB8+fPR6dOnQAwvd/Fixejf//+quP27duHp59+GpGRkfjvf/+LvLw81b6AgAD8+OOPGDlyJCIiIvDee++hfoIIded+//33GDlyJEaOHAkAWLt2LYYOHYo+ffpgwoQJSElJAQCcOXMGX375Jf744w8IBAI888wzAJjfw6+//gqA+Z58/vnniImJQVRUFF5//XWUl5cDePybPHjwIJ566qlGv4/WXC+9QSitIiYmhiQnJ5ORI0eS27dvk7q6OjJ48GCSm5tLevbsSXJycgghhJw/f57cuHGDyOVykpGRQaKiosiJEydU5SxdupS88cYbpKSkhAwcOJCcPn1ao/r3799Ppk6dSn777TeyZMkSQggh3333HVm1ahX55JNPyBtvvEEIISQnJ4f07NmTyGQyQggh06dPJ8OGDSN3794l1dXVZPr06WTjxo1N1lFSUkKOHTtGqqqqSHl5OXn55ZfJggULVPtbKiszM5OEh4eTixcvEqlUStavX08CAwNJcnJys2168803yeLFi0ltbS3p168fOXbsGCGEkIMHD5IpU6aojsvMzCR9+/YlUqmUVFZWkiFDhpB9+/YRmUxGrl+/Tvr160cyMzMJIYS88cYbpE+fPiQlJYXI5XJSU1PT4jVR2n3p0iUilUrJBx98QHr37q2y++uvvyaTJ08mBQUFRCqVklWrVpFXX321yfY8+dkTQkjPnj1JVlaWyrZPPvmkwTkxMTFk4sSJRCwWk4cPH5JRo0aRH374gRBCyPXr18mAAQPIlStXSF1dHTlw4ACJiYkhUqmU3LlzhwwZMoSIxWJV3ffv3yeEEPLss8+SgwcPEkIIqaioIKmpqU3ae/bsWdKvXz9y7do1IpVKyZo1a8hzzz1HCCHk4sWLZMiQIUShUBBCCCktLSUhISFELBYTuVxOxo8fT7Zu3UqkUinJzs4msbGx5MyZM4QQQrZs2UJ69+5NTpw4QeRyOamurm5Qb2VlJenVqxc5f/58k3YpOXHiBBk+fDi5ffs2kclkZPv27Q2+Fz179iTz5s0jZWVlJC8vj/Tv35/8888/Gp87a9Ys8vDhQ5V9hw4dIiUlJUQmk5Hdu3eT6OhoUlNTo2rTsmXLGtg3ffp08ssvvxBCCPn111/J8OHDSXZ2NqmoqCALFy4ky5cvV12bnj17krfeeotUV1eTjIwMEhQURG7fvt2q66VPaA+ijSh7EcnJyejevTt4PF6D/f3790dAQADYbDZ69eqF+Ph4XLx4UbX/nXfewfnz5zFz5kzExsYiJiamVfWPGDECFy9eRHl5OQ4fPoxx48apPWfChAno2rUrrK2tMWrUKGRkZDR5nIuLC+Li4mBjYwN7e3ssWLAAly5d0qisY8eO4amnnkJkZCS4XC6WLFkCNrv5r1l1dTWOHTuGsWPHwtLSEnFxcare2fDhw3Hjxg3VE9+RI0cwYsQIcLlc/P333/D29sbEiRPB4XDQu3dvxMXF4dixY6qyhw0bhr59+4LNZsPKyqrFa3Ls2DHExMQgIiICXC4XixcvBovFUpX1008/4dVXXwWfzweXy8WiRYtw/PhxrYYlZsyYAR6PB2dnZ8TExKg+059//hlTpkxBWFgYLCwsMH78eFhaWuLKlSuwsLBAbW0t7ty5A5lMBh8fH/j5+QEAOBwOsrOzUVJSAjs7O4SHhzdZ75EjRzBx4kQEBQWBy+Vi6dKluHLlCnJzcxEREQEWi6V6ij5+/DjCw8PB4/Fw9epVlJSUYNGiReByufD19cWzzz7boCcXHh6O4cOHg81mw9raukG9jx49gkKhUPUcAOCjjz5CREQEwsPD8fnnn6s++3nz5qF79+7gcDh48cUXkZGR0aAn8MILL8DR0RFeXl7o37+/qoelybnz5s2Ds7Ozyr5x48bBxcUFHA4Hc+bMQW1tLe7du6fRNTxy5AhmzZoFX19f2NnZYenSpTh69GiD78miRYtgbW2NXr16oVevXipbNb1e+oRjaANMlXHjxmH69OnIzc1t8uaclpaGTZs2ITMzEzKZDLW1tarQAwA4Ojpi1KhR2LNnD7Zs2dLq+q2trTF06FB8/vnnKC0tRd++fXHmzJkWz3F3d1f9b2Njg6qqqiaPq66uxoYNG/Dvv/+qhMLKykrI5XKV6N1cWYWFheDz+ap9tra2cHZ2btamEydOgMPhYMiQIQCAsWPHYvbs2SgpKYGrqyuGDh2KxMREzJs3D7///jvWrl0LAMjLy4NIJEJERISqLLlcrur2A4Cnp2eDulq6Jk/abWNj08Du/Px8LFy4sIGzY7PZKC4ubvRw0Fae/EwLCwtVdR86dKhBuE0mk6GwsBD9+vXDypUrsXXrVty+fRuDBg3CihUrwOPxsG7dOmzZsgVPP/00fHx8sGjRoiYfRAoLCxEUFKR6b2dnB2dnZ0gkEvj4+GD06NH4/fffERkZiSNHjqg+47y8PBQWFja6BvXf1/9Mn8TR0RFsNhtFRUXo3r07AOD111/H66+/juXLl6t0o/z8fKxfv75BqIsQAolEAm9v7yY/u8rKSo3PffJ7snv3buzbtw+FhYVgsVioqKjAw4cPm21HfQoLC1XlAoxeWVdXh+LiYtW2+g6x/m9H0+ulT6iDaCPe3t7w8fHBP//8g3Xr1jXav2zZMkyfPh27du2ClZUV1q1b1+BLlpGRgf3792PMmDFYu3Ytdu/e3WobEhIS8J///AeLFi1qV1ue5KuvvsK9e/fwyy+/wN3dHRkZGUhISGgQ120ODw8P3LlzR/W+urq6yVi9kkOHDqGqqkr1QyCEQCaT4ciRI/jPf/6DMWPGYNu2bYiMjIRUKlXFpT09PREZGamKhWtCS9fEw8OjwVNiTU1NA7v5fD7Wr1+Pvn37alxfc9TvmWiCp6cnXnzxRSxYsKDJ/WPHjsXYsWNRUVGB1atXY9OmTdi4cSO6dOmCTz75BAqFAn/++ScWL16MCxcuwNbWtsH5Hh4eDZ6oq6qqUFpaqnJ8Y8aMwZw5czBv3jyIRCJs375dZZePj49KE2ptW21tbREWFoYTJ05gwIABattf3/lriibn1rdRKZJ//fXX8Pf3B5vNRmRkpOq7r+7aPflZ5ufng8PhwM3NrYGO1xSaXi99QkNM7WDdunX45ptvmryAlZWVcHJygpWVFUQiEX7//XfVPqlUitdeew2vvvoqNmzYgMLCQnz//feq/TNmzGgkoDZFv379sGfPHkyfPl07Dapnu5WVFRwdHVFaWopt27ZpfG5cXBz+/vtvpKSkoLa2Flu2bIFCoWjyWIlEgnPnzuGLL77AoUOHcOjQIRw+fBgvvPACDh8+DAAYOnQo8vPzsWXLFowePVr1BP/UU08hKysLhw4dgkwmg0wmg0gkauCcmmpXc9ckLi4Op0+fhlAoRG1tLbZu3drAIU6bNg2fffaZ6sdfUlKCkydPavy51MfNza1Vw6EnT56Mn376CWlpaSCEoKqqCn///TcqKipw9+5dnDt3DrW1teByubCyslJ9RocPH0ZJSQnYbDYcHR0BoMlw35gxY3DgwAFkZGSgtrYWn3zyCUJDQ+Hj4wMA6N27N1xcXPD2229j0KBBqrJCQ0NhZ2eHnTt3oqamBnK5HLdu3YJIJNK4bcuXL8f+/fuxc+dO1VO2WCxu8PlMnToVO3fuRGZmJgCgvLwcf/zxh0blt/bcyspKWFhYwNXVFXV1ddi2bZtqMAnAXLu8vLxmv9NjxozBN998g5ycHFRWVuLTTz/F008/DQ5H/bO4ptdLn1AH0Q78/PwQEhLS5L533nkHW7ZsgUAgwPbt2/H000+r9n388cfg8/l47rnnwOVysXHjRmzevBlZWVkAgIKCAvTp00dt/SwWC1FRUS2GcNrCf/7zH0ilUgwYMABTpkzB4MGDNT7X398fq1evxvLlyzF48GA4Ojo2G2Y4fPgwAgMDMWjQILi7u6teM2bMwM2bN3Hr1i1wuVyMGDECZ8+exZgxY1Tn2tvbY/fu3Th69CgGDx6MQYMGYdOmTaoRUE3R0jXx9/fHqlWrsHTpUgwePBi2trZwdXUFl8sFAJVWNGfOHAgEAjz77LOtuhHWZ9KkSbh9+zYiIiLw0ksvqT0+JCQE77//PtasWYPIyEiMHDkSBw4cAADU1tbi448/Rv/+/TFo0CCUlJRg6dKlAIB///0X8fHxEAgEWLduHT799NNGOgAAREdHY8mSJXj55ZcxaNAg5OTkNJqnMWbMmEbXwMLCAl988QVu3LiBYcOGYcCAAXj77bcb3FDVERERgW+++QaXLl1CXFwcIiIiMHfuXPTv31/14DNixAjMnTsXS5cuRZ8+fTBmzBi14VQlrT130KBBGDx4MOLi4hAbGwsrK6sGIShlSLJ///5Nzn2YOHEinnnmGUyfPh3Dhg0Dl8vFqlWrNLJV0+ulT1hEk7gBRW+IxWK88sor+OmnnwxtSoemsrISkZGROH78uGqCG4XS0aAOgkL5H6dPn0ZUVBQIIfjggw8gEolw8ODBVmsGFIq5QENMFMr/OHXqFAYPHozBgwfj/v37+OSTT6hzoHRoaA+CQqFQKE1CexAUCoVCaRKzmQdx5coVWFlZtfl8qVTarvNNkY7W5o7WXoC2uaPQnjZLpdJmZ22bjYOwsrJCYGBgm8/PyMho1/mmSEdrc0drL0Db3FFoT5ubS7kD0BAThUKhUJqBOggKhUKhNAl1EBQKhUJpErPRIJpCJpMhNzcXNTU1Gh3bUizO1LC2toaPjw8sLS0NbQqFQjFRzNpB5ObmwsHBAV26dFE74am6uho2NjZ6sky3EEJQXFyM3NxcdO3a1dDmUCgUE8WsQ0w1NTVwc3PrcLNhWSwW3NzcNOo5USgUSnPotAdx5swZrFu3DgqFApMnT8a8efMa7D9w4AA++ugjVd756dOnY/LkyQCAwMBA9OzZEwCT0/2LL75okw0dzTko6ajtplAo2kNnDkIul2PNmjXYs2cPeDweJk2ahNjYWPTo0aPBcaNHj8bq1asbnW9tba1aE4BCaS+VtZXYf3c/3ur1FnWeFIqG6CzEJBKJ0LlzZ/j6+oLL5SI+Ph6nTp3SVXVGS0BAAJYvX656X1dXhwEDBmD+/PkAmARxO3fuNJR5HYYfrv6AVSmrcD73vKFNoVBMBp31ICQSSYOFYng8XpMLrPz555+4dOkSunbtijfffFO1OIdUKsWECRPA4XAwb948DB8+vMX6pFJpo1FIMpkM1dXVGtlLCNH42NZgY2ODmzdv4uHDh7C2tkZSUhLc3d0hl8tRXV2N6OhoREdH66RudSOzampqzGrkVkv8lfEXAOB42nE4Vzgb1hg90pGusRLaZu1h0FFMMTExGDNmDLhcLn766Se88cYb2Lt3LwDgr7/+Ao/HQ05ODv7zn/+gZ8+e8PPza7asplJtZGRkaDwySVejmFgsFmJiYnDhwgWMGjUKJ06cwNixY3H58mXY2NjgwIEDuHbtGlavXo0VK1bA3t4e165dQ1FREV577TXVClZtwdLSssXp9x0pJUHO+RwAQCGrsMO0GehY11gJbXPrz20OnTkIHo/XYJFuiUSiEqOVuLi4qP6fPHkyNm7c2OB8APD19UW/fv2Qnp7eooNQx969wFdfNb9foeCitcu/zpkDzJyp/rjRo0fj888/R0xMDG7evImJEyfi8uXLTR5bWFiIH374AXfv3sWCBQva5SAoDIQQiCRM71X5l0KhqEdnGkRISAiysrKQk5OD2tpaJCYmIjY2tsExhYWFqv9Pnz6N7t27AwDKyspUawuXlJRAKBQ2ErdNiV69eiE3Nxe///47hg4d2uKxw4cPB5vNRo8ePfDgwQM9WWjeZJVm4ZH0Eewt7SGSiECXQKFQNENnPQgOh4PVq1dj7ty5kMvlmDhxIvz9/bF582YEBwdj2LBh+Pbbb3H69GlYWFjAyckJGzZsAADcuXMH77zzDlgsFggheOGFF9rtIGbObPlpv7q6VqcT5WJjY/HRRx9h7969KC0tbfY4LperMxs6KmmSNADASJ+ROHDvALJKs9DVhU4gpFDUoVMNYujQoY2emJcsWaL6f9myZVi2bFmj8/r06YMjR47o0jS9M2nSJDg6OiIgIAAXLlwwtDkdCpFEBBZYGOM3BgfuHYBIIqIOgkLRALOeSW1M8Pl8zNREsKBonTRJGnq49kCIawhYYKl6FBQKpWXMOheTMZCamtpoW//+/dG/f38AwIQJEzBhwgQAwAcffKD2XErrSROnQeApgJ2lHbq7dqdCNYWiIbQHQTFrKmorcOfhHYTxwgAAobxQ2oOgUDSEOgiKWXNVchUA4xgAIIwXhjsld1BRW2FIsygUk4A6CIpZo+wt1O9BEBBcL7xuSLMoFJOAOgiKWSOSiOBs7Qw/J2aSpdJR0DAThaIe6iAoZk2aJA2hvFBVBtfOzp3hwHWgQjWFogHUQVDMFgVRQCQRIdQjVLWNzWJToZpC0RDqIHSMunTfFN1x7+E9VNRWIIwf1mB7KC+UptygUDSAOggdY2tri8zMTNXyn8nJyY2SFlJ0gzKMpNQdlITyQvFI+gjZZdmGMItCMRmog9ADQ4cOxd9//w0ASExMRHx8vGpfVVUV3nzzTUyaNAkJCQk4efIkACA3NxfPPfccxo8fj/Hjx0MoFAIALly4gBkzZmDx4sUYNWoUli1bRp+EmyFNkgY2i40gj6AG26lQTaFoRoeZSb03bS++Sm0+37dCoQC7lfm+5wjmYGaY+vQZLaX7/uKLLzBgwABs2LABjx49wuTJkxEdHQ03Nzfs2bMHVlZWyMrKwtKlS3HgwAEAQHp6OhITE+Hh4YFp06bh8uXLiIiIaJXtHYE0SRr8Xf1ha2nbYHuwRzAApofxTMAzhjBNL/yR+Qfe/etdJPVMgqWFpaHNoZggHcZBGJKW0n0nJSXh9OnT+Op/i1VIpVIUFBTAw8MDa9aswY0bN8Bms5GVlaU6JzQ0VLVaX69evZCXl0cdRBOIJCJEeDX+XBysHNDdpbvZ9yB+Tf8VF4su4saDGwjhhRjaHIoJ0mEcxMywmS0+7etqRTklLaX73rJlC7p169Zg29atW9GpUyccPnwYCoUCoaGPR+LUTwluYWEBuVyuM7tNlUfSR7j78C7mhM9pcr9SqDZnUvJTAACp4lTqIChtgmoQemLSpElYuHAhAgICGmwfNGgQvvvuO5WOkJ6eDgAoLy+Hu7s72Gw2Dh8+TJ1AK1Gm2HhyBJOSUF4oMoszUSWr0qdZeqOythLXi5jZ4qkFNOkjpW1QB6Enmkv3/dJLL6Gurg7PPPMM4uPjsXnzZgDAc889h4MHD+KZZ57B3bt3YWtr2+hcSvM0N4JJSRgvDAQE1wqv6dMsvXFFfAUKooAFywKpYuogKG2jw4SYDIW6dN/W1tZYs2ZNo2O6dOnSYNGk1157rdG5ALB69Wptm2wWpEnS4GztDB9Hnyb3K5P3iSQi9PPup0/T9IIyvDTUcyguiy+DEKKaTU6haArtQVDMkjRJGsJ4Yc3eFLu6dIU91x5pYvMUqlMKUuDl4IUhnkNQJi3DvdJ7hjaJoiMyijJwo/SGTsqmDoJidiiIAlclV5sNLwFMyo0QjxCICs1TqE7JT0GEVwQCXQIBUB3CnHnhyAv47OpnOimbOgiK2XH34V1UyiqbFaiVhPJCkSZOM7uJho+kj3DzwU1EeEagp1NPqkOYMXKFHKniVHS276yT8qmDoJgdyrCRUmdojjBeGMqkZch5lKMPs/SGsEAIAoJI70hYWVgh0D2QOggz5VbxLVTJqtDbpbdOyqcOgmJ2qFJsuAe1eFx9odqcUArUfT37AgDC+eE0xGSmCAuYFDyBzoE6KZ86CIrZIZKIEOAWABvLlic+KiePmZtQnZKfgs5OneFu5w4AEPAFKKgogKRCYmDLKNomVZwKa441ujl2U39wG6AOQsfQdN/6R7lIkDocrRzR1bmr2QnVSoFaiYAvAMDMjaCYF8ICIUI8QsBh62bGAnUQOoam+9YvZTVlyCrNanEEU33C+GFm1YN4WP0Qdx7eaeAgwvnhAEB1CDODEIJUcSr6ePbRWR2tchAKhQIVFRW6ssVsaSndt0gkwpQpU5CQkICpU6fi7t27AICvv/4ab775JgDg5s2bGDNmDKqrq/Vuu6lxtbDlFBtPEuoRiswS80m5odQf6jsIFxsXdHHuQh2EmZFVmoXSmlJVD1EXqO2XLFu2DO+99x7YbDYmTZqEiooKzJw5E3PnztWZUTph717gq+bTfXMVCqCV6b4xZw7QRPqMJ2kp3Xe3bt3w/fffg8Ph4OzZs/j000+xdetWzJw5EzNmzMCJEyewY8cOvPfeezpNJmguaDqCSUkoLxQKokB6UXqTmV9NjScFaiUCvoAK1WaGUqDu49kHeKSbOtTeEW/fvg17e3ucPHkSQ4YMwalTp3D48GHdWGOmtJTuu7y8HEuWLMGYMWOwYcMGZGZmAgDYbDY++OADvP766+jXrx/69u3bVNGUJ0iTpMHVxhXeDt4aHa/saZhLmCmlIAU9XHvAxcalwXYBX4DMkkyUS8sNZBlF26SKU2HBstBppl61PYi6ujrIZDKcPHkS06dPh6WlpWnmdJk5s8Wn/VoDpfvevHkz+vfvj+3btyM3N7dBQr+srCzY2tqisLBQZ3aZG+pSbDxJN5dusLO0M5uhrin5KYj2jW60XeDJhCHSJGkY5DdI32ZRdICwQIje7r1hzbHWWR1qexBTpkxBbGwsqqurERkZiby8PNjb2+vMIHOluXTf5eXlKtH64MGDDbavXbsW3333HUpLS3Hs2DG92muKyBVyXCu8prFADfwv5QYvxCwWDyqsLER2WTYiPBuHypRxahpmMh90LVADGjiImTNn4t9//8X//d//gcViwdvbG3v37tWpUeZIc+m+586di08++QQJCQmoq6tTbV+/fj2ef/55dO3aFevWrcPHH3+M4uJifZpsctx5eAdVsiqN9QcloR7M4kGmnnKjKYFaiZeDF9xt3alQbSYUlBdAXCHWqUANaBBi+uabbzBx4kTY2dnhrbfeQkZGBpYtW4ZBg2g3VRPUpfsWCAQ4fvy4at+rr74KANiwYYNqm6enJ06cOKFjS00fpY6g6QgmJaG8UOwU7kReeV6z6cFNgZT8FLDAavKpksViQeApoA7CTFBeR4P3IPbv3w97e3skJSXh0aNH+Oijj/Dxxx9rVPiZM2cQFxeHESNGYOfOnY32HzhwAAMGDMC4ceMwbtw4/Prrr6p9Bw8exMiRIzFy5MgGoRcKpTlEEhEsWBbo7d66vDTmIlSn5KegV6decLByaHK/gC/A9cLrqJXX6tkyirZRjmBq7cNQa1Hbg1B2u//55x+MGzcO/v7+GnXF5XI51qxZgz179oDH42HSpEmIjY1Fjx49Ghw3evToRovelJaWYtu2bdi/fz9YLBYmTJiA2NhYODk5taZtlA5GmiQNAZ0CWi3ahXgwo0BEEhHie8arOdp4SclPwfBuw5vdH84Ph0whw/XC6yrRmmKaCAuE8Hf1h6OVo07rUduDCA4Oxpw5c3DmzBkMGjQIFRUVYGswX0AkEqFz587w9fUFl8tFfHw8Tp06pZFRSUlJGDhwIJydneHk5ISBAwfi33//1ejcJzH1uHJb6YjtVo5gai1O1k7o4tzFpIXq/PJ8FFQUtDiXQyVU0zCTyaMPgRrQoAexbt06ZGRkwNfXFzY2Nnj48CHWr1+vtmCJRAI+n696z+PxIBI1Hkr4559/4tKlS+jatSvefPNNeHp6NnmuRNJyojGpVIqMjIwG2+rq6lBQUABnZ2e1wx4JIWYzU5kQgtLSUtTV1TX6TOpTU1PT4n5T4lHtI2SXZWOS36Rm29RSe7vadsWlnEsm+3mczjsNAOhU26lBG+q3WUEUsOHY4FT6KURZRxnETn1gTt/rpiiVliKrNAsTfCeo2qmrNqt1ECwWC7dv38Zff/2FRYsWobq6GrW12olhxsTEYMyYMeByufjpp5/wxhtvtHmElJWVFQIDG6a8lclkyM3Nxf3799WeL5PJYGlp2aa6jRFra2v06tWrxTZlZGQ0+sxMlTP3zwAAhocMR6B/021qqb2DxIPwz7//oEuPLmqzwBojPxT8AAuWBRIGJMDW0la1/ck2C84LkF2bbTbXvSnM6XvdFKfvMQ8Do8JGIbA70872tLklx6LWQbz77rtgs9k4f/48Fi1aBDs7O7z88svYv39/i+fxeDyIxWLVe4lE0ihJnYvL49mekydPxsaNG1XnXrx4scG5/fq1fmF5S0tLdO3aVaNjzf1LZe60dQSTkvopN/p6md6s9ZSCFAR5BDVwDk0h4AvwTdo3UBAF2Cyaq9MUUQrU+tCR1H5DRCIR3nnnHVhZWQEAnJycIJPJ1BYcEhKCrKws5OTkoLa2FomJiYiNjW1wTP0ZwqdPn0b37t0BAIMGDUJSUhLKyspQVlaGpKQkOqyW0iIiiQidbDvB096zTeeb8uJBhBAmxXcTE+SeRMAXoKK2AndK7ujBMoouSBWnwtfRF51sO+m8LrU9CA6HA7lcrorhl5SUaCRSczgcrF69GnPnzoVcLsfEiRPh7++PzZs3Izg4GMOGDcO3336L06dPw8LCAk5OTqqx/87OznjppZcwadIkAMDChQvh7OzcjmZSzB3lGhBtTQPT3aU7bC1tTVKozi7LxoOqBxolG1Q+daaKU+Hv5q9r0/ROWU0ZcityEQjzjQYIC4R6G4Wm1kHMmDEDCxcuRHFxMT799FMcO3YMr7zyikaFDx06tFFyuiVLlqj+X7ZsGZYtW9bkuZMmTVI5CAqlJZQpNl6MeLHNZViwLRDsEWySPYiWZlA/SZB7EDhsDlILUvFs0LO6Nk3vvHbiNRy4fgCFEYVmGUKrqK3AzQc3MTVoql7qU+sgnnnmGQQFBeH8+fMghODzzz9XhYIoFGMgsyQT1XXVbRriWp8wXhj2Z+wHIcSkElJeyr8ES7alRilGrDhWCHIPMtuhrn9l/YViaTFuPLjR6gmTpoBIIgIB0csQV0DDBYO6dOmC4cOHIzY2FjY2NsjPz9e1XRSKxiif+ts7qzSUF4qS6hLkl5vW9zslPwWhvFBYcaw0Ol6ZcsPc5sqIK8S4XXIbAHAu55yBrdEN+hSoAQ16EN9++y22bduGTp06NdAejhw5olPDKBRNSROngcPmILBT++LO9YVqb0fN1pMwNEqBemqw5iEHAV+Ar698jYKKAng5eOnQOv2SnJ0MAGCBhbM5Z/HfPv81sEXaJ7UgFe627hqvd9Je1DqIvXv34tixYw2GpFIoxkSaJA29OvXS+Am6OZQOIk2Shqf9n9aGaTrnzsM7KJOWtWo1vPqpv83KQeQkw5pjjb5ufXE296yhzdEJQjEjUOsrBKo2xMTn8+Hg0HTyLwrFGBBJRO3WHwDA2doZfk5+JiVUt0agVqIMxZmbDpGUnYR+3v0Q6RGJGw9uoKS6xNAmaRVpnRTXC6+jD18/+gOgQQ/C19cXM2bMwFNPPQUul6vaPnv2bJ0aRqFoQkl1CXIe5bR6DYjmCOOFmdRQ10t5l2DNsUaQe5DG5zhaOaK7S3ezchCVtZUQFgjxxsA3EMBhFuU6n3seo/1HG9gy7XG96DpkCpleEy2q7UF4eXlh4MCBkMlkqKysVL0oFGNAJVBroQcBMGGmmw9uoqauRivl6ZqUghSE88NhadG6NDECT4FZrS53Me8i5ESOgX4DEeQaBAuWBc7mmFeYSSlQ62sEE6BBD6J79+54+umG8dg//vhDZwZRKK1BWyOYlITxwiAncmQUZRh9Smy5Qg5hgRCzwma1+lwBX4B96ftQWlMKZ2tnrdumb5Kyk8ACC1E+URBniRHOD8e5XPMayZRakApHK0d0c+mmtzrV9iCaWuinqW0UiiFIE6fB3dYdPDue+oM1oL5QbezcKr6FitqKVukPSpRCtakvkqQkOScZwR7BcLFhBtNE+0bjQu4F1Cnq1JxpOgjFQoTzw/U6AbDZHsQ///yDM2fOQCKRYO3atartFRUVsLCw0ItxFIo60iRpCOOHaW1URw/XHrDh2JiEUN0WgVpJ/ZQbQ7sMVXO0cSNXyHE25yyeD3letS3KJwpbL27FVclVo+8JaoJcIUeaOA3z+87Xa73NuiIej4fg4GBYWVkhKChI9YqNjcXu3bv1aSOF0iR1ijpcK7ymNf0BeJxywxR6ECn5KbCztEOvTr1afS7fng++Pd8shOqrhVdRXluOQX6PE3pG+0YDgNnoEDeLb6K6rlrvzq7ZHkSvXr3Qq1cvjB07FhyOWqmCQtE7mcWZkMqlWnUQABNmOnTjkNGn3LiUfwl9PPvAgt22Hr2Abx5CtXKCXH0H4efkBy8HL5zLPYeF/RYayjStYQiBGmjBQSxZsgSbN2/G+PHjm9xPZ1JTDI3yKV9bQ1yVhPHCsDt1t1HPNK5T1CFVnIoFEQvaXIaAL8Cfd/5ETV1Nq9fxNiaScpLg7eANPyc/1TYWi4Vo32iz6UGkFqTCmmPdpt5ie2jWQaxYsQIA8MUXX+jNGAqlNaSJ02DJtkSgu3ZTO9dPuWGsDiK9KB01dTVt0h+UCDwFkBMmE257yjE0SdlJGOQ3qFFvL8onCvvS90FcIQbfnt/M2aaBUCxEKC8UHLZ+oznNahAvvfQSAMDb2xtfffUVvL29G7woFEMjKhQh0D0QXAuu+oNbgSksHtQegVpJ/ZQbpkp2WTZyH+U2CC8pUeoQpp64jxCC1IJUvc6gVtKsg6if6VEoFOrFGAqlNaSJ07QeXgIAFxsX+Dr6GrVQnZKfAkcrR/Rw7dHmMrq6dIWjlaNJC9VJ2UkAgIG+AxvtE/AFsLKwMvkw073SeyiTlhlkNFazDsKYxTkKpbiqGHnleVoXqJWE8kKNugdxKf8S+nr2bdeYeDaLjXB+uEk7iOTsZDhwHRDCC2m0z4pjhb5epp+4T9nD07dADbSgQdy9exdjx44FAGRnZ6v+V0JFaooh0XaKjScJ44Xh+J3jkNZJ250lVttI66RIE6fh1QGvtruscF44dqXuglwhb/NoKEOSlJOEAT4Dmo3NR/tEY8vFLUZ5HTVFWCCEBYsZfq1vmnUQR48e1acdFEqr0NUIJiWhvFDUKeqQ8SAD4fxwndTRVq4VXoNMIdOKsCzwFKDqYhVuFd/Sutiva0prSnFVchUTn5rY7DHRvtHYdG4TUsWpGOAzQI/WaQ+hWIggjyCDjDRr1kFQIZpizKRJ0sCz44Fnr50UG09SX6g2NgehDYFaiUqoFqeanIM4n3seBKRJgVpJlG8UAGbCnCk6CEIIhAVCPN3DMOuTmN+q3pQOQZo4TWsJ+prC380f1hxro8xVlJKfAlcbV3Rx7tLusnq79wbXgmuSI5mSspNgwbJAf+/+zR7Dt+ejq3NXkxWqCyoKUFhZaBD9AaAOgmKC1CnqcL3ous70BwDgsDkIcg+CqND4hOqUghREeEVoZSCJpYUlgj2CcUVypf2G6Zmk7CQIPAWw49q1eFy0bzTO5Z4zyTW4DSlQAxo6iJqaGty9e1fXtlAoGnHzwU3Uymt1pj8oCeOFIU2cZlQ3lmpZNa5KriLSK1JrZSpTbhhTO9VRK6/FxbyLGOTbfHhJSbRvNPLL85Fdlq0Hy7SLsEAIFlg6fRhqCbUO4vTp0xg3bhzmzp0LAMjIyMCLL76oc8MolOZQCtS6/tGE8kJRVFUESaVEp/W0hjRJGuRErtWZzwK+AMXVxch9lKu1MnVNakEqquuqMdCv8fyHJ4nyeaxDmBqp4lT4u/nDwcowyz6rdRDbtm3Dvn374OjoCAAIDAxEXl6ezg2jUJpDJBHBkm2p87w0So3DmOZDaFOgVlI/9bep0NIEuScJ4YXAztLOJBcQEhYIVQMJDIFaB8HhcODgYBjvRaE0RZokDb3de7d6mc3WEuLBTL4yJqE6JT8FPDsevB20N8owlBcKFlgmJVQn5ySju0t3eDp4qj2Ww+agv09/k+tBFFcV437ZfYPpD4AGDqJHjx44cuQI5HI5srKy8P7770MgMP0FOCimi65HMClxs3WDt4O3UQnVKfnaE6iV2HPt0dOtp8n0IAghSMpO0ii8pCTKJwpXxFdQWVupQ8u0yxXxFQCGE6gBDRzEqlWrcPv2bXC5XCxduhT29vZ466239GEbhdKIosoiFFQU6E20C+OHGU0PoqK2AhkPMrQqUCsReApMxkHcLrmNoqoijQRqJdG+0ZATOS7lX9KhZdpFuQaEIUNManPH2tjY4NVXX8Wrr7Z/Wj+F0l6UeoCuRzApCfUIxZ93/kStvFbrWWNbS2pBKhREoZPU3AK+AD9d+wnFVcVws3XTevnaRKU/tKIHoZwkdy7nHJ7q8pQuzNI6qeJU+Dn5GfR6qHUQTY1YcnBwQHBwMKZOnQorK9PMb0IxTfQ1gklJGD+MSblRlKGXsFZLKAXqvl59tV62crb4FfEVDOs2TOvla5Ok7CS42ri2apCCq40rAjsFmlTiPkML1IAGISYfHx/Y2dnh2WefxbPPPgt7e3vY2dkhKysLb7/9tj5spFBUpEnS4GnvCXc7d73UZ0xrQ6QUpMDH0Ucni9/UT7lh7CTnJGOg78BWZ7KN8onCuRzTmDBXUVuBW8W3DKo/ABr0IFJTU7F//37V+9jYWEycOBH79+9HfHy8To2jUJ5EJBHpLbwEAD3desLKwso4HMT/BGpd4G7nDm8Hb6N3EEWVRbhZfBOzw2e3+txo32h8deUrZJZkoqdbTx1Ypz3SxGkgIMbfg6iqqkJ+fr7qfX5+PqqqqgAAlpa6HWZIodRHJpchvShdr7NKOWwOgjyCDL54UGlNKW4V30KEp+6WBhV4Cox+qGtyTjIAtJigrzmUK8yZwnBXpUBt6B6EWgexYsUKPPfcc5gxYwZmzJiB559/Hm+88QaqqqqQkJDQ4rlnzpxBXFwcRowYgZ07dzZ73PHjxxEQEICrV68CAHJzcxEaGopx48Zh3LhxWL16detaRTFLbjy4gVp5rd61AGNYPEh5w4j01v4IJiUCvgA3i2+iSlalszraS3J2MqwsrNrUkwroFABna2eTcBCp4lR42HkYfE10tSGmoUOH4s8//1TlYuratatKmJ41a1az58nlcqxZswZ79uwBj8fDpEmTEBsbix49Gi6RWFFRgb179yIsrOGP3s/PD4cPH25teyhmjK4XCWqOMF4Yvr7yNSQVEp2lF1eHSqD21L5ArUTAF0BBFLgquYr+Ps1nSDUkSTlJiPCKaNPiP2wWG1E+USbhIJQCtaFX9tRI5cnKysLdu3dx48YN/PHHHzh06JDac0QiETp37gxfX19wuVzEx8fj1KlTjY7bvHkzXnjhBToaiqKWNEkauBZcvcePjUGoTslPQVfnrjod8mjsKTeqZdW4nH+5TeElJdG+0UgvSkdpTan2DNMy0joprhddN3h4CdCgB7Ft2zZcuHABd+7cwdChQ3HmzBn07dtXbXhJIpGAz3882oLH40EkavgDu379OsRiMZ566ins3r27wb7c3FwkJCTA3t4er7zyCiIiWu5SSqVSZGRkqGtOs9TU1LTrfFPE1Np89s5ZdHfojtu3brfp/La211rKrOR14uoJ+NT6tKnu9nLu/jkEuQS12v7WtJkQAkeuI05nnMZQu6FtMVOnpBSlQKaQoTOrc4ttaqnNXgovEBD8eu5XDOK33dHokusl11GnqIO73F3ja6er37JaB3H8+HEcPnwYCQkJ2LBhAx48eIDXXnut3RUrFAp88MEH2LBhQ6N9Hh4e+Ouvv+Di4oJr165h4cKFSExMhL29fbPlWVlZITCw7StiZWRktOt8U8TU2nzn6B2M6jGqzTa3p71ep70gJmKDfF7FVcXIrczFy1Evt7r+1ra576W+yKrJMsrvxcEHBwEAU6KnwNXGtdnjWmqzTzcfvHDmBeQi1yjbCADJQkaIfybiGXR37a7ROe35brfkWNSGmKysrMBms8HhcFBRUQE3NzcUFBSorZTH40EsFqveSyQS8HiP47eVlZW4desWZs6cidjYWFy5cgULFizA1atXweVy4eLiAgAIDg6Gn58f7t27p7ZOivlSWFkIcYUYoR76G+JaH0MK1Ur9QRcpNp5EwBfgauFV1CnqdF5Xa0nOSUZv994tOgd1OFg5IJQXatSZXYUFQjhaOaKrS1dDm6LeQQQHB+PRo0eYPHkyJkyYgPHjx2uUrC8kJARZWVnIyclBbW0tEhMTERsbq9rv4OCACxcu4PTp0zh9+jTCw8OxY8cOhISEoKSkBHK5HACQk5ODrKws+Pr6tqOZFFNHmQ/JULOZw3hhSC9KR628Vu91Kx2EPmLSAk8BaupqcOPBDZ3X1RoURIHk7GSN0nurI9onGudzz0OukGvBMu2jFKhbOxFQF7QYYiKEYP78+XB0dMS0adMwePBgVFRUoFcv9VPcORwOVq9ejblz50Iul2PixInw9/fH5s2bERwcjGHDmp/Of+nSJWzZsgUcDgdsNhvvvfcenJ2dW904ivmgnIegz0ly9QnlhUKmkOHmg5sI4YXote6UghT0dOsJJ2snndelmlFdkIpgj2Cd16cp1wuvo0xa1i6BWkmUbxQ+T/kc14uuG+z71Bx1ijqIJCK8GGEci7K16CBYLBbmzZuHI0eOAGDSbrSGoUOHYujQhmLXkiVLmjz222+/Vf0fFxeHuLi4VtVFMW9EEhG8HLzQybaTQepXDq0VSUT6dxD5KRjSeYhe6groFABrjjVSxamYETZDL3VqQnsmyD2JcsLcuZxzRucgbj64ieq6aoPPoFaitg/Tu3fvRqOPKBR9kyZJM9i6vACTcoNrwdX7jGpxhRi5j3J1OoO6Phw2ByEeIUY31DUpOwl8ez66Orc/Lt/VuSt4djyjTNyn/NyNYYgroMEoprS0NBw5cgReXl6wsbFRbVf2KigUXVMrr0VGUQZG9xhtMBssLSzR27233oVqlUCtwxnUTyLgC/BL+i8ghBh8opaSpOwkDPIbpBV7WCwWonyNc8KcsEAIa441AjoFGNoUABo4iCfnJ1Ao+ubGgxuQKWQGT7cdxgvD8TvH9VpnSn4K2Cy2Kh23PhB4CrBTuBNZpVlGMZIm91Eu7pfdx6sDtLcmTbRPNA7dOITCykJ42Hlordz2IiwQIowXBg5b7a1ZL6gNMXl7e6OgoADnz5+Ht7c3bGxsoFAo9GEbhQLg8QgmQ8eLQ3mhEFeIUVhZqLc6U/JTENgpEPbc5ucAaRtjS/2dnM3oD61ZIEgdSh3ifO55rZXZXhREgVRxqtHoD4AGDmLbtm3YtWuXKtmeTCbTykQ5CkVT0iRpsLKwMniK5vpCtT4ghOg0xXdzhPBCwGaxjSaza3JOMuws7bTai+rr1ReWbEujCjPde3gPj6SPjEZ/ADRwECdOnMCOHTtU+gOPx0Nlpeks/E0xfUQSEYI9gg3e7dZ3Tqa88jxIKiV6dxC2lrbo1akXrkiu6LXe5kjKTsIAnwFavf7WHGv08exjVA7C2ARqQAMHYWlpCRaLpRKHlGtBUCj6Ik2SZvDwEsAsquNp76k3B6HPGdRPIuAbx9oQ5dJypEnStDJB7kmifaNxKf8SZHKZ1stuC8ICIThsjlHNP1HrIJ5++mmsXr0ajx49wi+//ILZs2fj2Wef1YdtFIoq5m/IIa71CeWF6m2o66W8S+CwOQZxjgK+AHnleSiqLNJ73fU5n3seCqLQyvyHJ4n2jUZNXQ2uiK9ovey2kCpORZB7UJtSmesKtQ7iv//9L+Li4jBy5Ejcu3cPixcvxowZxjOBhmLeqNaAMPAIJiWhvFCkF6Xr5akzpSAFwR7BsLG0UX+wljGW1N9J2Ulgs9gY4DNA62VH+UQBMI4V5gghuJx/WfW5Gwtqg3p79uzB6NGjMXCg9rt4FIo6jGUEk5IwXhhq5bW4WXxTp6EApUA9odcEndXREkpBOLUgFSO7jzSIDQCzQFAYLwwOVg5aL9vb0Rt+Tn44m3sWS9B0hgd9kV+ej6KqIvThG4/+AGjQg6isrMScOXPw3HPP4bvvvsODBw/0YReFAoDRH3wcfdqVwVOb6EuozirNQkl1id4FaiWuNq7o7NTZoD0ImVyGC7kXdBJeUhLtG41zOYbP7Kr8nI2tB6HWQSxatAiJiYlYvXo1ioqKMH369BaXGqVQtImhU2w8Sa9OvWDJttS5g1AK1IZyEABzszKkg0iTpKFSVqkTgVpJlE8Uch7lIKcsR2d1aIKwQAgWWEb1XQc0XHIUANzc3NCpUyc4OzujuLhYlzZRKACYpRdvPLhhNOEl4HHKDV0L1ZfyL4FrwdV7YsD6hPPCkVmciYraCoPUn5SdBEC7E+SeRJW4z8DrQ6SKU9HTradOQmntQa2D+P777zFjxgzMmjULpaWlWLt2Lc3DRNELGQ8yUKeoM7qnKn0sHpSSn4IwXhi4Flyd1tMSAk8BCIhKB9I3yTnJ6OLcBT6OulvmNYwXBhuOjcHDTMICodGFlwANHIRYLMbKlSuRmJiIl19+Gb6+vvjjjz/0YRulg2PoRYKaI4wXhvzyfDyo0o0epyAKXC64bNDwEmDYlBuEECRlJ+k0vAQwPcJI70iDZnYtripGdlm20QnUgAYOYtmyZejZsyf++ecfvPbaa4iJiaEOgqIXRBIRrDnW8Hf1N7QpDdC1UH275DYeSR8Z3EH4OPrAzcbNIBPm7pXeg7hCrFOBWkm0TzSEBUJUy6p1XldTGKtADagZ5nrx4kX8/vvv+OeffxAaGgqhUIhTp041SPtNoeiKNEkagj2CYcG2MLQpDVD2aEQSEWK7xqo5uvUYg0ANMGmxDSVUK/UHvTgI32jUJdchJT8FgzsP1nl9TyIsEAKAUSXpU9JsD2LIkCH45JNP0KdPHyQmJmLr1q2wsrKizoGiFwghRjeCSYmHnQd4djydCdWX8i7BhmOD3u69dVJ+axDwBbhWeE3va3EnZSfB2dpZL59BlC8zYc5QQnWqOBV+Tn5ws3UzSP0t0ayDiIuLQ2FhIf744w/89ddfqKqqMprFQyjmj7hCjAdVD4zSQQC6FapTClIg8BQYPDkhwDgImUKGjKIMvdabnJOMaN9osFkaD7RsM51sO8Hf1d9gM6qFBUKjStBXn2Y//bfeegunTp3C7NmzcfHiRYwaNQolJSU4evQozeZK0TnKp3NjGuJanzBeGK4XXkedok6r5coVcggLhHpbYlQdhki5UVxVjPSidJ0L1PWJ9o3G2ZyzIITorU6ASUaYWZxplAI1oEakZrFYGDBgAN5//32cOnUKn3zyCU6dOoXYWO3HXSmU+hhbio0nCeWFQiqX4lbxLa2We+PBDVTJqgyuPyjxd/WHraWtXoVq5ZO8PvQHJdG+0SiqKsLdh3f1VifAPAgREKMUqAENcjEpsbS0RExMDGJiYlBTU6NLmygUpEnS4OfkBxcbF0Ob0iT1hWptxsmNRaBWYsG2QBgvTK89iOScZFiyLfWa5rx+4r7urt31Vq/S8ZpciKklrK2ttW0HhdIAkURktL0H4HHKDW1PIkvJT4E9195oFq0HGB3iivgKFEQ/Sw0nZSehr1dfvWax7e3eG45WjnrXIYRiITzsPOBp76nXejVF9woQhdJKaupqcOPBDaMVqAGAa8FFoHsgRIXaFaov5V9CX8++ehFnNUXgKUB5bblewi81dTW4lH8Jg3z1F14CmJ7SAJ8Beh/JpBSojXUAULPfwi+//BLp6en6tIVCAQCkF6VDTuRG7SCA/y0epMUehEwuwxXxFaMJLympn/pb11zOv4xaea1e9QclUT5RuFp4FY+kj/RSX01dDdKL0o1y/oOSZh2Er68v9u7di4SEBKxYsQJHjx5FWVmZPm2jdFCMbZGg5gj1CEVeeR6Kq7STvPJ60XVI5VKjcxDBHsGwYFnoRYdQTpBTJtHTJ9G+0VAQBS7mXdRLfdcKr6FOUWe0+gPQgkg9evRojB49GgCQnp6Of//9F4sWLYJCoUBUVBSGDBmC0FDjjRFTTJc0cRpsODbo7qI/sbAt1BeqY7rGtLs8YxOolVhzrNHbvbdeHERyTjIC3ALgbueu87qepL93f7DAwtmcsxjebbjO6zN2gRrQUIPo3bs35s+fj2+//RZffvkl/P398euvv+raNkoHJU2ShhBeiNGl2HgSbedkSslPgbO1s1E6RoGnQOchJgVRIDkn2SDhJQBwsnZCsEew3nQIYYEQTlZO6OrcVS/1tYVWK2H29vaIi4vD+++/rwt7KB0cQghEEpHR6w8AwLfnw8POQ2sO4lL+JUR4RRilYCngCyCplKCgvEBnddx4cAMl1SV6nSD3JFE+UTiXc04vI7ZSxakQeAqM8norMZ6hEhQKmLV5i6uLjXqIa31CeaFayclUU1eDq5KrRjOD+kn0kfo7OTsZgH4nyD1JtG80yqRlOk8tUqeoQ5okzagFaoA6CIqRobzZmkIPAmCE6utF7U+5cVVyFTKFzOj0ByX6GMmUlJMEd1t39HDtobM61KGvFeZuPLiBmroao9YfAA1nUkskEuTl5UEul6u2RUbqb5YjpeNg7Ck2niSMH4aauhpkFmci0D2wzeUYq0CtxMnaCd1cuuGK5IrO6kjKTsIgv0EGDbn0cO2BTradcDbnLOb2mauzepSO1uR7EBs3bsS0adOwY8cO7N69W/XShDNnziAuLg4jRozAzp07mz3u+PHjCAgIwNWrV1XbvvzyS4wYMQJxcXH4999/NaqPYvqICkXo7NQZTtZOhjZFI7QlVKfkp6CTbSf4OflpwyydIODrTqguKC/A3Yd3DRpeApj8c1E+UTqfUS0sEMKGY2NUM+abQm0P4uTJkzh27Bi43NatjSuXy7FmzRrs2bMHPB4PkyZNQmxsLHr0aNh9rKiowN69exEW9jikcPv2bSQmJiIxMRESiQSzZ8/G8ePHYWFh3KNaKO0nTZxm9PMf6hPYKRAcNgciiQhTgqe0uZxL+ZcQ6RVp1IKlgC/A/oz9KKsp07oDT85h9AdDCtRKon2jceTWERRXFetsjYZUcSrC+GFGkdK9JdT2IHx9fSGTyVpdsEgkQufOneHr6wsul4v4+HicOnWq0XGbN2/GCy+8ACsrK9W2U6dOIT4+HlwuF76+vujcuTNEIt0uEk8xPNWyatwsvmky+gMAWHGs0KtTr3YJ1VWyKlwvum604SUlyoyjulgoKTk7GTYcG6PIaqrUIc7nntdJ+QqiYEYwGXl4CdCgB2FjY4OEhARERUU16EW8/fbbLZ4nkUjA5/NV73k8XqOb/PXr1yEWi/HUU081CFtJJJIGPQoejweJRKK+NRSTJr0oHQqiMBn9QUkYLwxn7p9p8/nKRHhG7yCUI5kKUjGk8xCtlp2Uk4T+Pv3BtWhdpEIXRHhFgMPm4GzOWcT3jNd6+Xcf3sUj6SOjF6gBDRxEbGysTtZ/UCgU+OCDD7BhwwatlCeVSpGR0fahaTU1Ne063xQxtjb/cfcPAIBdhZ1O7NJVe/ksPnIe5eDclXNwtnJu9fm/3/odAOBU6aR1+7TZZkII3Kzc8PfNvzHSaaRWygSASlklUgtSMbfXXK3Yqo0293LqhZM3T2K61/R22/Mkx3KOAQCcq521dm109d1W6yDGjx/fpoJ5PB7EYrHqvUQiAY/HU72vrKzErVu3MHPmTABAUVERFixYgB07dqg9tymsrKwQGNj2USQZGRntOt8UMbY2F2UVwc7SDnGRcTrJZqqr9g63HI6PRR+j1rkWgV1aX37ujVx42ntiaJ+hWrdN222OEEbgbsVdrZZ5+t5pyIkcCX0SENij/eVqo82xWbHYlboL/gH+WtcJ9ubtBYfNwdj+Y2HFsVJ/gga0p80tOZZmf4VLliwBAIwdO7bJlzpCQkKQlZWFnJwc1NbWIjExsUFPxMHBARcuXMDp06dx+vRphIeHY8eOHQgJCUFsbCwSExNRW1uLnJwcZGVl0bxPHQBRoQghvBCjSnWtCUrNpK0jmVLyUxDpbRrDxgV8AdKL0iGtk2qtzKTsJLDAUi3aYwxE+UahSlalk3XHU8WpCHIP0ppz0CXNusa33noLAPDFF1+0rWAOB6tXr8bcuXMhl8sxceJE+Pv7Y/PmzQgODsawYcOaPdff3x9PP/00Ro8eDQsLC6xevZqOYDJzCCFIE6dhcu/Jhjal1fDt+ehk26lN4m25tBw3HtzAtOBpOrBM+wg8BahT1OFa4TX09eqrlTKTspMQwgsxqqHNSqH6bM5ZrWoFhBAIC4QY03OM1srUJc06CA8PDwCAt7d3mwsfOnQohg5t2G1W9kye5Ntvv23wfsGCBViwYEGb66aYFrmPcvGw5qFJDXFVwmKxEMYLa9PTprBACAJi9AK1kvopN7ThIOoUdTiXew4zQ2e2uyxt4uvoC28Hb5zLPYdF/RZprdz88nwUVRWZhEANtOAgBIKGSaQIIWCxWKq/QqFQLwZSOgamlmLjSUJ5ofgi5QvIFfJWZaFVzqDW1tO4runu2h0OXAetTZi7KrmKitoKDPQz/PyH+rBYLET5an/CnLCAuW+awhBXoAUHERUVhQcPHmDEiBGIj4+Hl5eXPu2idDCUT98hvBADW9I2QnmhqK6rxu2S262aHZtSkAI/Jz942Hno0DrtwWaxEcYP01rSPuUCQYaeQd0U0T7R2Je+D/nl+fBy0M79T1ggBAssk+kpN6sGfv7559i9ezdcXV2xatUqTJ8+Hd9//z1KS0v1aB6lo5AmSUNX565wtHI0tCltQtnzaa0OkZKfYjLhJSUCvgBpkjTIFXL1B6shOScZvo6+RpliRJW4L0d7iftSxano6dYT9lx7rZWpS1ocLuLg4ICJEyfi//7v/zBlyhRs2bIFBw8e1JdtlA6EqaXYeJJA90BYsCxapUM8rH6I2yW3EellGiOYlAj4AlTJqnC75Ha7yiGEICk7yejCS0oEngJYWVhpNbOrsEBoMvoDoGYehFAoRGJiIlJSUtC3b19s374dERGm9bRDMX6qZFXILMnElKC25zIyNNYca/Tq1KtVDuJywWUAxpvBtTmU6TBSxantSjaXXZaNvPI8DPI1vvASAHAtuIjwitCaDvGg6gFyHuWYh4OIjY2Fg4MD4uPj8f7776uGmV6/fh0AEBQUpB8LKWbP9cLrUBCFSfcgAEaHUCad0wSVQO1pGgK1kt7uvWHJtkRqQSqmBk9tcznGrD8oifaNxuYLmyGtk7Z73oKppPiuT7MOQjm89d9//0VSUhIIIap9LBYLe/fu1b11lA6BqY9gUhLGC8OP135EaU0pnK2d1R6fkp+C7i7d4WLjonvjtAjXgotgj+B2C9VJ2UlwtHJEsEewlizTPtG+0dh4diOEBUJE+bZvIp/y8zKGhISa0qyDeHJeAoWiK0QSEey59ujqYryLt2tC/bUhNElml5KfggE+A3Rtlk4Q8AX47dZvqmHvbSE5JxlRPlGtGhasb5Szu8/mnG23gxAWCNHZqTNcbVy1YZpeMK2cBhSzJE2ShhAP00ux8SStWTyoqLII98vum5xArSScH44HVQ+QV57XpvMfVj/EtcJrRh1eAgCePQ/dXLrhbG77dQhTE6gB6iAoBkaZYsPUw0sA4OXgBTcbN40chLEvMaoOlVDdxglz53LPgYAYxQJB6oj2jcbZnLMNwuyt5ZH0ETJLMk1KfwBacBBtWSSIQmkt2WXZKJOWmbxADTDaXCgvVKO5ECn5KWCBZVLx6PqE8cLAAqvNOkRydjI4bA76effTsmXaJ8onCuIKMe6X3W9zGcq11s2mBzFlyhS89NJL+PHHH5Gbm6tPmygdCOXTtqktEtQcYbwwXCu8pnYSWUpBCgI6BZjsxEAHKwf0cO3RZgeRlJMEAV8AO66dli3TPvUT97UV5edkNg7iwIEDWLlyJQBg/fr1mDhxItavX4+kpCTU1tbqzUCKeaN82g7xMM0UG08SygtFlawKdx7eafE4U5xB/SQCT0GbQky18lpczLto9PqDkmCPYNhz7dvlIIQFQvDsePB08NSiZbqnRQ3Cx8cH06ZNw+eff46ffvoJMTExOHv2LJ577jnMmzdPXzZSzJg0SRq6u3SHg5WDoU3RCpoI1fnl+cgvzzdZgVqJgC/A/bL7KKkuadV5wgIhaupqTMZBcNgc9Pfu364Z1aniVJPrPQAarCinxNLSElFRUYiKYoZ6mcsa0YQQTPxlImoqazC2YiyGdB6CQPdAkx9RYyqIJCKzCS8BQJBHENgsNkQSESb1ntTkMaYuUCtRCq5XxFcQ21XzZYmVE+RMQaBWEuUThQ1JG1BRW9HqPEo1dTW4XngdY/xNYw2I+rT5LqhuCVBTgYDAz8kPwgdCvHT0JQTvCIbHRg9M+HkCPjv/GYQFQq0kJaM0prK2EpnFmWYxgkmJNccaAW4BLQrVKfkpYLPYCOeH688wHdDWkUzJOcno4doDPHvTuYdE+0ZDTuS4lHep1edelVyFnMjNuwdhrrBZbHw26jPM85sHa09rnLl/Bmfun8E/9//BwRtMYkJHK0cM8huEIX5DMKTzEER4RcDSwtLAlps+1wqvgYCYxQim+oTxw3A+93yz+1PyUxDkHgRbS1s9WqV9POw84OXghSuSKxqfo0zQZyorqilRTmg8l3sOMV1jWnWuKc6gVqLWQUilUlhZNcxBUlJSAldX05kNqAksFgvdXLqhm0s3zAqfBYBZ5ezf+/+qHMbRzKMAAFtLW0T5RGFIZ8Zh9PfuDxtLGwNab5oo4/Tm1IMAgFCPUPx07SeU1ZQ1WkaTEIKU/BSTu0E2h4DfOqH6VvEtPKh6YFLhJQBwsXFBYKfANgnVwgIhnKyc0NXZ9DIFqA0xTZo0CVeuXFG9P378OKZNM431c9uLj6MPpoVMw44xO5C+MB2S5RLsf3Y/5grmori6GO/+/S5ivomB84fOGLxnMN469Rb+vPMnyqXlhjbdJEiTpMGB64DOzp0NbYpWUWoqVwuvNtqX8ygHRVVFJq8/KBHwBbjx4AaqZdUaHa9MZmgqAnV9on2jcS73HBRE0arzlAJ1W1OSGBK1PYhNmzZh5cqV6NevHwoLC1FaWopvvvlGH7YZHR52HpgQOAETAicAAEprSpGUnaQKS32Y/CHWJ62HBcsCfTz7YEjnIRjaeSgG+Q0yuYRs+iBNkoZQXqjZDQhQhszSxGmNboTKGLapj2BSIvAUQE7kuFp4VaNJb0nZSXCzcUOAW9vThBuKaN9o7E7djVvFt9CrUy+NzqlT1EEkEeGliJd0bJ1uUOsgAgICsGDBArz22muws7PD999/Dz6frw/bjB5na2eM6TlGFS6oqK3AuZxzjMPIPoNtF7fh43MfgwUWQnghGNp5KIZ0HoLBfoNNSqDTBYQQiCQiTA+ZbmhTtI63gzdcrF2aHOqakp8CS7al2YzcUgrtqQWpGjuIgX4DTfJpWpm471zOOY0dxI0HN1BTV2OSAjWggYNYuXIlcnJy8NtvvyErKwvz58/HjBkz8Pzzz+vDPpPCnmuPEd1HYET3EQCY4W0X8y6qehi7U3dj68WtAIAAtwAM7TwUYwPGIt4/3iR/MO3hftl9PJI+MpsbZX1YLGbNYVFhEw6iIAUhvJB2ry1gLHR17gonKyeNZlQXVhYisyQTL/R5QQ+WaZ+ATgFwsXbB2ZyzmC2YrdE5wgIhANMUqAENHETPnj2xbt06sFgs+Pr64tdff8WGDRv0YZvJY82xVgnZACCTyyAsEKp6GD9f/xk7hTsR6RWJ9cPWY3i34Qa2WH8oc9OY2wgmJaEeodiduhsKolCF0JQC9bO9nzWwddqDxWIhnB+ukYNIzmb0B2NdYlQdbBYbUb5RrcrsKiwQwoZjY5IhNUADkXrWrFkNnm4dHBywfv16nRplrlhaWKK/T3+8NvA1HJl2BA9ef4A94/ZAUinBiG9HYNjeYS0OjzQn0iRpYIFl1IvFtIcwfhgqZZW4+/Cuatvdh3dRWlNqNgK1EgFfAJFEhDpFXYvHJeckw8rCyuRW0KtPtE800ovSUVpTqtHxqeJUhPHDjHrNi5ZQ6yCysrKwePFijB49GsOGDVO9KO2Hw+ZgVvgs3Fp0C5tHbca1wmuI2h2FhJ8ScFXSeASMOSGSiNDdtXurZ6WaCsrQmbKnBDyeQR3pbR4CtRKBpwA1dTW4+eBmi8clZSehn3c/kw6vKRcN0uRBTkEUSC1IRR++aeoPgAYO4s0338S0adNgYWGBvXv3IiEhAc8884w+bOswWHGssLj/YtxZfAfrYtfh76y/EfZFGKYfmI47JS0nfTNV0iTmsQZEcwS5P065oeRS/iVYWVghyN281nNXptxoKcxUJavC5YLLJjf/4Un6efcDm8XWaD7E3Yd3UV5bbrICNaCBg5BKpar8S97e3nj55Zfxzz//6Nywjog91x4rB6/E3SV38cbAN3Ag4wB6be+FBb8vQH55vqHN0xoVtRW4U3LHrB2EjaUNerr1bCBUp+SnIJwfbnaz8Ht16gUrC6sWJ8xdyruEOkWdSc5/qI891x5hvDCNHISpC9SABg6Cy+VCoVCgc+fO+O6773DixAlUVlbqw7YOi6uNKzYM34A7i+9gXp952JW6Cz229MAbJ95AcVWxoc1rN8oUG+Y4gqk+obxQVYhJQRS4XHDZ7PQHgNHWQnghLfYglAn6lGsrmDJRPlG4kHdBbY42YYEQlmxLk+4xqnUQK1euRHV1Nd5++21cv34dhw8fxocffqgP2zo8ng6e2B6/HTcX3cSk3pOw8exGdNvSDWvPrEVFbYWhzWsz5j6CSUkYLwz3Su/hkfQRbhXfQkVthVk6COB/KTfEqc0uy5mUk4Qg9yCzmDAa7RuNitoKXCu81uJxqeJUBHkEmbTmotZBhIaGws7ODnw+Hxs2bMC2bdsQHh6uB9MoSrq5dMPe8XtxdcFVxHaNxaq/VqHb5m7YfH4zaupqDG1eq0mTpMHJygmdncwrxcaTqFJuSK4+FqjNZAb1kwj4ApTWlCK7LLvRPrlCjnM550w+vKREkxXmCCEQFghNWqAGWpgH8eKLL7Z44hdffKF1YygtE+QRhINTDuJi3kWsPLUSrxx/BR+f+xjvPvUuZobNBIdtGsl5lWtA6H1yoFy/advrLx5048EN2FraajwD19RQpf4WpzbKrXW96DrKpGVm4yC6OHcBz46Hc7nnsCByQZPH5JXn4UHVA5PWH4AWHMSVK1fg6emJ+Ph4hIWFNdt1pOifft79cHLmSZy6eworT6/Ef3/7Lz5K/gjvx7yPib0nGnVuIwVRQCQRYWbYTP1VWlcHrFiBgO3bgY8/BhYsAPTgnHwdfeFs7QyRRARRoQh9PPuY7Hh4dShzaqUWpCKhV0KDfaa4QFBLsFgsRPtGt9iDUArUpjyCCWghxJScnIxXX30VmZmZWLduHZKTk+Hi4oJ+/fqhXz/1OVdMBkKAFSvg/NNPgImJ78O6DcP5/57HwSkHwWFz8Oy+ZxGxMwLHbh8zWoeeVZqF8tpy/Y1gKioC4uKAjz+GzMsLWLgQmDULqNYs+2h7YLFYCOWFQigWIrUgFRGe5qk/AEwK/J5uPZsUqpNzkuHl4IUuzl30b5iOiPaNxp2Hd1BYWdjk/tSCVLDAMvmRes06CAsLCwwZMgQffvghfvnlF3Tu3BkzZszAd999p3HhZ86cQVxcHEaMGIGdO3c22v/jjz9i7NixGDduHKZNm4bbt28DAHJzcxEaGopx48Zh3LhxWL16dRuapiEKBZCSAs81awAfH+C114CsLN3Vp2VYLBYSeiUg7cU07E3Yi9KaUjz9/dMY+vVQVWoDY0KvAvXly0BEBJCcDOzZg7u//Qa89x7w7bdAdDRw9676MtpJGC8MF/Muorqu2mwFaiVKofpJkrKTMNDXNBP0NYdShziX0/Q61UKxEAGdAmDHtdOnWVqnxVhEbW0t/vzzTyxfvhzff/89ZsyYgREjRmhUsFwux5o1a7Br1y4kJibi999/VzkAJWPHjsWRI0dw+PBhzJ07t0GOJz8/Pxw+fBiHDx/GmjVr2tA0DbGwAE6cQNb33zNPmp9+CnTvDowfD/z9N9PDMAEs2BaYETYDNxbdwPbR25FZkolBewYh/od4XBFfMbR5KkQSEVhg6X7o39dfAwMHMtcvKYnpNbDZwOrVwO+/Mw8BERHAH3/o1Iz6Q3k7goPIfZSLB1UPVNtyynKQXZZtNvqDkj6efWDJtmw2zCQsEJp8eAloQYN4/fXXkZmZiSFDhmDRokXo2bNnqwoWiUTo3LkzfH19AQDx8fE4deoUevTooTrG3v5xmoXq6mrDPWGwWKgWCIDnngNyc4EdO4AvvwQOHQJCQ4HFi5l9Nsa/ahzXgouXIl/CrPBZ2HphKz5M/hCCLwWYGjwVa55aA383f53VTQhBdV01iquKUVxdjJLqkkb/H7l1BP5u/rp7sqqtBZYuBbZvB2JigJ9/BtzdGx4zejTTu5gwAYiPB959F3j7bcaBaBlliMHRylGnn32T3LoFp0OHAH9/gKP7AQz116hWZjQ25QWCWsKaY42+Xn2bTNxXVFmE3Ee5qhnmpkyz35rffvsNNjY2yMrKwrfffqvaTggBi8WCUChssWCJRNJg3QgejweRqHH64++//x579uyBTCZrsBBRbm4uEhISYG9vj1deeQURES0/fUmlUmRkZLR4TEvU1NQ8Pn/6dLAmTYJjYiJcv/sO1nPnom75cpROnoyHU6eiztOzzfXok2fcnsFTo57Cnpt7sPfGXvx6/VdM6DoBC3ovAN+W37DNTyBTyFBWW4ZSaSlKa0sb/d9oX20pSqWlqFXUNmuPDccGTlwnTO0+tV3Xqjksiorg8+qrsBUKUTxrFgqXLgUePGBeQKP2sr76Cvz33oPzO++g/PRp5H/wARROTs0V3zab6izAAguBToG4eaPlXEVao7YWnXbvhtsXX8BLJkPVvn3I++gj1Hl56bRaOynj9I+LjsOn1gcAcCTtCGw4NrAssURGqfaveVO09L3WJgG2Afjx9o9Iu5YGrgVXtT1ZzDhFt1o3vdgB6K7NzTqIGzduaL2ypnj++efx/PPP48iRI9ixYwc+/PBDeHh44K+//oKLiwuuXbuGhQsXIjExsUGP40msrKwQGBjYZjsyMjIany8QAG+9BZw5A87mzei0ezc6ffUV8+S5ZAkTxzaBuGr/sP54r+I9rP93Pb64/AV+u/8b5vaZC0WlAixbFkpqGj/pl9c2v2yqJdsSbrZucLVxhZuNG7zdvOFq7Qo3Wze42fxv+xP/u9q4wppjrbtGnjsHTJ0KlJYCP/wAt2nT4PbEIU1e40OHgB074PDKKwiYPh04cIDpNWqROffmYLDf4HZ9PzXm7FnghReA9HRgyhQUBAbC8+OP4T95MrBrF/Pd1SG+f/kiT5Gnamv6mXQM9BuIkKAQndZbnyavsw4YQ8bgm1vfQOoiRZj3Y03tt+LfAADj+o+Dq42rzu0A2tfmlhyLzvqdPB4PYrFY9V4ikYDHa34Vtfj4eLz77rsAmPQeXC7jkYODg+Hn54d79+4hJER/XzIVLBYwdCjzyspiQhe7dgG//gr07cuEn6ZMAayMe7Ykz56HzU9vxtKopXjvn/ewI2UHCCFwsXFR3cj59nwEuQepbvz1nUD9/+259sYlOO7cCSxaBPj6AseOte4Gz2IBL73EPAxMmgQMGAD83/8BWlwQa9czu7RWVrOUlQFvvsmER/38GJ0lPh6lGRnwnD4dmDYNmDgRePFF4JNPdBYuFXgKVDmZymrKIJKIsHqIDgeZGJD6E+bqr6YnFAvRxbmL3pyDTiE6QiaTkdjYWJKdnU2kUikZO3YsuXXrVoNj7t27p/r/1KlTZPz48YQQQoqLi0ldXR0hhJDs7GwyaNAg8vDhwxbrS09Pb5e9rTq/ooKQHTsICQwkBCDEw4OQ1asJyc9vlw36pFpWTa5eu2poM9pHdTUhc+cy1yAujpDi4hYPV3uNCwoIGTKEKe/llwmRSrVorA7Zv58QT09C2GxCXn2VkPJy1S5Vm6VSQl57jWlbUBAhV3Vz7d/56x3CepdFyqXl5FjmMYJ3QU7cOaGTupqjvfeC1tD5085k8i+TG2zz3+JPJvw8QW82ENK+Nrd0rs5mVHE4HKxevRpz587F6NGj8fTTT8Pf3x+bN2/GqVOnAADfffcd4uPjMW7cOOzZs0eV4+nSpUt45plnMG7cOCxevBjvvfcenJ2ddWVq67GzY57Erl8HTpwA+vUD3n8f6NwZmD4duHjR0BaqxZpjbdqTtnJzmV7drl3AypVAYiLg2s4nNj4fOHmSEbm3bgViY4F8I86im5vLjLabOBHg8YALF5jeQVOhWC4X+Ogj4PhxRpOJjAS++ELro/QEfAEImPXGk3OSYcGyQH/v/lqtw5iI9o3GudzHQ10fSR8hsyRTfwK1UAhERoL/3nu6Kb/NbsfI0GsPoikyMwlZsoQQBwfmKW3AAEJ++MGon0L1+aSlVf75h+m12dszT88a0qr2/vQTIXZ2hPD5hJw50wYjdUhdHSHbtjHfNRsbQj76iJDa2iYPbbLNYjHT4wIImTBBbc+rNdwvvU/wLsj2i9tJzNcxpO+XfbVWtqbo83u95fwWgndBskuzCSGE/JP1D8G7IIm3EnVbcU0NIW+9RYiFBSGeniTrm2/aXJRBehAdjh49gM8+A/LygC1bgOJiZmhsly7A2rVAYdMzLimtgBDmsx02DHB2Zp6YdSW6TpnClO/gwPQkNm82jjkxV68CgwYxmsuAAcC1a8zkTstWrDHB4wFHjwKbNgFHjgDh4cC//2rFPF9HX7jauOJS/iWczz1vNuk1muPJxH1K/UWnPYiLF4E+fYB164CZM4Hr11EVqZskkNRBaBsHB+Dll4EbN5iwR0gIsGoVIxzOng2kql/cndIEVVXMj2HJEmYew8WLQO/euq0zKAi4dImZK/HKK4xwbah0LDU1zFyNPn2A27eB775jwkXdurWtPDYbWLaMGfVkZQU89RSwZk27ExqyWCwI+ALsS9+H6rpq/c1/IIQJDw4bhq7jxjHzX/Tg0EN5obC1tFU5CKFYCL49H54OOhgKX10NvP46EBUFPHrETPL86ivARXcp1KmD0BVsNnMjO36cGXI4Zw7wyy/MD3zIEGDfPiaJHEU9WVnMU/P33zM3sYMHAS3PV2gWJydm6Ou6dcBPPzFP7ZmZ+qlbyV9/MSOz1q1jnFRGBvNXGyPJIiKYOPZzzwHvvMP0lnJz21WkgC9QrVcy0E/HPQhCmF7QgAHAiBHMgxnADHmOjmYcoA6xtLBEpFekSodILUjVTe8hOZnp6W3cyAxjvn4dGDVK+/U8AXUQ+iAwEPj8cyb8tGkTkJMDTJ7MPP29/jozTPPPP4GbN5knRcpjTp5kbmJ37zI3glWrdDLjuUXYbEYIP3YMKChg7DlyRPf1FhczDxaxsUzOsJMnmRQinTpptx4HByY/1d69jLMICwMOH25zceH8cABAV+eu8HLQ0eQ8uZzpJYSHA888w4Rwv/gCuHsX9w4cAHbvBu7fZ9KtPPusTvNuRftGI1WcipLqEqQXpWs3xUZlJdNrHjyYyRJw8iTTTkdH7dXREm1WNowMg4vUraGujpBDhwiJjSWEw2HEwvovPp8RuadOJWTFCkK++IKQY8cIycggpKpKa2YYtUitUBDy4YfM0M2gIEKeGCLdFrTS3qwsQvr2Za7T228z11LbKBSEfP89Ie7ujAi5YkWbr3ur23zr1uP2LVzYpnqvF14neBdkxoEZrT5XLbW1hOzZQ0jPnoyNvXoR8s03DUR6VZvLywl55x1CbG0J4XIJWbaMkJISrZt05OYRgndBNiVvIngXZH+65gMnWuT0aUK6dXs87Lre8OUn0dUwV+ogtHR+m6mrIyQ7mxkps3cvIWvWEDJnDuM8uncnxNKysQPh8Qjp35+QKVMIeeMNZk7GH3+02oEYrYMoLyfk2WeZtk6a1OIPozVorb3V1cw1Us6/ePBAO+USQsjdu49HGPXrR0haWruKa1ObpVLmZgoQEhJCyPXrrTq9Tl5HEn5KICfvnGx93c1RXU3I558T0rkzY1dYGCG//tqkg27U5rw8QmbPJoTFIsTVlZDPPtPq6MKiyiKCd0G6be5G8C7I3ZK77Svw0SNCXnyRaWePHhqNoqMOQg0m6yDUUVdHSE4OIf/+S8i33xLy/vuE/Pe/hAwb1rwD8fBgbi7PPkvI668zP6yjRwlJTyekslJVtFG2OTOTkOBgpufw4YfM07SW0Hp7d+5knky7dCHk8uX2lSWTEbJpE/O0a29PyJYtWumdtKvNR48yvRgbG0K+/FKr10JjysuZz8XTk/luR0UR8vvvLdrSbJtTU5nfDUCIvz8hBw5orU09t/YkeBfE+QNnomhPmcePE+LnxzizpUsb/F5bgjoINZitg1BHXR0hubmEJCUR8t13hKxdy8wuHj6cefpoyoG4uxPSrx8pmTSJkO3bCTl3TuMvok75/XdCnJyYp7w//9R68Tq5xhcuEOLrS4i1NRP6aAspKYQIBMy1GTuW6VFqiXa3uaCAkBEjHvfmdBCiaZKHD5mHITc3pu7YWCbkosHNt8U2KxSEJCY+zoIwZAghly6129xZh2YRvAsS83VM2wp4+PBxr7RXL+Y32Qqog1BDh3UQ6pDLGQeSnMzEtdetI+SFFwiJiSEyJ6fHToPNJqR3b0Kef56Qjz9mfoxq0pto1cY1a5inpvBwJsyiA3R2jQsLmRsYQMj8+cwkJk2oqGCeEtlsRnf69VetP6Vrpc1yOdOb43CYp9vk5PaX2RyFhYS8+SYhjo7M5zlmjG5uljIZE5p1d2fqef55Qu7fb6PRhOxM2UnwLsiy48taf/KRI4R4eT3Wm6qrW10EdRBqoA6i9aRfv86IrgcPErJqFfNj9PJq2Nvo2pWQiROZnsnRo8wTpTYpLSVk3LjHP1Id9mR0eo1lMiacBzD6UE5Oy8cfPfo4nj5/vs6csVbbfOECI5paWDDfB20K9Lm5hLzyChPOYrEImTyZCQm1gVa1uayMcUhWVkwv8M03mW2t5OaDm4T9HpsczDio+UnFxYRMn858B4KD29WToQ5CDdRBtJ5m2ywWM6L3+vVMWKF794ZOw9OTkNGjmVE8+/cTcu9e255809MJCQhgbjiffabzGLdervG+fYyG4O7O9MKeRCwmZNq0x6EEHafx0Hqby8oe2//UU8yNvT3cvcs4SC6X+R7MnMkMtmgHbWrz/fvMA4pSw9uxg3H6rSmi9L7m+sP+/cxgEw6HSfTZTtGcOgg1UAfRelrV5tJSQv7+m5BPPyVkxozHQrLSabi4MGGWZcuYUFZ6estPmAcOPL6R/v13u9uiCXq7xhkZzM2fzSZk40bG8SkUhOzezXxOXC4h776reSiqHeikzQoFo7fY2TEawW+/tb6M9HTme2RhwXweL76otdBiu9p88SIhgwcz3+nevRm9QpsPLoWFj0foCQRt7iU9CXUQamjPB/TgASFJSTcNMkjDkLT75lFVxYQdduwgZN48QiIimK660mnY2jKjThYuJGTXLkKEQuaclSuZ/ZGRWhVk1aHXh4BHj5jelzIh3lNPMf8PGsTcHPWETtt848Zjcf3llzWLnQuFzOfCYjHfj1dfbX8v5Ana3WaFgnmA6dGDadvw4YRcudL+Mn/8kZBOnRiHuHZtswkW2wJ1EGpo6wekUDAPssr7We/eTPTkpZeYJJm//sqEBouKDDPKT5fo5OZRW8uM3f/6a0IWL2aexpQfMMDcGABmqG4bxLj2oPdeokLB9CDYbGZ01pdfMoKvHtF5m2tqGO1AOTehufDQ2bPMDwtgBOi33mKepnWA1toslTKhTxcX5ns7Zw4zp6K15OcTkpDweG7LtWvasa8eunIQul/J3MhhsZjZ64cPi1FTw0dWFnDvHpPCpbS04bF2dkxy1uZebm4msQKpbrG0ZPIGhYYC//kPs02hAO7cYdI4XLnCJDCcNs38PywWC1i+nMnJ1akT4OFhaIu0j5UV8OmnwPDhwKxZzCqLW7YwKUIA4PRpJofUX38xP5C1a4GFC5lsvMYOl8ukuZg5k7F761YmH9frrzPX1c6u5fMJYVKYvPIKk2zyo4+AV18FOKZz2zUdS3VI//6Ao+NDBAbyG2wvLWXSuWRlNX4lJ7fOgXTtyqxnY+73xCZhswF/f+Y1ZYqhrdE/us46awzExwNpacCMGcDcucySp2IxcP484OkJfPwxMG9e04sZGTsuLoz9L70ErFgBvPsukz9t7VrGeVg0sfBWbi4wfz6TVn3gQCY3VECA3k1vL9RBtICzM/MKC2t6/5MO5N69x///+y+Tkbc+9vaNHYeXF/P9c3Zu+Pd/S3JTKKaDlxeTdHLjRiY1uY8Ps0b2rFmAtbWhrWs/3bsza9EnJzOp0ufMYdYJ2bSJ6UEBTK9h925mv0zGrBGzaFHTTsQEoA6iHWjiQJrqfWRlAWfONHYg9bGxeewwnnQe9f82tc3RUf8JTykUAMyNcMUK5mnbxqZ1CxmZCgMHAufOMdlkV6xg0ozHxzPhow8/ZJYhfuopZjnc7t0NbW27oA5Chzg7M9mIw8Ob3l9ayix5XFrKvB4+bPy/8m9BAbOshHJ/S2uhsFjMMgYtORQnJ6CoyAV8fvPlmBtFRc7o25d5sPXxoZqRTtFXOmpDwWIxa04kJDCay7p1zAJh9vZMav/5883iKY06CAOivGG3FoUCKC9v2pE0t+3mzcfbqqqUJXUg7wAAaLjKl5XVY2fh4wP4+jZ87+MDuLubxe+coiusrRnRevZsZpW/CROAzp0NbZXWoA7CBGGzmR6Ak1Pbvou1tUx468aNW+jZs6f2DTRSrl/PhL29P3JyGA2x/uvsWeavTNbwHEtLwNu7aeehfPF4JhtipmgLd3cmxGRmUAfRAeFymVGXbm5ysxx52RzFxXUIDASaW99doQCKiho7D6VDuXiRWX1UKm14HofD6LPNORAfH8MN3ikrY+PhQ8PUbSgM1WYWi1mcz5weFqiDoFD+B5vN9AZ4PGY4f1MQwqwE+qTzUL6uXGFWI62u1qvpLWB6Qyvbj2Hb7OzM6Fv1X66uLb+3tzdOPYw6CAqlFbBYTO+rU6fmBx8Qwmg99R2HoRyGRCIGj9extCZDtVmhAMrKmAcI5auwELhxg/m/pVGLlpbqnciT793cdD8cnjoICkXLsFjMj9nVlZlQbkgyMhpPADV3jLXNMhlQUsK86juR4uLG2+7cYUKaxcWNQ5r1sbNjHMWzz7pi40bt20wdBIVCoegBS8vHIUxNIYQZddiUE6n/3ttbpr6wNkAdBIVCoRgpLBbTS7CzA/z8mj8uI6NcJ/XTEd4UCoVCaRLqICgUCoXSJNRBUCgUCqVJqIOgUCgUSpNQB0GhUCiUJqEOgkKhUChNQh0EhUKhUJqEOggKhUKhNAmLkJaWnjEdrly5AisrK0ObQaFQKCaFVCpFeDOJxczGQVAoFApFu9AQE4VCoVCahDoICoVCoTQJdRAUCoVCaRLqICgUCoXSJNRBUCgUCqVJqIOgUCgUSpN0eAdx5swZxMXFYcSIEdi5c6ehzdE5BQUFmDFjBkaPHo34+Hh88803hjZJb8jlciQkJGD+/PmGNkUvPHr0CIsXL8aoUaPw9NNPIzU11dAm6Zyvv/4a8fHxGDNmDJYuXQppS+t1mihvvvkmoqKiMGbMGNW20tJSzJ49GyNHjsTs2bNRVlamlbo6tIOQy+VYs2YNdu3ahcTERPz++++4ffu2oc3SKRYWFlixYgWOHj2Kn3/+GT/88IPZt1nJ3r170b17d0OboTfWrVuHwYMH49ixYzh8+LDZt10ikWDv3r3Yv38/fv/9d8jlciQmJhraLK0zYcIE7Nq1q8G2nTt3IioqCn/++SeioqK09rDboR2ESCRC586d4evrCy6Xi/j4eJw6dcrQZukUDw8PBAUFAQDs7e3RrVs3SCQSA1ule8RiMf7++29MmjTJ0KbohfLycly6dEnVXi6XC0dHRwNbpXvkcjlqampQV1eHmpoaeHh4GNokrRMZGQknJ6cG206dOoWEhAQAQEJCAk6ePKmVujq0g5BIJODz+ar3PB6vQ9wsleTm5iIjIwNhYWGGNkXnrF+/Hq+99hrY7I7xlc/NzYWrqyvefPNNJCQk4K233kJVVZWhzdIpPB4Pc+bMQUxMDAYNGgR7e3sMGjTI0GbpheLiYpUzdHd3R3FxsVbK7Ri/FkojKisrsXjxYqxcuRL29vaGNken/PXXX3B1dUVwcLChTdEbdXV1SE9Px7Rp03Do0CHY2NiYvcZWVlaGU6dO4dSpU/j3339RXV2Nw4cPG9osvcNiscBisbRSVod2EDweD2KxWPVeIpGAx+MZ0CL9IJPJsHjxYowdOxYjR440tDk6RygU4vTp04iNjcXSpUtx/vx5LF++3NBm6RQ+nw8+n6/qHY4aNQrp6ekGtkq3nD17Fj4+PnB1dYWlpSVGjhzZIYR5AHBzc0NhYSEAoLCwEK6urlopt0M7iJCQEGRlZSEnJwe1tbVITExEbGysoc3SKYQQvPXWW+jWrRtmz55taHP0wrJly3DmzBmcPn0an3zyCQYMGIBNmzYZ2iyd4u7uDj6fj7t37wIAzp07Z/YitZeXF9LS0lBdXQ1CSIdos5LY2FgcOnQIAHDo0CEMGzZMK+VytFKKicLhcLB69WrMnTsXcrkcEydOhL+/v6HN0imXL1/G4cOH0bNnT4wbNw4AsHTpUgwdOtTAllG0zapVq7B8+XLIZDL4+vpiw4YNhjZJp4SFhSEuLg7jx48Hh8NBYGAgpkyZYmiztM7SpUtx8eJFPHz4EEOGDMHLL7+MefPm4ZVXXsG+ffvg5eWFzz77TCt10XTfFAqFQmmSDh1iolAoFErzUAdBoVAolCahDoJCoVAoTUIdBIVCoVCahDoICoVCoTRJhx7mSunYPHjwABs2bMCVK1fg5OQES0tLzJ07FyNGjNC7LRcuXIClpSX69OkDAPjxxx9hY2Ojyq9DoRgC6iAoHRJCCBYuXIiEhAR8/PHHAIC8vDycPn1aZ3XW1dWBw2n6J3fx4kXY2tqqHMS0adN0ZgeFoil0HgSlQ3Lu3Dls374d3333XaN9crkcmzZtwsWLF1FbW4vnn38eU6dOxYULF7Bt2za4uLjg1q1bCAoKwqZNm8BisXDt2jV88MEHqKqqgouLCzZs2AAPDw/MmDEDvXr1wuXLlzFmzBh06dIFO3bsgEwmg7OzMzZt2oSamhpMmTIFbDYbrq6uWLVqFc6dOwdbW1v897//RUZGBt555x1UV1fDz88P69evh5OTE2bMmIHQ0FBcuHAB5eXlWLduHSIiIgzwaVLMFapBUDokmZmZ6N27d5P79u3bBwcHB+zfvx/79+/HL7/8gpycHABAeno6Vq5ciaNHjyI3NxeXL1+GTCbD2rVrsWXLFhw4cAATJ07Ep59+qipPJpPhwIEDmDNnDvr27YtffvkFhw4dQnx8PHbt2gUfHx9MnToVs2bNwuHDhxvd5F9//XUsX74cR44cQc+ePbFt2zbVPrlcjn379mHlypUNtlMo2oCGmCgUAO+99x4uX74MS0tLeHt74+bNmzh+/DgAZm2F+/fvw9LSEqGhoaoU8b169UJeXh4cHR1x69YtVW4rhUIBd3d3VdmjR49W/S8Wi/Hqq6+iqKgItbW18PHxadGu8vJylJeXo1+/fgCA8ePHY8mSJar9Sr0kKCgIeXl5WvgkKJTHUAdB6ZD4+/vjzz//VL1/5513UFJSgkmTJsHLywtvv/02Bg8e3OCcCxcugMvlqt5bWFhALpeDEAJ/f3/8/PPPTdZlY2Oj+n/t2rWYNWsWhg0bpgpZtQelPWw2G3K5vF1lUShPQkNMlA7JgAEDIJVK8cMPP6i21dTUAAAGDRqEH3/8ETKZDABw7969Fhfb6dq1K0pKSlSppWUyGTIzM5s8try8XJVSXpl9EwDs7OxQWVnZ6HgHBwc4OjoiJSUFAHD48GFERka2oqUUStuhPQhKh4TFYmH79u3YsGEDdu3aBVdXV9jY2GD58uUYNWoU8vLyMGHCBBBC4OLigs8//7zZsrhcLrZs2YK1a9eivLwccrkc//nPf5rMDLxo0SIsWbIETk5O6N+/P3JzcwEAMTExWLx4MU6dOoVVq1Y1OOfDDz9UidQdISsrxXigo5goFAqF0iQ0xEShUCiUJqEOgkKhUChNQh0EhUKhUJqEOggKhUKhNAl1EBQKhUJpEuogKBQKhdIk1EFQKBQKpUn+Hz5hmV05Na2+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 81.86588796774546 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 8  # max of individuals per generation\n",
    "max_generations = 10   # number of generations\n",
    "gene_length = 5      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001 , Batch size: 256\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:4])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[4:5])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    \n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1], \n",
    "          ', Learning rate:', best_learning_rate[-1], ', Batch size:', best_batch_size[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.313251</td>\n",
       "      <td>0.8886</td>\n",
       "      <td>93.518755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.323057</td>\n",
       "      <td>0.8863</td>\n",
       "      <td>89.805508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.316921</td>\n",
       "      <td>0.8863</td>\n",
       "      <td>162.438391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.320403</td>\n",
       "      <td>0.8856</td>\n",
       "      <td>111.453523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.315929</td>\n",
       "      <td>0.8843</td>\n",
       "      <td>143.670272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.322823</td>\n",
       "      <td>0.8838</td>\n",
       "      <td>110.866719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.322291</td>\n",
       "      <td>0.8836</td>\n",
       "      <td>57.092397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.319948</td>\n",
       "      <td>0.8824</td>\n",
       "      <td>73.340029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.320040</td>\n",
       "      <td>0.8822</td>\n",
       "      <td>85.835146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.323070</td>\n",
       "      <td>0.8809</td>\n",
       "      <td>94.995943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.324356</td>\n",
       "      <td>0.8809</td>\n",
       "      <td>143.496940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.328383</td>\n",
       "      <td>0.8808</td>\n",
       "      <td>76.301801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.325970</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>57.151561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.326777</td>\n",
       "      <td>0.8802</td>\n",
       "      <td>73.052799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.331786</td>\n",
       "      <td>0.8802</td>\n",
       "      <td>143.709303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.333812</td>\n",
       "      <td>0.8797</td>\n",
       "      <td>61.373424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.329001</td>\n",
       "      <td>0.8797</td>\n",
       "      <td>67.063263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.325781</td>\n",
       "      <td>0.8793</td>\n",
       "      <td>75.533257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.330782</td>\n",
       "      <td>0.8793</td>\n",
       "      <td>103.796935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.332849</td>\n",
       "      <td>0.8792</td>\n",
       "      <td>80.842756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.336463</td>\n",
       "      <td>0.8789</td>\n",
       "      <td>143.467715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.326225</td>\n",
       "      <td>0.8788</td>\n",
       "      <td>65.039943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.326869</td>\n",
       "      <td>0.8787</td>\n",
       "      <td>66.334062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.345194</td>\n",
       "      <td>0.8786</td>\n",
       "      <td>171.917981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.334347</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>78.190488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.331211</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>83.386525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.329531</td>\n",
       "      <td>0.8768</td>\n",
       "      <td>76.690553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.339006</td>\n",
       "      <td>0.8767</td>\n",
       "      <td>70.544263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.339615</td>\n",
       "      <td>0.8755</td>\n",
       "      <td>58.173034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.349005</td>\n",
       "      <td>0.8755</td>\n",
       "      <td>112.257652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.334945</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>128.490213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.343011</td>\n",
       "      <td>0.8753</td>\n",
       "      <td>74.438792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.341449</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>127.530655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.347580</td>\n",
       "      <td>0.8751</td>\n",
       "      <td>64.002707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.337025</td>\n",
       "      <td>0.8740</td>\n",
       "      <td>69.528060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.339239</td>\n",
       "      <td>0.8738</td>\n",
       "      <td>92.767078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.338036</td>\n",
       "      <td>0.8736</td>\n",
       "      <td>71.207559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.351455</td>\n",
       "      <td>0.8730</td>\n",
       "      <td>163.991128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.350247</td>\n",
       "      <td>0.8721</td>\n",
       "      <td>65.871676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.343140</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>127.209906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.343826</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>133.051143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.366778</td>\n",
       "      <td>0.8690</td>\n",
       "      <td>152.818103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.353471</td>\n",
       "      <td>0.8683</td>\n",
       "      <td>98.863181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.358558</td>\n",
       "      <td>0.8681</td>\n",
       "      <td>86.614739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.365426</td>\n",
       "      <td>0.8677</td>\n",
       "      <td>101.607405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.375668</td>\n",
       "      <td>0.8638</td>\n",
       "      <td>94.199218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.493854</td>\n",
       "      <td>0.8115</td>\n",
       "      <td>160.780473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>256</td>\n",
       "      <td>0.520450</td>\n",
       "      <td>0.8002</td>\n",
       "      <td>110.240731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.528203</td>\n",
       "      <td>0.7968</td>\n",
       "      <td>84.795579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>256</td>\n",
       "      <td>0.550519</td>\n",
       "      <td>0.7894</td>\n",
       "      <td>72.248670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss  Accuracy  \\\n",
       "0             4        200        0.00100         256  0.313251    0.8886   \n",
       "1             4        200        0.00100         256  0.323057    0.8863   \n",
       "2             3        200        0.00100         128  0.316921    0.8863   \n",
       "3             4        100        0.00100         128  0.320403    0.8856   \n",
       "4             4        200        0.00100         256  0.315929    0.8843   \n",
       "5             3        200        0.00100         256  0.322823    0.8838   \n",
       "6             3        100        0.01000         256  0.322291    0.8836   \n",
       "7             4        100        0.00100         256  0.319948    0.8824   \n",
       "8             3        100        0.01000         256  0.320040    0.8822   \n",
       "9             3        200        0.00100         256  0.323070    0.8809   \n",
       "10            3        200        0.00100         256  0.324356    0.8809   \n",
       "11            4        100        0.00100         256  0.328383    0.8808   \n",
       "12            4        100        0.00100         256  0.325970    0.8805   \n",
       "13            4        100        0.00100         256  0.326777    0.8802   \n",
       "14            4        200        0.00100         256  0.331786    0.8802   \n",
       "15            4        100        0.00100         256  0.333812    0.8797   \n",
       "16            4        100        0.00100         256  0.329001    0.8797   \n",
       "17            4        100        0.00100         256  0.325781    0.8793   \n",
       "18            4        100        0.00100         256  0.330782    0.8793   \n",
       "19            4        100        0.00100         256  0.332849    0.8792   \n",
       "20            3        200        0.01000         256  0.336463    0.8789   \n",
       "21            4        100        0.00100         256  0.326225    0.8788   \n",
       "22            4        100        0.01000         256  0.326869    0.8787   \n",
       "23            4        200        0.01000         128  0.345194    0.8786   \n",
       "24            4        100        0.00100         256  0.334347    0.8783   \n",
       "25            4        100        0.00100         256  0.331211    0.8781   \n",
       "26            4        100        0.00100         256  0.329531    0.8768   \n",
       "27            4        100        0.01000         256  0.339006    0.8767   \n",
       "28            3        100        0.00100         256  0.339615    0.8755   \n",
       "29            4        200        0.01000         256  0.349005    0.8755   \n",
       "30            4        200        0.01000         256  0.334945    0.8754   \n",
       "31            3        100        0.01000         256  0.343011    0.8753   \n",
       "32            3        200        0.01000         128  0.341449    0.8752   \n",
       "33            3        100        0.01000         256  0.347580    0.8751   \n",
       "34            3        100        0.00100         256  0.337025    0.8740   \n",
       "35            4        100        0.01000         128  0.339239    0.8738   \n",
       "36            4        100        0.00100         256  0.338036    0.8736   \n",
       "37            4        200        0.01000         128  0.351455    0.8730   \n",
       "38            3        100        0.00100         256  0.350247    0.8721   \n",
       "39            3        200        0.01000         128  0.343140    0.8713   \n",
       "40            3        200        0.01000         128  0.343826    0.8713   \n",
       "41            4        200        0.01000         128  0.366778    0.8690   \n",
       "42            4        100        0.01000         128  0.353471    0.8683   \n",
       "43            3        200        0.01000         256  0.358558    0.8681   \n",
       "44            4        100        0.00010         128  0.365426    0.8677   \n",
       "45            3        100        0.00010         128  0.375668    0.8638   \n",
       "46            4        200        0.00001         128  0.493854    0.8115   \n",
       "47            4        200        0.00001         256  0.520450    0.8002   \n",
       "48            3        100        0.00001         128  0.528203    0.7968   \n",
       "49            4        100        0.00001         256  0.550519    0.7894   \n",
       "\n",
       "    Elapsed time  \n",
       "0      93.518755  \n",
       "1      89.805508  \n",
       "2     162.438391  \n",
       "3     111.453523  \n",
       "4     143.670272  \n",
       "5     110.866719  \n",
       "6      57.092397  \n",
       "7      73.340029  \n",
       "8      85.835146  \n",
       "9      94.995943  \n",
       "10    143.496940  \n",
       "11     76.301801  \n",
       "12     57.151561  \n",
       "13     73.052799  \n",
       "14    143.709303  \n",
       "15     61.373424  \n",
       "16     67.063263  \n",
       "17     75.533257  \n",
       "18    103.796935  \n",
       "19     80.842756  \n",
       "20    143.467715  \n",
       "21     65.039943  \n",
       "22     66.334062  \n",
       "23    171.917981  \n",
       "24     78.190488  \n",
       "25     83.386525  \n",
       "26     76.690553  \n",
       "27     70.544263  \n",
       "28     58.173034  \n",
       "29    112.257652  \n",
       "30    128.490213  \n",
       "31     74.438792  \n",
       "32    127.530655  \n",
       "33     64.002707  \n",
       "34     69.528060  \n",
       "35     92.767078  \n",
       "36     71.207559  \n",
       "37    163.991128  \n",
       "38     65.871676  \n",
       "39    127.209906  \n",
       "40    133.051143  \n",
       "41    152.818103  \n",
       "42     98.863181  \n",
       "43     86.614739  \n",
       "44    101.607405  \n",
       "45     94.199218  \n",
       "46    160.780473  \n",
       "47    110.240731  \n",
       "48     84.795579  \n",
       "49     72.248670  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_sdss_1.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Accuracy\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Accuracy\", \"Elapsed time\"], ascending=[0,1], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 81.860 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
