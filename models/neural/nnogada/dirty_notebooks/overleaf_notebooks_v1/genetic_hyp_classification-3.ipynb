{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>delta</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135.689107</td>\n",
       "      <td>32.494632</td>\n",
       "      <td>23.87882</td>\n",
       "      <td>22.27530</td>\n",
       "      <td>20.39501</td>\n",
       "      <td>19.16573</td>\n",
       "      <td>18.79371</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144.826101</td>\n",
       "      <td>31.274185</td>\n",
       "      <td>24.77759</td>\n",
       "      <td>22.83188</td>\n",
       "      <td>22.58444</td>\n",
       "      <td>21.16812</td>\n",
       "      <td>21.61427</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142.188790</td>\n",
       "      <td>35.582444</td>\n",
       "      <td>25.26307</td>\n",
       "      <td>22.66389</td>\n",
       "      <td>20.60976</td>\n",
       "      <td>19.34857</td>\n",
       "      <td>18.94827</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>338.741038</td>\n",
       "      <td>-0.402828</td>\n",
       "      <td>22.13682</td>\n",
       "      <td>23.77656</td>\n",
       "      <td>21.61162</td>\n",
       "      <td>20.50454</td>\n",
       "      <td>19.25010</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345.282593</td>\n",
       "      <td>21.183866</td>\n",
       "      <td>19.43718</td>\n",
       "      <td>17.58028</td>\n",
       "      <td>16.49747</td>\n",
       "      <td>15.97711</td>\n",
       "      <td>15.54461</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        alpha      delta         u         g         r         i         z  \\\n",
       "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
       "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
       "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
       "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
       "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
       "\n",
       "    class  \n",
       "0  GALAXY  \n",
       "1  GALAXY  \n",
       "2  GALAXY  \n",
       "3  GALAXY  \n",
       "4  GALAXY  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/igomezv/nnogada/main/data/star_classification.csv\"\n",
    "data = pd.read_csv(url)\n",
    "cols = ['alpha','delta','u','g','r','i','z','class']\n",
    "data = data[cols]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        alpha      delta         u         g         r         i         z  \\\n",
      "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
      "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
      "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
      "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
      "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
      "\n",
      "   class  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n"
     ]
    }
   ],
   "source": [
    "data[\"class\"]=[0 if i == \"GALAXY\" else 1 if i == \"STAR\" else 2 for i in data[\"class\"]]\n",
    "print(data.head())\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function layers\n",
    "\n",
    "# f1 = lambda x: Dense(x, activation='relu')      #ReLU\n",
    "# f2 = lambda x: Dense(x, activation='elu')       #ELU\n",
    "# f3 = lambda x: keras.layers.LeakyReLU(0.3)      #LReLU\n",
    "# f4 = lambda x: Dense(x, kernel_initializer='lecun_normal', activation='selu')   #SELU\n",
    "\n",
    "# f_names = [\"ReLU\", \"ELU\", \"LReLU\", \"SELU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([3, 4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([200, 100]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-5,1e-4,1e-3, 0.01])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([128,256]) \n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=50,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=1)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 50\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into X and Y and implement hot_ones in Y\n",
    "def prepare_dataset(data):\n",
    "    X, Y = np.empty((0)), np.empty((0))\n",
    "    X = data[:,0:7]\n",
    "    Y = data[:,7]\n",
    "    Y = to_categorical(Y, num_classes=3)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation and test sets\n",
    "X,Y = prepare_dataset(data)\n",
    "\n",
    "# Defines ratios, w.r.t. whole dataset.\n",
    "ratio_train = 0.8\n",
    "ratio_val = 0.1\n",
    "ratio_test = 0.1\n",
    "\n",
    "# Produces test split.\n",
    "x_, X_test, y_, Y_test = split(X, Y, test_size = ratio_test, random_state=0)\n",
    "\n",
    "# Adjusts val ratio, w.r.t. remaining dataset.\n",
    "ratio_remaining = 1 - ratio_test\n",
    "ratio_val_adjusted = ratio_val / ratio_remaining\n",
    "\n",
    "# Produces train and val splits.\n",
    "X_train, X_val, Y_train, Y_val = split(x_, y_, test_size=ratio_val_adjusted, random_state=0)\n",
    "\n",
    "# Normalize and scale the input sets.\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "X_val   = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:1])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[1:2])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[2:4])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[4:5])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(num_units, input_shape=(int(X_train.shape[1]),)))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(3, activation=tf.nn.softmax))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=1, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Accuracy:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5      # probability for crossover\n",
    "    P_MUTATION = 0.8         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1  # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3833 - accuracy: 0.8592\n",
      "Accuracy: 0.8592000007629395 , Elapsed time: 53.96163034439087\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3393 - accuracy: 0.8776\n",
      "Accuracy: 0.8776000142097473 , Elapsed time: 53.516313314437866\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3575 - accuracy: 0.8689\n",
      "Accuracy: 0.8689000010490417 , Elapsed time: 80.84070014953613\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 894us/step - loss: 0.3838 - accuracy: 0.8620\n",
      "Accuracy: 0.8619999885559082 , Elapsed time: 50.27140522003174\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 694us/step - loss: 0.3271 - accuracy: 0.8819\n",
      "Accuracy: 0.8819000124931335 , Elapsed time: 53.36823105812073\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 796us/step - loss: 0.3902 - accuracy: 0.8563\n",
      "Accuracy: 0.8562999963760376 , Elapsed time: 51.334245681762695\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 765us/step - loss: 0.5466 - accuracy: 0.7931\n",
      "Accuracy: 0.7930999994277954 , Elapsed time: 43.17325043678284\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 781us/step - loss: 0.5707 - accuracy: 0.7841\n",
      "Accuracy: 0.7840999960899353 , Elapsed time: 39.680017948150635\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin   \tavg     \tmax     \n",
      "0  \t8     \t0.3271\t0.412302\t0.570706\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 781us/step - loss: 0.3297 - accuracy: 0.8796\n",
      "Accuracy: 0.8795999884605408 , Elapsed time: 42.071065187454224\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 860us/step - loss: 0.3515 - accuracy: 0.8717\n",
      "Accuracy: 0.8716999888420105 , Elapsed time: 62.31708765029907\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 815us/step - loss: 0.3880 - accuracy: 0.8567\n",
      "Accuracy: 0.8567000031471252 , Elapsed time: 50.645991563797\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 887us/step - loss: 0.3677 - accuracy: 0.8662\n",
      "Accuracy: 0.8661999702453613 , Elapsed time: 53.72163939476013\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 866us/step - loss: 0.3715 - accuracy: 0.8629\n",
      "Accuracy: 0.8629000186920166 , Elapsed time: 54.0647759437561\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 1 , Number of neurons: 50 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 753us/step - loss: 0.4064 - accuracy: 0.8558\n",
      "Accuracy: 0.8557999730110168 , Elapsed time: 32.955405712127686\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t6     \t0.3271\t0.35862 \t0.406366\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 756us/step - loss: 0.3451 - accuracy: 0.8770\n",
      "Accuracy: 0.8769999742507935 , Elapsed time: 42.66214060783386\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 872us/step - loss: 0.5311 - accuracy: 0.7995\n",
      "Accuracy: 0.7994999885559082 , Elapsed time: 52.32416820526123\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 949us/step - loss: 0.3123 - accuracy: 0.8856\n",
      "Accuracy: 0.8855999708175659 , Elapsed time: 72.33402609825134\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 827us/step - loss: 0.4076 - accuracy: 0.8512\n",
      "Accuracy: 0.8511999845504761 , Elapsed time: 42.44373607635498\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 837us/step - loss: 0.3654 - accuracy: 0.8637\n",
      "Accuracy: 0.8636999726295471 , Elapsed time: 52.35024452209473\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t5     \t0.312252\t0.378469\t0.53109 \n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 835us/step - loss: 0.3702 - accuracy: 0.8656\n",
      "Accuracy: 0.8655999898910522 , Elapsed time: 52.3208863735199\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 913us/step - loss: 0.3207 - accuracy: 0.8858\n",
      "Accuracy: 0.8858000040054321 , Elapsed time: 53.85330533981323\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 822us/step - loss: 0.6072 - accuracy: 0.7764\n",
      "Accuracy: 0.7764000296592712 , Elapsed time: 37.41761612892151\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 843us/step - loss: 0.4040 - accuracy: 0.8535\n",
      "Accuracy: 0.8535000085830688 , Elapsed time: 48.676785469055176\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 871us/step - loss: 0.3681 - accuracy: 0.8629\n",
      "Accuracy: 0.8629000186920166 , Elapsed time: 55.15323305130005\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 797us/step - loss: 0.3332 - accuracy: 0.8779\n",
      "Accuracy: 0.8779000043869019 , Elapsed time: 42.73960757255554\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 788us/step - loss: 0.4901 - accuracy: 0.8195\n",
      "Accuracy: 0.8195000290870667 , Elapsed time: 37.8155722618103\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t7     \t0.312252\t0.400713\t0.607153\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 883us/step - loss: 0.3221 - accuracy: 0.8812\n",
      "Accuracy: 0.8812000155448914 , Elapsed time: 52.51398515701294\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 903us/step - loss: 0.3515 - accuracy: 0.8714\n",
      "Accuracy: 0.871399998664856 , Elapsed time: 76.31679248809814\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 924us/step - loss: 0.3433 - accuracy: 0.8777\n",
      "Accuracy: 0.8776999711990356 , Elapsed time: 43.2390615940094\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 848us/step - loss: 0.3916 - accuracy: 0.8601\n",
      "Accuracy: 0.8600999712944031 , Elapsed time: 48.17084336280823\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 745us/step - loss: 0.3418 - accuracy: 0.8758\n",
      "Accuracy: 0.8758000135421753 , Elapsed time: 36.97847533226013\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 884us/step - loss: 0.3568 - accuracy: 0.8750\n",
      "Accuracy: 0.875 , Elapsed time: 52.45194053649902\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t6     \t0.312252\t0.342497\t0.391576\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 956us/step - loss: 0.3455 - accuracy: 0.8731\n",
      "Accuracy: 0.8730999827384949 , Elapsed time: 63.07110595703125\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 828us/step - loss: 0.3623 - accuracy: 0.8687\n",
      "Accuracy: 0.8687000274658203 , Elapsed time: 37.17971110343933\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 883us/step - loss: 0.3456 - accuracy: 0.8738\n",
      "Accuracy: 0.8737999796867371 , Elapsed time: 51.911401987075806\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 760us/step - loss: 0.4013 - accuracy: 0.8605\n",
      "Accuracy: 0.8604999780654907 , Elapsed time: 41.78486371040344\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 816us/step - loss: 0.3516 - accuracy: 0.8747\n",
      "Accuracy: 0.8747000098228455 , Elapsed time: 54.511274099349976\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 836us/step - loss: 0.3325 - accuracy: 0.8759\n",
      "Accuracy: 0.8758999705314636 , Elapsed time: 40.61135482788086\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t6     \t0.312252\t0.345423\t0.40134 \n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 997us/step - loss: 0.3246 - accuracy: 0.8862\n",
      "Accuracy: 0.8862000107765198 , Elapsed time: 71.82431411743164\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 853us/step - loss: 0.3309 - accuracy: 0.8792\n",
      "Accuracy: 0.8791999816894531 , Elapsed time: 40.59672832489014\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 871us/step - loss: 0.3395 - accuracy: 0.8759\n",
      "Accuracy: 0.8758999705314636 , Elapsed time: 48.34616279602051\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 42 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 972us/step - loss: 0.3506 - accuracy: 0.8708\n",
      "Accuracy: 0.8708000183105469 , Elapsed time: 52.36428737640381\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 43 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 959us/step - loss: 0.3141 - accuracy: 0.8896\n",
      "Accuracy: 0.8895999789237976 , Elapsed time: 72.6924774646759\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 44 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 784us/step - loss: 0.3334 - accuracy: 0.8772\n",
      "Accuracy: 0.8772000074386597 , Elapsed time: 37.04599189758301\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 45 ---------------\n",
      "Deep layers: 3 , Number of neurons: 150 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 838us/step - loss: 0.3279 - accuracy: 0.8804\n",
      "Accuracy: 0.8804000020027161 , Elapsed time: 54.5674684047699\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t7     \t0.312252\t0.329155\t0.350565\n",
      "\n",
      "--------------- Starting trial: 46 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 907us/step - loss: 0.3203 - accuracy: 0.8854\n",
      "Accuracy: 0.8853999972343445 , Elapsed time: 61.368940114974976\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 47 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 816us/step - loss: 0.3102 - accuracy: 0.8889\n",
      "Accuracy: 0.8888999819755554 , Elapsed time: 54.170961141586304\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 48 ---------------\n",
      "Deep layers: 3 , Number of neurons: 150 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 854us/step - loss: 0.5161 - accuracy: 0.8023\n",
      "Accuracy: 0.802299976348877 , Elapsed time: 54.72392010688782\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 49 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 958us/step - loss: 0.3248 - accuracy: 0.8842\n",
      "Accuracy: 0.8841999769210815 , Elapsed time: 71.92215704917908\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 50 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 854us/step - loss: 0.5154 - accuracy: 0.8029\n",
      "Accuracy: 0.8029000163078308 , Elapsed time: 53.94865798950195\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 51 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 944us/step - loss: 0.3647 - accuracy: 0.8686\n",
      "Accuracy: 0.8686000108718872 , Elapsed time: 71.13837361335754\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t6     \t0.310193\t0.372221\t0.516075\n",
      "\n",
      "--------------- Starting trial: 52 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 890us/step - loss: 0.3206 - accuracy: 0.8878\n",
      "Accuracy: 0.8877999782562256 , Elapsed time: 61.75194025039673\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 53 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 883us/step - loss: 0.3157 - accuracy: 0.8837\n",
      "Accuracy: 0.8837000131607056 , Elapsed time: 61.37836146354675\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 54 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 972us/step - loss: 0.3684 - accuracy: 0.8680\n",
      "Accuracy: 0.8679999709129333 , Elapsed time: 71.43123388290405\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 55 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 921us/step - loss: 0.3468 - accuracy: 0.8702\n",
      "Accuracy: 0.870199978351593 , Elapsed time: 62.01481866836548\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 56 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 862us/step - loss: 0.3317 - accuracy: 0.8781\n",
      "Accuracy: 0.8780999779701233 , Elapsed time: 44.07865858078003\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 57 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 942us/step - loss: 0.3132 - accuracy: 0.8863\n",
      "Accuracy: 0.8863000273704529 , Elapsed time: 71.67773985862732\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t6     \t0.310193\t0.327095\t0.368381\n",
      "\n",
      "--------------- Starting trial: 58 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 895us/step - loss: 0.3102 - accuracy: 0.8878\n",
      "Accuracy: 0.8877999782562256 , Elapsed time: 61.70019459724426\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 59 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 921us/step - loss: 0.3166 - accuracy: 0.8875\n",
      "Accuracy: 0.887499988079071 , Elapsed time: 70.06152391433716\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 60 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 829us/step - loss: 0.3182 - accuracy: 0.8831\n",
      "Accuracy: 0.8830999732017517 , Elapsed time: 53.49945521354675\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 61 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 894us/step - loss: 0.3086 - accuracy: 0.8881\n",
      "Accuracy: 0.8881000280380249 , Elapsed time: 62.48350715637207\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 62 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 822us/step - loss: 0.3243 - accuracy: 0.8822\n",
      "Accuracy: 0.8822000026702881 , Elapsed time: 53.89736604690552\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 63 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 792us/step - loss: 0.3222 - accuracy: 0.8845\n",
      "Accuracy: 0.8845000267028809 , Elapsed time: 47.86388802528381\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 64 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 810us/step - loss: 0.3241 - accuracy: 0.8824\n",
      "Accuracy: 0.8823999762535095 , Elapsed time: 53.40134882926941\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t7     \t0.30857 \t0.316804\t0.324297\n",
      "\n",
      "--------------- Starting trial: 65 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 784us/step - loss: 0.5572 - accuracy: 0.7855\n",
      "Accuracy: 0.7854999899864197 , Elapsed time: 39.57015037536621\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 66 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 835us/step - loss: 0.3141 - accuracy: 0.8864\n",
      "Accuracy: 0.8863999843597412 , Elapsed time: 54.1703097820282\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 67 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 872us/step - loss: 0.3221 - accuracy: 0.8823\n",
      "Accuracy: 0.8823000192642212 , Elapsed time: 62.69003176689148\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 68 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 882us/step - loss: 0.3117 - accuracy: 0.8878\n",
      "Accuracy: 0.8877999782562256 , Elapsed time: 54.00963044166565\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 69 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 812us/step - loss: 0.3315 - accuracy: 0.8787\n",
      "Accuracy: 0.8787000179290771 , Elapsed time: 47.85060358047485\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 70 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 956us/step - loss: 0.3126 - accuracy: 0.8895\n",
      "Accuracy: 0.8895000219345093 , Elapsed time: 70.8064615726471\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t6     \t0.30857 \t0.346799\t0.557232\n",
      "-- Best Individual =  [1, 1, 0, 0, 1, 0]\n",
      "-- Best Fitness =  0.3085699677467346\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 62.66667586565018 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 8  # max of individuals per generation\n",
    "max_generations = 10   # number of generations\n",
    "gene_length = 5      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:4])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[4:5])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    \n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1], \n",
    "          ', Learning rate:', best_learning_rate[-1], ', Batch size:', best_batch_size[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.314139</td>\n",
       "      <td>0.8896</td>\n",
       "      <td>72.692477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.312640</td>\n",
       "      <td>0.8895</td>\n",
       "      <td>70.806462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.310193</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>54.170961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.308570</td>\n",
       "      <td>0.8881</td>\n",
       "      <td>62.483507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.311658</td>\n",
       "      <td>0.8878</td>\n",
       "      <td>54.009630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.310249</td>\n",
       "      <td>0.8878</td>\n",
       "      <td>61.700195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.320556</td>\n",
       "      <td>0.8878</td>\n",
       "      <td>61.751940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.316628</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>70.061524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.314108</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>54.170310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.313243</td>\n",
       "      <td>0.8863</td>\n",
       "      <td>71.677740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.324594</td>\n",
       "      <td>0.8862</td>\n",
       "      <td>71.824314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.320663</td>\n",
       "      <td>0.8858</td>\n",
       "      <td>53.853305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.312252</td>\n",
       "      <td>0.8856</td>\n",
       "      <td>72.334026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.320294</td>\n",
       "      <td>0.8854</td>\n",
       "      <td>61.368940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.322192</td>\n",
       "      <td>0.8845</td>\n",
       "      <td>47.863888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.324781</td>\n",
       "      <td>0.8842</td>\n",
       "      <td>71.922157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.315697</td>\n",
       "      <td>0.8837</td>\n",
       "      <td>61.378361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.318178</td>\n",
       "      <td>0.8831</td>\n",
       "      <td>53.499455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.324127</td>\n",
       "      <td>0.8824</td>\n",
       "      <td>53.401349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.322077</td>\n",
       "      <td>0.8823</td>\n",
       "      <td>62.690032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.324297</td>\n",
       "      <td>0.8822</td>\n",
       "      <td>53.897366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.327100</td>\n",
       "      <td>0.8819</td>\n",
       "      <td>53.368231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.322109</td>\n",
       "      <td>0.8812</td>\n",
       "      <td>52.513985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.327914</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>54.567468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.329692</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>42.071065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.330877</td>\n",
       "      <td>0.8792</td>\n",
       "      <td>40.596728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.331477</td>\n",
       "      <td>0.8787</td>\n",
       "      <td>47.850604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.331730</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>44.078659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.333191</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>42.739608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.8777</td>\n",
       "      <td>43.239062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.339341</td>\n",
       "      <td>0.8776</td>\n",
       "      <td>53.516313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.333371</td>\n",
       "      <td>0.8772</td>\n",
       "      <td>37.045992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.345056</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>42.662141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.332510</td>\n",
       "      <td>0.8759</td>\n",
       "      <td>40.611355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.339524</td>\n",
       "      <td>0.8759</td>\n",
       "      <td>48.346163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.341830</td>\n",
       "      <td>0.8758</td>\n",
       "      <td>36.978475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.356752</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>52.451941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.351649</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>54.511274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.345574</td>\n",
       "      <td>0.8738</td>\n",
       "      <td>51.911402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.345514</td>\n",
       "      <td>0.8731</td>\n",
       "      <td>63.071106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.351525</td>\n",
       "      <td>0.8717</td>\n",
       "      <td>62.317088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.351511</td>\n",
       "      <td>0.8714</td>\n",
       "      <td>76.316792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.350565</td>\n",
       "      <td>0.8708</td>\n",
       "      <td>52.364287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.346766</td>\n",
       "      <td>0.8702</td>\n",
       "      <td>62.014819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.357494</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>80.840700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.362288</td>\n",
       "      <td>0.8687</td>\n",
       "      <td>37.179711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.364665</td>\n",
       "      <td>0.8686</td>\n",
       "      <td>71.138374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.368381</td>\n",
       "      <td>0.8680</td>\n",
       "      <td>71.431234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.367688</td>\n",
       "      <td>0.8662</td>\n",
       "      <td>53.721639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.370245</td>\n",
       "      <td>0.8656</td>\n",
       "      <td>52.320886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate      Loss  Accuracy  Elapsed time\n",
       "0             4        200         0.0010  0.314139    0.8896     72.692477\n",
       "1             4        200         0.0010  0.312640    0.8895     70.806462\n",
       "2             4        100         0.0010  0.310193    0.8889     54.170961\n",
       "3             4        150         0.0010  0.308570    0.8881     62.483507\n",
       "4             4        100         0.0010  0.311658    0.8878     54.009630\n",
       "5             4        150         0.0010  0.310249    0.8878     61.700195\n",
       "6             3        200         0.0010  0.320556    0.8878     61.751940\n",
       "7             4        200         0.0010  0.316628    0.8875     70.061524\n",
       "8             4        100         0.0010  0.314108    0.8864     54.170310\n",
       "9             4        200         0.0010  0.313243    0.8863     71.677740\n",
       "10            4        200         0.0010  0.324594    0.8862     71.824314\n",
       "11            2        200         0.0010  0.320663    0.8858     53.853305\n",
       "12            4        200         0.0010  0.312252    0.8856     72.334026\n",
       "13            3        200         0.0010  0.320294    0.8854     61.368940\n",
       "14            3        100         0.0010  0.322192    0.8845     47.863888\n",
       "15            4        200         0.0010  0.324781    0.8842     71.922157\n",
       "16            3        200         0.0010  0.315697    0.8837     61.378361\n",
       "17            4        100         0.0010  0.318178    0.8831     53.499455\n",
       "18            4        100         0.0010  0.324127    0.8824     53.401349\n",
       "19            4        150         0.0010  0.322077    0.8823     62.690032\n",
       "20            4        100         0.0010  0.324297    0.8822     53.897366\n",
       "21            2        200         0.0010  0.327100    0.8819     53.368231\n",
       "22            2        200         0.0010  0.322109    0.8812     52.513985\n",
       "23            3        150         0.0010  0.327914    0.8804     54.567468\n",
       "24            2        100         0.0100  0.329692    0.8796     42.071065\n",
       "25            3         50         0.0010  0.330877    0.8792     40.596728\n",
       "26            2        150         0.0010  0.331477    0.8787     47.850604\n",
       "27            4         50         0.0010  0.331730    0.8781     44.078659\n",
       "28            2        100         0.0100  0.333191    0.8779     42.739608\n",
       "29            2        100         0.0100  0.343284    0.8777     43.239062\n",
       "30            2        100         0.0100  0.339341    0.8776     53.516313\n",
       "31            2         50         0.0100  0.333371    0.8772     37.045992\n",
       "32            2        100         0.0100  0.345056    0.8770     42.662141\n",
       "33            3         50         0.0010  0.332510    0.8759     40.611355\n",
       "34            3        100         0.0100  0.339524    0.8759     48.346163\n",
       "35            2         50         0.0100  0.341830    0.8758     36.978475\n",
       "36            2        200         0.0100  0.356752    0.8750     52.451941\n",
       "37            4        100         0.0100  0.351649    0.8747     54.511274\n",
       "38            2        200         0.0100  0.345574    0.8738     51.911402\n",
       "39            4        150         0.0100  0.345514    0.8731     63.071106\n",
       "40            4        150         0.0001  0.351525    0.8717     62.317088\n",
       "41            4        200         0.0100  0.351511    0.8714     76.316792\n",
       "42            2        200         0.0100  0.350565    0.8708     52.364287\n",
       "43            4        150         0.0010  0.346766    0.8702     62.014819\n",
       "44            4        150         0.0001  0.357494    0.8689     80.840700\n",
       "45            2         50         0.0010  0.362288    0.8687     37.179711\n",
       "46            4        200         0.0001  0.364665    0.8686     71.138374\n",
       "47            4        200         0.0100  0.368381    0.8680     71.431234\n",
       "48            4        100         0.0001  0.367688    0.8662     53.721639\n",
       "49            2        200         0.0100  0.370245    0.8656     52.320886"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_sdss_3.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Accuracy\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Accuracy\", \"Elapsed time\"], ascending=[0,1], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 62.663 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
