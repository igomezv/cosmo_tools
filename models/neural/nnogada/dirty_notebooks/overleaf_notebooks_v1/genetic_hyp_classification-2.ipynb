{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>delta</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135.689107</td>\n",
       "      <td>32.494632</td>\n",
       "      <td>23.87882</td>\n",
       "      <td>22.27530</td>\n",
       "      <td>20.39501</td>\n",
       "      <td>19.16573</td>\n",
       "      <td>18.79371</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144.826101</td>\n",
       "      <td>31.274185</td>\n",
       "      <td>24.77759</td>\n",
       "      <td>22.83188</td>\n",
       "      <td>22.58444</td>\n",
       "      <td>21.16812</td>\n",
       "      <td>21.61427</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142.188790</td>\n",
       "      <td>35.582444</td>\n",
       "      <td>25.26307</td>\n",
       "      <td>22.66389</td>\n",
       "      <td>20.60976</td>\n",
       "      <td>19.34857</td>\n",
       "      <td>18.94827</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>338.741038</td>\n",
       "      <td>-0.402828</td>\n",
       "      <td>22.13682</td>\n",
       "      <td>23.77656</td>\n",
       "      <td>21.61162</td>\n",
       "      <td>20.50454</td>\n",
       "      <td>19.25010</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345.282593</td>\n",
       "      <td>21.183866</td>\n",
       "      <td>19.43718</td>\n",
       "      <td>17.58028</td>\n",
       "      <td>16.49747</td>\n",
       "      <td>15.97711</td>\n",
       "      <td>15.54461</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        alpha      delta         u         g         r         i         z  \\\n",
       "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
       "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
       "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
       "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
       "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
       "\n",
       "    class  \n",
       "0  GALAXY  \n",
       "1  GALAXY  \n",
       "2  GALAXY  \n",
       "3  GALAXY  \n",
       "4  GALAXY  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/igomezv/nnogada/main/data/star_classification.csv\"\n",
    "data = pd.read_csv(url)\n",
    "cols = ['alpha','delta','u','g','r','i','z','class']\n",
    "data = data[cols]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        alpha      delta         u         g         r         i         z  \\\n",
      "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
      "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
      "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
      "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
      "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
      "\n",
      "   class  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n"
     ]
    }
   ],
   "source": [
    "data[\"class\"]=[0 if i == \"GALAXY\" else 1 if i == \"STAR\" else 2 for i in data[\"class\"]]\n",
    "print(data.head())\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function layers\n",
    "\n",
    "# f1 = lambda x: Dense(x, activation='relu')      #ReLU\n",
    "# f2 = lambda x: Dense(x, activation='elu')       #ELU\n",
    "# f3 = lambda x: keras.layers.LeakyReLU(0.3)      #LReLU\n",
    "# f4 = lambda x: Dense(x, kernel_initializer='lecun_normal', activation='selu')   #SELU\n",
    "\n",
    "# f_names = [\"ReLU\", \"ELU\", \"LReLU\", \"SELU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([3, 4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([200, 100]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-5,1e-4,1e-3, 0.01])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([128,256]) \n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=50,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=1)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 50\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into X and Y and implement hot_ones in Y\n",
    "def prepare_dataset(data):\n",
    "    X, Y = np.empty((0)), np.empty((0))\n",
    "    X = data[:,0:7]\n",
    "    Y = data[:,7]\n",
    "    Y = to_categorical(Y, num_classes=3)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation and test sets\n",
    "X,Y = prepare_dataset(data)\n",
    "\n",
    "# Defines ratios, w.r.t. whole dataset.\n",
    "ratio_train = 0.8\n",
    "ratio_val = 0.1\n",
    "ratio_test = 0.1\n",
    "\n",
    "# Produces test split.\n",
    "x_, X_test, y_, Y_test = split(X, Y, test_size = ratio_test, random_state=0)\n",
    "\n",
    "# Adjusts val ratio, w.r.t. remaining dataset.\n",
    "ratio_remaining = 1 - ratio_test\n",
    "ratio_val_adjusted = ratio_val / ratio_remaining\n",
    "\n",
    "# Produces train and val splits.\n",
    "X_train, X_val, Y_train, Y_val = split(x_, y_, test_size=ratio_val_adjusted, random_state=0)\n",
    "\n",
    "# Normalize and scale the input sets.\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "X_val   = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:1])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[1:2])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[2:4])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[4:5])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(num_units, input_shape=(int(X_train.shape[1]),)))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(3, activation=tf.nn.softmax))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=1, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Accuracy:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5      # probability for crossover\n",
    "    P_MUTATION = 0.5         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1  # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3418 - accuracy: 0.8681\n",
      "Accuracy: 0.8680999875068665 , Elapsed time: 66.18607974052429\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3537 - accuracy: 0.8736\n",
      "Accuracy: 0.8736000061035156 , Elapsed time: 92.18644499778748\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3401 - accuracy: 0.8809\n",
      "Accuracy: 0.8809000253677368 , Elapsed time: 66.2889244556427\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.8781\n",
      "Accuracy: 0.8780999779701233 , Elapsed time: 52.643043994903564\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3744 - accuracy: 0.8637\n",
      "Accuracy: 0.8636999726295471 , Elapsed time: 64.95815205574036\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3500 - accuracy: 0.8759\n",
      "Accuracy: 0.8758999705314636 , Elapsed time: 91.42904281616211\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3816 - accuracy: 0.8605\n",
      "Accuracy: 0.8604999780654907 , Elapsed time: 60.36984300613403\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3721 - accuracy: 0.8568\n",
      "Accuracy: 0.8568000197410583 , Elapsed time: 42.295509576797485\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin    \tavg     \tmax     \n",
      "0  \t8     \t0.33387\t0.355962\t0.381616\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3430 - accuracy: 0.8767\n",
      "Accuracy: 0.8766999840736389 , Elapsed time: 41.160865783691406\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t1     \t0.33387\t0.351129\t0.37211 \n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3440 - accuracy: 0.8762\n",
      "Accuracy: 0.8762000203132629 , Elapsed time: 78.93900609016418\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3453 - accuracy: 0.8729\n",
      "Accuracy: 0.8729000091552734 , Elapsed time: 42.22618842124939\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3386 - accuracy: 0.8759\n",
      "Accuracy: 0.8758999705314636 , Elapsed time: 67.28417301177979\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3471 - accuracy: 0.8734\n",
      "Accuracy: 0.8733999729156494 , Elapsed time: 58.943113803863525\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3361 - accuracy: 0.8765\n",
      "Accuracy: 0.8765000104904175 , Elapsed time: 43.25183296203613\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3841 - accuracy: 0.8652\n",
      "Accuracy: 0.8651999831199646 , Elapsed time: 80.06116938591003\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t6     \t0.33387\t0.346139\t0.38414 \n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3619 - accuracy: 0.8646\n",
      "Accuracy: 0.8646000027656555 , Elapsed time: 66.1145715713501\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8830\n",
      "Accuracy: 0.8830000162124634 , Elapsed time: 36.44920253753662\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3692 - accuracy: 0.8666\n",
      "Accuracy: 0.866599977016449 , Elapsed time: 51.86169505119324\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3359 - accuracy: 0.8756\n",
      "Accuracy: 0.8755999803543091 , Elapsed time: 52.37738060951233\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t4     \t0.322051\t0.343526\t0.3692  \n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3503 - accuracy: 0.8670\n",
      "Accuracy: 0.8669999837875366 , Elapsed time: 52.23947191238403\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3797 - accuracy: 0.8589\n",
      "Accuracy: 0.8589000105857849 , Elapsed time: 52.47543978691101\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8761\n",
      "Accuracy: 0.8761000037193298 , Elapsed time: 41.12067651748657\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3230 - accuracy: 0.8833\n",
      "Accuracy: 0.8833000063896179 , Elapsed time: 41.54266834259033\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.8773\n",
      "Accuracy: 0.8773000240325928 , Elapsed time: 37.67718577384949\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t5     \t0.322051\t0.339711\t0.37974 \n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3394 - accuracy: 0.8771\n",
      "Accuracy: 0.8770999908447266 , Elapsed time: 37.49925088882446\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3486 - accuracy: 0.8701\n",
      "Accuracy: 0.8701000213623047 , Elapsed time: 52.393239974975586\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3169 - accuracy: 0.8855\n",
      "Accuracy: 0.8855000138282776 , Elapsed time: 63.10061812400818\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.5748 - accuracy: 0.7837\n",
      "Accuracy: 0.7836999893188477 , Elapsed time: 37.70667028427124\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3433 - accuracy: 0.8754\n",
      "Accuracy: 0.8754000067710876 , Elapsed time: 82.74072742462158\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t5     \t0.316914\t0.362989\t0.574817\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3675 - accuracy: 0.8611\n",
      "Accuracy: 0.8611000180244446 , Elapsed time: 80.98010873794556\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3370 - accuracy: 0.8793\n",
      "Accuracy: 0.8792999982833862 , Elapsed time: 82.09261965751648\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4069 - accuracy: 0.8488\n",
      "Accuracy: 0.848800003528595 , Elapsed time: 38.53517770767212\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t3     \t0.316914\t0.342178\t0.406903\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3199 - accuracy: 0.8824\n",
      "Accuracy: 0.8823999762535095 , Elapsed time: 62.97182869911194\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3126 - accuracy: 0.8861\n",
      "Accuracy: 0.8860999941825867 , Elapsed time: 64.83854866027832\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3795 - accuracy: 0.8607\n",
      "Accuracy: 0.8607000112533569 , Elapsed time: 55.04101014137268\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3296 - accuracy: 0.8795\n",
      "Accuracy: 0.8794999718666077 , Elapsed time: 38.67044973373413\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3290 - accuracy: 0.8845\n",
      "Accuracy: 0.8845000267028809 , Elapsed time: 97.33291435241699\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3823 - accuracy: 0.8635\n",
      "Accuracy: 0.8634999990463257 , Elapsed time: 53.85362911224365\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3915 - accuracy: 0.8576\n",
      "Accuracy: 0.8575999736785889 , Elapsed time: 43.6172935962677\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t7     \t0.312589\t0.345166\t0.391506\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8837\n",
      "Accuracy: 0.8837000131607056 , Elapsed time: 63.81658673286438\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8818\n",
      "Accuracy: 0.8817999958992004 , Elapsed time: 66.00565791130066\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 42 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3250 - accuracy: 0.8813\n",
      "Accuracy: 0.8812999725341797 , Elapsed time: 50.63113594055176\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 43 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 982us/step - loss: 0.3276 - accuracy: 0.8793\n",
      "Accuracy: 0.8792999982833862 , Elapsed time: 51.31545448303223\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 44 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 887us/step - loss: 0.3679 - accuracy: 0.8670\n",
      "Accuracy: 0.8669999837875366 , Elapsed time: 30.73652148246765\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t5     \t0.316914\t0.328684\t0.36794 \n",
      "\n",
      "--------------- Starting trial: 45 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3271 - accuracy: 0.8801\n",
      "Accuracy: 0.8801000118255615 , Elapsed time: 49.39158749580383\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 46 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 926us/step - loss: 0.3143 - accuracy: 0.8837\n",
      "Accuracy: 0.8837000131607056 , Elapsed time: 49.98030066490173\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 47 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3236 - accuracy: 0.8797\n",
      "Accuracy: 0.8797000050544739 , Elapsed time: 53.78217434883118\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 48 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3238 - accuracy: 0.8816\n",
      "Accuracy: 0.881600022315979 , Elapsed time: 50.31403875350952\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 49 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3249 - accuracy: 0.8805\n",
      "Accuracy: 0.8805000185966492 , Elapsed time: 50.236674785614014\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t5     \t0.314333\t0.322077\t0.32903 \n",
      "\n",
      "--------------- Starting trial: 50 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 988us/step - loss: 0.3228 - accuracy: 0.8843\n",
      "Accuracy: 0.8842999935150146 , Elapsed time: 74.5244767665863\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 51 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 913us/step - loss: 0.5383 - accuracy: 0.7918\n",
      "Accuracy: 0.7918000221252441 , Elapsed time: 34.36917853355408\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 52 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 723us/step - loss: 0.3185 - accuracy: 0.8833\n",
      "Accuracy: 0.8833000063896179 , Elapsed time: 43.02338743209839\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 53 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 740us/step - loss: 0.3194 - accuracy: 0.8828\n",
      "Accuracy: 0.8827999830245972 , Elapsed time: 41.76996088027954\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 54 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 816us/step - loss: 0.5476 - accuracy: 0.7899\n",
      "Accuracy: 0.789900004863739 , Elapsed time: 36.36767077445984\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t5     \t0.314333\t0.375618\t0.54756 \n",
      "-- Best Individual =  [1, 0, 1, 0, 1]\n",
      "-- Best Fitness =  0.316914439201355\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 50.29766321182251 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 8  # max of individuals per generation\n",
    "max_generations = 10   # number of generations\n",
    "gene_length = 5      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001 , Batch size: 256\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "# best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:4])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[4:5])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    \n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1], \n",
    "          ', Learning rate:', best_learning_rate[-1], ', Batch size:', best_batch_size[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.312589</td>\n",
       "      <td>0.8861</td>\n",
       "      <td>64.838549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.316914</td>\n",
       "      <td>0.8855</td>\n",
       "      <td>63.100618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.329030</td>\n",
       "      <td>0.8845</td>\n",
       "      <td>97.332914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.322762</td>\n",
       "      <td>0.8843</td>\n",
       "      <td>74.524477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.314333</td>\n",
       "      <td>0.8837</td>\n",
       "      <td>49.980301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.321424</td>\n",
       "      <td>0.8837</td>\n",
       "      <td>63.816587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.322958</td>\n",
       "      <td>0.8833</td>\n",
       "      <td>41.542668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.318548</td>\n",
       "      <td>0.8833</td>\n",
       "      <td>43.023387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.322051</td>\n",
       "      <td>0.8830</td>\n",
       "      <td>36.449203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.319379</td>\n",
       "      <td>0.8828</td>\n",
       "      <td>41.769961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.319869</td>\n",
       "      <td>0.8824</td>\n",
       "      <td>62.971829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.321670</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>66.005658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.323806</td>\n",
       "      <td>0.8816</td>\n",
       "      <td>50.314039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.324975</td>\n",
       "      <td>0.8813</td>\n",
       "      <td>50.631136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.340147</td>\n",
       "      <td>0.8809</td>\n",
       "      <td>66.288924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.324880</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>50.236675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.327113</td>\n",
       "      <td>0.8801</td>\n",
       "      <td>49.391587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.323622</td>\n",
       "      <td>0.8797</td>\n",
       "      <td>53.782174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.329586</td>\n",
       "      <td>0.8795</td>\n",
       "      <td>38.670450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.327646</td>\n",
       "      <td>0.8793</td>\n",
       "      <td>51.315454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.336965</td>\n",
       "      <td>0.8793</td>\n",
       "      <td>82.092620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.333870</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>52.643044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.335161</td>\n",
       "      <td>0.8773</td>\n",
       "      <td>37.677186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.339374</td>\n",
       "      <td>0.8771</td>\n",
       "      <td>37.499251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.342973</td>\n",
       "      <td>0.8767</td>\n",
       "      <td>41.160866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.336055</td>\n",
       "      <td>0.8765</td>\n",
       "      <td>43.251833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.343972</td>\n",
       "      <td>0.8762</td>\n",
       "      <td>78.939006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.335544</td>\n",
       "      <td>0.8761</td>\n",
       "      <td>41.120677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.338563</td>\n",
       "      <td>0.8759</td>\n",
       "      <td>67.284173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.350018</td>\n",
       "      <td>0.8759</td>\n",
       "      <td>91.429043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.335897</td>\n",
       "      <td>0.8756</td>\n",
       "      <td>52.377381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.343285</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>82.740727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.353714</td>\n",
       "      <td>0.8736</td>\n",
       "      <td>92.186445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.347080</td>\n",
       "      <td>0.8734</td>\n",
       "      <td>58.943114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.345289</td>\n",
       "      <td>0.8729</td>\n",
       "      <td>42.226188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.348619</td>\n",
       "      <td>0.8701</td>\n",
       "      <td>52.393240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.341847</td>\n",
       "      <td>0.8681</td>\n",
       "      <td>66.186080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.367940</td>\n",
       "      <td>0.8670</td>\n",
       "      <td>30.736521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.350280</td>\n",
       "      <td>0.8670</td>\n",
       "      <td>52.239472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>0.8666</td>\n",
       "      <td>51.861695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.384140</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>80.061169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.361873</td>\n",
       "      <td>0.8646</td>\n",
       "      <td>66.114572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.374377</td>\n",
       "      <td>0.8637</td>\n",
       "      <td>64.958152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>256</td>\n",
       "      <td>0.382298</td>\n",
       "      <td>0.8635</td>\n",
       "      <td>53.853629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.367538</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>80.980109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>256</td>\n",
       "      <td>0.379533</td>\n",
       "      <td>0.8607</td>\n",
       "      <td>55.041010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.381616</td>\n",
       "      <td>0.8605</td>\n",
       "      <td>60.369843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.379740</td>\n",
       "      <td>0.8589</td>\n",
       "      <td>52.475440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>256</td>\n",
       "      <td>0.391506</td>\n",
       "      <td>0.8576</td>\n",
       "      <td>43.617294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>256</td>\n",
       "      <td>0.372110</td>\n",
       "      <td>0.8568</td>\n",
       "      <td>42.295510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>256</td>\n",
       "      <td>0.406903</td>\n",
       "      <td>0.8488</td>\n",
       "      <td>38.535178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>256</td>\n",
       "      <td>0.538335</td>\n",
       "      <td>0.7918</td>\n",
       "      <td>34.369179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>256</td>\n",
       "      <td>0.547560</td>\n",
       "      <td>0.7899</td>\n",
       "      <td>36.367671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>256</td>\n",
       "      <td>0.574817</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>37.706670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss  Accuracy  \\\n",
       "0             4        200        0.00100         256  0.312589    0.8861   \n",
       "1             4        200        0.00100         256  0.316914    0.8855   \n",
       "2             4        200        0.00100         128  0.329030    0.8845   \n",
       "3             4        200        0.00100         128  0.322762    0.8843   \n",
       "4             4        200        0.00100         256  0.314333    0.8837   \n",
       "5             4        200        0.00100         256  0.321424    0.8837   \n",
       "6             4        100        0.00100         256  0.322958    0.8833   \n",
       "7             4        200        0.00100         256  0.318548    0.8833   \n",
       "8             3        100        0.01000         256  0.322051    0.8830   \n",
       "9             4        200        0.00100         256  0.319379    0.8828   \n",
       "10            4        200        0.00100         256  0.319869    0.8824   \n",
       "11            4        200        0.00100         256  0.321670    0.8818   \n",
       "12            4        200        0.00100         256  0.323806    0.8816   \n",
       "13            4        200        0.00100         256  0.324975    0.8813   \n",
       "14            4        100        0.01000         128  0.340147    0.8809   \n",
       "15            4        200        0.00100         256  0.324880    0.8805   \n",
       "16            4        200        0.00100         256  0.327113    0.8801   \n",
       "17            4        200        0.00100         256  0.323622    0.8797   \n",
       "18            3        100        0.01000         256  0.329586    0.8795   \n",
       "19            4        200        0.00100         256  0.327646    0.8793   \n",
       "20            3        200        0.01000         128  0.336965    0.8793   \n",
       "21            3        200        0.01000         256  0.333870    0.8781   \n",
       "22            3        100        0.01000         256  0.335161    0.8773   \n",
       "23            3        100        0.01000         256  0.339374    0.8771   \n",
       "24            4        100        0.01000         256  0.342973    0.8767   \n",
       "25            4        100        0.01000         256  0.336055    0.8765   \n",
       "26            3        200        0.01000         128  0.343972    0.8762   \n",
       "27            4        100        0.01000         256  0.335544    0.8761   \n",
       "28            4        100        0.01000         128  0.338563    0.8759   \n",
       "29            4        200        0.01000         128  0.350018    0.8759   \n",
       "30            3        200        0.01000         256  0.335897    0.8756   \n",
       "31            3        200        0.01000         128  0.343285    0.8754   \n",
       "32            4        200        0.01000         128  0.353714    0.8736   \n",
       "33            3        100        0.01000         128  0.347080    0.8734   \n",
       "34            4        100        0.01000         256  0.345289    0.8729   \n",
       "35            3        200        0.01000         256  0.348619    0.8701   \n",
       "36            4        100        0.01000         128  0.341847    0.8681   \n",
       "37            3        100        0.01000         256  0.367940    0.8670   \n",
       "38            3        200        0.01000         256  0.350280    0.8670   \n",
       "39            3        200        0.01000         256  0.369200    0.8666   \n",
       "40            3        200        0.01000         128  0.384140    0.8652   \n",
       "41            4        100        0.01000         128  0.361873    0.8646   \n",
       "42            4        100        0.00010         128  0.374377    0.8637   \n",
       "43            3        200        0.00010         256  0.382298    0.8635   \n",
       "44            3        200        0.01000         128  0.367538    0.8611   \n",
       "45            3        200        0.00010         256  0.379533    0.8607   \n",
       "46            3        100        0.00010         128  0.381616    0.8605   \n",
       "47            3        200        0.01000         256  0.379740    0.8589   \n",
       "48            4        100        0.00010         256  0.391506    0.8576   \n",
       "49            4        100        0.01000         256  0.372110    0.8568   \n",
       "50            3        100        0.00010         256  0.406903    0.8488   \n",
       "51            4        100        0.00001         256  0.538335    0.7918   \n",
       "52            3        200        0.00001         256  0.547560    0.7899   \n",
       "53            3        100        0.00001         256  0.574817    0.7837   \n",
       "\n",
       "    Elapsed time  \n",
       "0      64.838549  \n",
       "1      63.100618  \n",
       "2      97.332914  \n",
       "3      74.524477  \n",
       "4      49.980301  \n",
       "5      63.816587  \n",
       "6      41.542668  \n",
       "7      43.023387  \n",
       "8      36.449203  \n",
       "9      41.769961  \n",
       "10     62.971829  \n",
       "11     66.005658  \n",
       "12     50.314039  \n",
       "13     50.631136  \n",
       "14     66.288924  \n",
       "15     50.236675  \n",
       "16     49.391587  \n",
       "17     53.782174  \n",
       "18     38.670450  \n",
       "19     51.315454  \n",
       "20     82.092620  \n",
       "21     52.643044  \n",
       "22     37.677186  \n",
       "23     37.499251  \n",
       "24     41.160866  \n",
       "25     43.251833  \n",
       "26     78.939006  \n",
       "27     41.120677  \n",
       "28     67.284173  \n",
       "29     91.429043  \n",
       "30     52.377381  \n",
       "31     82.740727  \n",
       "32     92.186445  \n",
       "33     58.943114  \n",
       "34     42.226188  \n",
       "35     52.393240  \n",
       "36     66.186080  \n",
       "37     30.736521  \n",
       "38     52.239472  \n",
       "39     51.861695  \n",
       "40     80.061169  \n",
       "41     66.114572  \n",
       "42     64.958152  \n",
       "43     53.853629  \n",
       "44     80.980109  \n",
       "45     55.041010  \n",
       "46     60.369843  \n",
       "47     52.475440  \n",
       "48     43.617294  \n",
       "49     42.295510  \n",
       "50     38.535178  \n",
       "51     34.369179  \n",
       "52     36.367671  \n",
       "53     37.706670  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_sdss_2.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Accuracy\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Accuracy\", \"Elapsed time\"], ascending=[0,1], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 50.295 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
