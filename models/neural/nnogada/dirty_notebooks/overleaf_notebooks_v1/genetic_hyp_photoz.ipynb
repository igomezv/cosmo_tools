{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 22:38:51.483068: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-08 22:38:51.583624: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-08 22:38:51.583644: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-08 22:38:52.246575: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-08 22:38:52.246663: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-08 22:38:52.246676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import astropy\n",
    "from astropy.io import fits\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open('https://github.com/igomezv/cosmo_tools/blob/main/COB_22/Viviana_Acquaviva/DEEP2_uniq_Terapix_Subaru_v1.fits?raw=true') as data:\n",
    "    df = pd.DataFrame(np.array(data[1].data).byteswap().newbyteorder()) #see https://numpy.org/devdocs/user/basics.byteswapping.html#changing-byte-ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objno_deep2</th>\n",
       "      <th>ra_deep2</th>\n",
       "      <th>dec_deep2</th>\n",
       "      <th>magb</th>\n",
       "      <th>magr</th>\n",
       "      <th>magi</th>\n",
       "      <th>pgal</th>\n",
       "      <th>sfd_ebv</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "      <th>...</th>\n",
       "      <th>ra_subaru</th>\n",
       "      <th>dec_subaru</th>\n",
       "      <th>y</th>\n",
       "      <th>yerr</th>\n",
       "      <th>y_apercor</th>\n",
       "      <th>yerr_aper</th>\n",
       "      <th>yerr_apercor</th>\n",
       "      <th>y(sexflag)</th>\n",
       "      <th>y_radius_arcsec</th>\n",
       "      <th>subaru_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11001673</td>\n",
       "      <td>213.868704</td>\n",
       "      <td>51.956445</td>\n",
       "      <td>23.487745</td>\n",
       "      <td>23.143082</td>\n",
       "      <td>22.582092</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010943</td>\n",
       "      <td>b''</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>213.868626</td>\n",
       "      <td>51.956443</td>\n",
       "      <td>21.869627</td>\n",
       "      <td>0.060918</td>\n",
       "      <td>21.926356</td>\n",
       "      <td>0.041955</td>\n",
       "      <td>0.141778</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.656514</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11001699</td>\n",
       "      <td>213.810471</td>\n",
       "      <td>51.942316</td>\n",
       "      <td>22.067692</td>\n",
       "      <td>20.034674</td>\n",
       "      <td>19.545080</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.011014</td>\n",
       "      <td>b'GALAXY'</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>213.810455</td>\n",
       "      <td>51.942321</td>\n",
       "      <td>18.757229</td>\n",
       "      <td>0.005813</td>\n",
       "      <td>18.811085</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.050987</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.744269</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11001770</td>\n",
       "      <td>213.848431</td>\n",
       "      <td>51.948876</td>\n",
       "      <td>24.144438</td>\n",
       "      <td>24.103180</td>\n",
       "      <td>24.020006</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>b'GALAXY'</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11001800</td>\n",
       "      <td>213.831758</td>\n",
       "      <td>51.952548</td>\n",
       "      <td>25.336836</td>\n",
       "      <td>23.508480</td>\n",
       "      <td>23.081087</td>\n",
       "      <td>0.509809</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>b'GALAXY'</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>213.831766</td>\n",
       "      <td>51.952544</td>\n",
       "      <td>22.404269</td>\n",
       "      <td>0.088970</td>\n",
       "      <td>22.535600</td>\n",
       "      <td>0.053497</td>\n",
       "      <td>0.094733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.455820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11001860</td>\n",
       "      <td>213.832550</td>\n",
       "      <td>51.954174</td>\n",
       "      <td>24.382738</td>\n",
       "      <td>23.401484</td>\n",
       "      <td>22.572845</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>b'GALAXY'</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>213.832574</td>\n",
       "      <td>51.954175</td>\n",
       "      <td>22.242717</td>\n",
       "      <td>0.070760</td>\n",
       "      <td>22.100980</td>\n",
       "      <td>0.033256</td>\n",
       "      <td>0.073067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.442022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   objno_deep2    ra_deep2  dec_deep2       magb       magr       magi  \\\n",
       "0     11001673  213.868704  51.956445  23.487745  23.143082  22.582092   \n",
       "1     11001699  213.810471  51.942316  22.067692  20.034674  19.545080   \n",
       "2     11001770  213.848431  51.948876  24.144438  24.103180  24.020006   \n",
       "3     11001800  213.831758  51.952548  25.336836  23.508480  23.081087   \n",
       "4     11001860  213.832550  51.954174  24.382738  23.401484  22.572845   \n",
       "\n",
       "       pgal   sfd_ebv      class subclass  ...   ra_subaru dec_subaru  \\\n",
       "0  1.000000  0.010943        b''      b''  ...  213.868626  51.956443   \n",
       "1  3.000000  0.011014  b'GALAXY'      b''  ...  213.810455  51.942321   \n",
       "2  3.000000  0.010856  b'GALAXY'      b''  ...  -99.000000 -99.000000   \n",
       "3  0.509809  0.010823  b'GALAXY'      b''  ...  213.831766  51.952544   \n",
       "4  3.000000  0.010827  b'GALAXY'      b''  ...  213.832574  51.954175   \n",
       "\n",
       "           y       yerr  y_apercor  yerr_aper  yerr_apercor  y(sexflag)  \\\n",
       "0  21.869627   0.060918  21.926356   0.041955      0.141778         3.0   \n",
       "1  18.757229   0.005813  18.811085   0.004386      0.050987         3.0   \n",
       "2 -99.000000 -99.000000 -99.000000 -99.000000    -99.000000       -99.0   \n",
       "3  22.404269   0.088970  22.535600   0.053497      0.094733         0.0   \n",
       "4  22.242717   0.070760  22.100980   0.033256      0.073067         0.0   \n",
       "\n",
       "   y_radius_arcsec  subaru_source  \n",
       "0         0.656514              1  \n",
       "1         0.744269              1  \n",
       "2       -99.000000            -99  \n",
       "3         0.455820              1  \n",
       "4         0.442022              1  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_apercor</th>\n",
       "      <th>g_apercor</th>\n",
       "      <th>r_apercor</th>\n",
       "      <th>i_apercor</th>\n",
       "      <th>z_apercor</th>\n",
       "      <th>y_apercor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.543491</td>\n",
       "      <td>23.430495</td>\n",
       "      <td>23.100311</td>\n",
       "      <td>22.768970</td>\n",
       "      <td>22.223810</td>\n",
       "      <td>21.926356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.848978</td>\n",
       "      <td>28.989668</td>\n",
       "      <td>19.027422</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>18.811085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.324670</td>\n",
       "      <td>24.273606</td>\n",
       "      <td>24.150319</td>\n",
       "      <td>23.446252</td>\n",
       "      <td>23.574236</td>\n",
       "      <td>-99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>24.804309</td>\n",
       "      <td>23.636544</td>\n",
       "      <td>23.009222</td>\n",
       "      <td>22.689591</td>\n",
       "      <td>22.535600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.362068</td>\n",
       "      <td>24.136913</td>\n",
       "      <td>23.490342</td>\n",
       "      <td>22.777181</td>\n",
       "      <td>22.319676</td>\n",
       "      <td>22.100980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   u_apercor  g_apercor  r_apercor  i_apercor  z_apercor  y_apercor\n",
       "0  23.543491  23.430495  23.100311  22.768970  22.223810  21.926356\n",
       "1  30.848978  28.989668  19.027422  99.000000  99.000000  18.811085\n",
       "2  24.324670  24.273606  24.150319  23.446252  23.574236 -99.000000\n",
       "3  99.000000  24.804309  23.636544  23.009222  22.689591  22.535600\n",
       "4  24.362068  24.136913  23.490342  22.777181  22.319676  22.100980"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df[['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor']]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.115261\n",
       "1    0.290608\n",
       "2    0.605744\n",
       "3    1.306796\n",
       "4    0.957669\n",
       "Name: zhelio, dtype: float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df['zhelio']\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = df[['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor','zquality','cfhtls_source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16857, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mags = mags[mags['zquality'] >= 3]\n",
    "\n",
    "mags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = mags[mags > -10].dropna()\n",
    "mags = mags[mags < 90].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = mags[mags['cfhtls_source'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor']\n",
    "features_ext = mags.copy()\n",
    "for i, name1 in enumerate(params):\n",
    "    for j, name2 in enumerate(params):\n",
    "        if i >=j: continue #build only one pair, avoid zero colors\n",
    "        features_ext[name1 + '-' + name2] = features[name1] - features[name2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_apercor</th>\n",
       "      <th>g_apercor</th>\n",
       "      <th>r_apercor</th>\n",
       "      <th>i_apercor</th>\n",
       "      <th>z_apercor</th>\n",
       "      <th>y_apercor</th>\n",
       "      <th>zquality</th>\n",
       "      <th>cfhtls_source</th>\n",
       "      <th>u_apercor-g_apercor</th>\n",
       "      <th>u_apercor-r_apercor</th>\n",
       "      <th>...</th>\n",
       "      <th>g_apercor-r_apercor</th>\n",
       "      <th>g_apercor-i_apercor</th>\n",
       "      <th>g_apercor-z_apercor</th>\n",
       "      <th>g_apercor-y_apercor</th>\n",
       "      <th>r_apercor-i_apercor</th>\n",
       "      <th>r_apercor-z_apercor</th>\n",
       "      <th>r_apercor-y_apercor</th>\n",
       "      <th>i_apercor-z_apercor</th>\n",
       "      <th>i_apercor-y_apercor</th>\n",
       "      <th>z_apercor-y_apercor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>24.244393</td>\n",
       "      <td>23.979583</td>\n",
       "      <td>23.522136</td>\n",
       "      <td>22.911041</td>\n",
       "      <td>22.525773</td>\n",
       "      <td>22.329098</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264811</td>\n",
       "      <td>0.722258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457447</td>\n",
       "      <td>1.068542</td>\n",
       "      <td>1.453810</td>\n",
       "      <td>1.650485</td>\n",
       "      <td>0.611094</td>\n",
       "      <td>0.996362</td>\n",
       "      <td>1.193037</td>\n",
       "      <td>0.385268</td>\n",
       "      <td>0.581943</td>\n",
       "      <td>0.196675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>24.489104</td>\n",
       "      <td>23.916151</td>\n",
       "      <td>22.923651</td>\n",
       "      <td>21.873752</td>\n",
       "      <td>21.306495</td>\n",
       "      <td>21.251440</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.572953</td>\n",
       "      <td>1.565453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>2.042399</td>\n",
       "      <td>2.609656</td>\n",
       "      <td>2.664711</td>\n",
       "      <td>1.049899</td>\n",
       "      <td>1.617157</td>\n",
       "      <td>1.672212</td>\n",
       "      <td>0.567258</td>\n",
       "      <td>0.622312</td>\n",
       "      <td>0.055055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>24.873959</td>\n",
       "      <td>22.973893</td>\n",
       "      <td>21.465850</td>\n",
       "      <td>20.788420</td>\n",
       "      <td>20.462283</td>\n",
       "      <td>20.413696</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.900066</td>\n",
       "      <td>3.408108</td>\n",
       "      <td>...</td>\n",
       "      <td>1.508042</td>\n",
       "      <td>2.185473</td>\n",
       "      <td>2.511610</td>\n",
       "      <td>2.560196</td>\n",
       "      <td>0.677430</td>\n",
       "      <td>1.003568</td>\n",
       "      <td>1.052154</td>\n",
       "      <td>0.326138</td>\n",
       "      <td>0.374724</td>\n",
       "      <td>0.048586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>24.529042</td>\n",
       "      <td>24.338631</td>\n",
       "      <td>23.891189</td>\n",
       "      <td>23.206102</td>\n",
       "      <td>22.989344</td>\n",
       "      <td>23.112382</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190411</td>\n",
       "      <td>0.637853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447442</td>\n",
       "      <td>1.132529</td>\n",
       "      <td>1.349287</td>\n",
       "      <td>1.226250</td>\n",
       "      <td>0.685087</td>\n",
       "      <td>0.901845</td>\n",
       "      <td>0.778808</td>\n",
       "      <td>0.216758</td>\n",
       "      <td>0.093721</td>\n",
       "      <td>-0.123037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>23.641180</td>\n",
       "      <td>23.387447</td>\n",
       "      <td>22.975301</td>\n",
       "      <td>22.235199</td>\n",
       "      <td>21.809658</td>\n",
       "      <td>21.559483</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253733</td>\n",
       "      <td>0.665879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412146</td>\n",
       "      <td>1.152248</td>\n",
       "      <td>1.577789</td>\n",
       "      <td>1.827964</td>\n",
       "      <td>0.740102</td>\n",
       "      <td>1.165643</td>\n",
       "      <td>1.415818</td>\n",
       "      <td>0.425541</td>\n",
       "      <td>0.675717</td>\n",
       "      <td>0.250175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      u_apercor  g_apercor  r_apercor  i_apercor  z_apercor  y_apercor  \\\n",
       "1251  24.244393  23.979583  23.522136  22.911041  22.525773  22.329098   \n",
       "1253  24.489104  23.916151  22.923651  21.873752  21.306495  21.251440   \n",
       "1261  24.873959  22.973893  21.465850  20.788420  20.462283  20.413696   \n",
       "1271  24.529042  24.338631  23.891189  23.206102  22.989344  23.112382   \n",
       "1272  23.641180  23.387447  22.975301  22.235199  21.809658  21.559483   \n",
       "\n",
       "      zquality  cfhtls_source  u_apercor-g_apercor  u_apercor-r_apercor  ...  \\\n",
       "1251         4            0.0             0.264811             0.722258  ...   \n",
       "1253         4            0.0             0.572953             1.565453  ...   \n",
       "1261         3            0.0             1.900066             3.408108  ...   \n",
       "1271         4            0.0             0.190411             0.637853  ...   \n",
       "1272         4            0.0             0.253733             0.665879  ...   \n",
       "\n",
       "      g_apercor-r_apercor  g_apercor-i_apercor  g_apercor-z_apercor  \\\n",
       "1251             0.457447             1.068542             1.453810   \n",
       "1253             0.992500             2.042399             2.609656   \n",
       "1261             1.508042             2.185473             2.511610   \n",
       "1271             0.447442             1.132529             1.349287   \n",
       "1272             0.412146             1.152248             1.577789   \n",
       "\n",
       "      g_apercor-y_apercor  r_apercor-i_apercor  r_apercor-z_apercor  \\\n",
       "1251             1.650485             0.611094             0.996362   \n",
       "1253             2.664711             1.049899             1.617157   \n",
       "1261             2.560196             0.677430             1.003568   \n",
       "1271             1.226250             0.685087             0.901845   \n",
       "1272             1.827964             0.740102             1.165643   \n",
       "\n",
       "      r_apercor-y_apercor  i_apercor-z_apercor  i_apercor-y_apercor  \\\n",
       "1251             1.193037             0.385268             0.581943   \n",
       "1253             1.672212             0.567258             0.622312   \n",
       "1261             1.052154             0.326138             0.374724   \n",
       "1271             0.778808             0.216758             0.093721   \n",
       "1272             1.415818             0.425541             0.675717   \n",
       "\n",
       "      z_apercor-y_apercor  \n",
       "1251             0.196675  \n",
       "1253             0.055055  \n",
       "1261             0.048586  \n",
       "1271            -0.123037  \n",
       "1272             0.250175  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target[features_ext.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_paper = features_ext[['u_apercor-g_apercor','g_apercor-r_apercor', \\\n",
    "            'r_apercor-i_apercor','i_apercor-z_apercor','z_apercor-y_apercor','i_apercor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(features_paper, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,5e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.5         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 22:41:44.484007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-08 22:41:44.484237: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-08 22:41:44.484299: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-08 22:41:44.484347: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-08 22:41:44.484394: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-08 22:41:44.484441: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-08 22:41:44.484488: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-08 22:41:44.484534: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-08 22:41:44.484580: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-08 22:41:44.484588: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-08 22:41:44.485686: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 685us/step - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Loss: 0.016241615638136864 , Elapsed time: 73.1549379825592\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.005\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Loss: 0.022390153259038925 , Elapsed time: 208.05959033966064\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Loss: 0.018503624945878983 , Elapsed time: 421.720828294754\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 704us/step - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Loss: 0.017455309629440308 , Elapsed time: 244.79276609420776\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 1 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 657us/step - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Loss: 0.01927628554403782 , Elapsed time: 200.99622416496277\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax      \n",
      "0  \t5     \t0.0162416\t0.0187734\t0.0223902\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Loss: 0.0183662511408329 , Elapsed time: 73.76896500587463\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.005\n",
      "40/40 [==============================] - 0s 846us/step - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Loss: 0.021347442641854286 , Elapsed time: 248.77696585655212\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Loss: 0.022542733699083328 , Elapsed time: 69.13948774337769\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 803us/step - loss: 0.0149 - mean_squared_error: 0.0149\n",
      "Loss: 0.014866065233945847 , Elapsed time: 272.196364402771\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.0148661\t0.0186728\t0.0225427\n",
      "2  \t0     \t0.0148661\t0.0156914\t0.0162416\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 845us/step - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Loss: 0.015654681250452995 , Elapsed time: 273.33480882644653\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t1     \t0.0148661\t0.015574 \t0.0162416\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 791us/step - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Loss: 0.016429908573627472 , Elapsed time: 281.5157582759857\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Loss: 0.017871148884296417 , Elapsed time: 265.2108681201935\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.0146 - mean_squared_error: 0.0146\n",
      "Loss: 0.014557688497006893 , Elapsed time: 279.3224472999573\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Loss: 0.02000277489423752 , Elapsed time: 78.5436270236969\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t4     \t0.0145577\t0.0167455\t0.0200028\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.005\n",
      "40/40 [==============================] - 0s 731us/step - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Loss: 0.02175462432205677 , Elapsed time: 78.20971870422363\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Loss: 0.017061948776245117 , Elapsed time: 273.63799810409546\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Loss: 0.016932714730501175 , Elapsed time: 268.3422176837921\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 702us/step - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Loss: 0.015831613913178444 , Elapsed time: 269.97740364074707\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.0148661\t0.0172894\t0.0217546\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Loss: 0.018281180411577225 , Elapsed time: 277.331485748291\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 859us/step - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Loss: 0.01744730956852436 , Elapsed time: 336.98114490509033\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Loss: 0.019688978791236877 , Elapsed time: 272.3614821434021\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t3     \t0.0148661\t0.017223 \t0.019689 \n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Loss: 0.01678842306137085 , Elapsed time: 271.71057963371277\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 854us/step - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Loss: 0.01752585358917713 , Elapsed time: 96.92655730247498\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 768us/step - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Loss: 0.016323493793606758 , Elapsed time: 278.1918041706085\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 854us/step - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Loss: 0.017174499109387398 , Elapsed time: 553.1448521614075\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.0148661\t0.0165357\t0.0175259\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Loss: 0.01650269702076912 , Elapsed time: 279.85624504089355\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Loss: 0.016599111258983612 , Elapsed time: 282.54309487342834\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Loss: 0.01806890219449997 , Elapsed time: 77.78395915031433\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 826us/step - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Loss: 0.01662837155163288 , Elapsed time: 284.351434469223\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t4     \t0.0148661\t0.016533 \t0.0180689\n",
      "9  \t0     \t0.0148661\t0.0162139\t0.0165991\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Loss: 0.017911823466420174 , Elapsed time: 78.26868009567261\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Loss: 0.016414470970630646 , Elapsed time: 292.4897267818451\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Loss: 0.01609259657561779 , Elapsed time: 287.25846672058105\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Loss: 0.01562041137367487 , Elapsed time: 279.9836220741272\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t4     \t0.0148661\t0.0161811\t0.0179118\n",
      "-- Best Individual =  [1, 1, 0, 1, 0, 0, 1]\n",
      "-- Best Fitness =  0.014866065233945847\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 130.50088500976562 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 2 , Number of neurons: 100\n",
      "Batch size 4 , Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.014558</td>\n",
       "      <td>0.014558</td>\n",
       "      <td>279.322447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.014866</td>\n",
       "      <td>0.014866</td>\n",
       "      <td>272.196364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.015620</td>\n",
       "      <td>0.015620</td>\n",
       "      <td>279.983622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.015655</td>\n",
       "      <td>0.015655</td>\n",
       "      <td>273.334809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.015832</td>\n",
       "      <td>0.015832</td>\n",
       "      <td>269.977404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016093</td>\n",
       "      <td>0.016093</td>\n",
       "      <td>287.258467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.016242</td>\n",
       "      <td>0.016242</td>\n",
       "      <td>73.154938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016323</td>\n",
       "      <td>0.016323</td>\n",
       "      <td>278.191804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>292.489727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016430</td>\n",
       "      <td>0.016430</td>\n",
       "      <td>281.515758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016503</td>\n",
       "      <td>0.016503</td>\n",
       "      <td>279.856245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>282.543095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016628</td>\n",
       "      <td>0.016628</td>\n",
       "      <td>284.351434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016788</td>\n",
       "      <td>0.016788</td>\n",
       "      <td>271.710580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016933</td>\n",
       "      <td>0.016933</td>\n",
       "      <td>268.342218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017062</td>\n",
       "      <td>0.017062</td>\n",
       "      <td>273.637998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017174</td>\n",
       "      <td>0.017174</td>\n",
       "      <td>553.144852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017447</td>\n",
       "      <td>0.017447</td>\n",
       "      <td>336.981145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017455</td>\n",
       "      <td>0.017455</td>\n",
       "      <td>244.792766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.017526</td>\n",
       "      <td>0.017526</td>\n",
       "      <td>96.926557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017871</td>\n",
       "      <td>0.017871</td>\n",
       "      <td>265.210868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.017912</td>\n",
       "      <td>0.017912</td>\n",
       "      <td>78.268680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.018069</td>\n",
       "      <td>0.018069</td>\n",
       "      <td>77.783959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.018281</td>\n",
       "      <td>0.018281</td>\n",
       "      <td>277.331486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.018366</td>\n",
       "      <td>0.018366</td>\n",
       "      <td>73.768965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.018504</td>\n",
       "      <td>0.018504</td>\n",
       "      <td>421.720828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>200.996224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.019689</td>\n",
       "      <td>0.019689</td>\n",
       "      <td>272.361482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.020003</td>\n",
       "      <td>0.020003</td>\n",
       "      <td>78.543627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.021347</td>\n",
       "      <td>0.021347</td>\n",
       "      <td>248.776966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.021755</td>\n",
       "      <td>0.021755</td>\n",
       "      <td>78.209719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.022390</td>\n",
       "      <td>0.022390</td>\n",
       "      <td>208.059590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.022543</td>\n",
       "      <td>0.022543</td>\n",
       "      <td>69.139488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             4        100         0.0001           4  0.014558  0.014558   \n",
       "1             4        100         0.0001           4  0.014866  0.014866   \n",
       "2             4        100         0.0001           4  0.015620  0.015620   \n",
       "3             4        100         0.0001           4  0.015655  0.015655   \n",
       "4             4        100         0.0001           4  0.015832  0.015832   \n",
       "5             4        100         0.0001           4  0.016093  0.016093   \n",
       "6             4        100         0.0001          16  0.016242  0.016242   \n",
       "7             4        100         0.0001           4  0.016323  0.016323   \n",
       "8             4        100         0.0001           4  0.016414  0.016414   \n",
       "9             4        100         0.0001           4  0.016430  0.016430   \n",
       "10            4        100         0.0001           4  0.016503  0.016503   \n",
       "11            4        100         0.0001           4  0.016599  0.016599   \n",
       "12            4        100         0.0001           4  0.016628  0.016628   \n",
       "13            4        100         0.0001           4  0.016788  0.016788   \n",
       "14            4        100         0.0001           4  0.016933  0.016933   \n",
       "15            4        100         0.0001           4  0.017062  0.017062   \n",
       "16            4        100         0.0001           2  0.017174  0.017174   \n",
       "17            4        200         0.0001           4  0.017447  0.017447   \n",
       "18            2        100         0.0001           4  0.017455  0.017455   \n",
       "19            4        200         0.0001          16  0.017526  0.017526   \n",
       "20            4         50         0.0001           4  0.017871  0.017871   \n",
       "21            4        100         0.0001          16  0.017912  0.017912   \n",
       "22            4        100         0.0001          16  0.018069  0.018069   \n",
       "23            4        100         0.0001           4  0.018281  0.018281   \n",
       "24            3        100         0.0001          16  0.018366  0.018366   \n",
       "25            1        150         0.0001           2  0.018504  0.018504   \n",
       "26            1         50         0.0001           4  0.019276  0.019276   \n",
       "27            4        100         0.0001           4  0.019689  0.019689   \n",
       "28            4        100         0.0050          16  0.020003  0.020003   \n",
       "29            2        100         0.0050           4  0.021347  0.021347   \n",
       "30            4        100         0.0050          16  0.021755  0.021755   \n",
       "31            1        100         0.0050           4  0.022390  0.022390   \n",
       "32            2        100         0.0001          16  0.022543  0.022543   \n",
       "\n",
       "    Elapsed time  \n",
       "0     279.322447  \n",
       "1     272.196364  \n",
       "2     279.983622  \n",
       "3     273.334809  \n",
       "4     269.977404  \n",
       "5     287.258467  \n",
       "6      73.154938  \n",
       "7     278.191804  \n",
       "8     292.489727  \n",
       "9     281.515758  \n",
       "10    279.856245  \n",
       "11    282.543095  \n",
       "12    284.351434  \n",
       "13    271.710580  \n",
       "14    268.342218  \n",
       "15    273.637998  \n",
       "16    553.144852  \n",
       "17    336.981145  \n",
       "18    244.792766  \n",
       "19     96.926557  \n",
       "20    265.210868  \n",
       "21     78.268680  \n",
       "22     77.783959  \n",
       "23    277.331486  \n",
       "24     73.768965  \n",
       "25    421.720828  \n",
       "26    200.996224  \n",
       "27    272.361482  \n",
       "28     78.543627  \n",
       "29    248.776966  \n",
       "30     78.209719  \n",
       "31    208.059590  \n",
       "32     69.139488  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_photoz.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 130.498 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
