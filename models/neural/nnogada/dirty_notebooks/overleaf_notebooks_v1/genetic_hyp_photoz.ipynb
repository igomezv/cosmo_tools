{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 17:03:11.697457: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-09 17:03:11.822331: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-09 17:03:11.822352: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-09 17:03:12.629255: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-09 17:03:12.629337: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-09 17:03:12.629347: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import astropy\n",
    "from astropy.io import fits\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open('https://github.com/igomezv/cosmo_tools/blob/main/COB_22/Viviana_Acquaviva/DEEP2_uniq_Terapix_Subaru_v1.fits?raw=true') as data:\n",
    "    df = pd.DataFrame(np.array(data[1].data).byteswap().newbyteorder()) #see https://numpy.org/devdocs/user/basics.byteswapping.html#changing-byte-ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objno_deep2</th>\n",
       "      <th>ra_deep2</th>\n",
       "      <th>dec_deep2</th>\n",
       "      <th>magb</th>\n",
       "      <th>magr</th>\n",
       "      <th>magi</th>\n",
       "      <th>pgal</th>\n",
       "      <th>sfd_ebv</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "      <th>...</th>\n",
       "      <th>ra_subaru</th>\n",
       "      <th>dec_subaru</th>\n",
       "      <th>y</th>\n",
       "      <th>yerr</th>\n",
       "      <th>y_apercor</th>\n",
       "      <th>yerr_aper</th>\n",
       "      <th>yerr_apercor</th>\n",
       "      <th>y(sexflag)</th>\n",
       "      <th>y_radius_arcsec</th>\n",
       "      <th>subaru_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11001673</td>\n",
       "      <td>213.868704</td>\n",
       "      <td>51.956445</td>\n",
       "      <td>23.487745</td>\n",
       "      <td>23.143082</td>\n",
       "      <td>22.582092</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010943</td>\n",
       "      <td>b''</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>213.868626</td>\n",
       "      <td>51.956443</td>\n",
       "      <td>21.869627</td>\n",
       "      <td>0.060918</td>\n",
       "      <td>21.926356</td>\n",
       "      <td>0.041955</td>\n",
       "      <td>0.141778</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.656514</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11001699</td>\n",
       "      <td>213.810471</td>\n",
       "      <td>51.942316</td>\n",
       "      <td>22.067692</td>\n",
       "      <td>20.034674</td>\n",
       "      <td>19.545080</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.011014</td>\n",
       "      <td>b'GALAXY'</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>213.810455</td>\n",
       "      <td>51.942321</td>\n",
       "      <td>18.757229</td>\n",
       "      <td>0.005813</td>\n",
       "      <td>18.811085</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.050987</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.744269</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11001770</td>\n",
       "      <td>213.848431</td>\n",
       "      <td>51.948876</td>\n",
       "      <td>24.144438</td>\n",
       "      <td>24.103180</td>\n",
       "      <td>24.020006</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>b'GALAXY'</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11001800</td>\n",
       "      <td>213.831758</td>\n",
       "      <td>51.952548</td>\n",
       "      <td>25.336836</td>\n",
       "      <td>23.508480</td>\n",
       "      <td>23.081087</td>\n",
       "      <td>0.509809</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>b'GALAXY'</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>213.831766</td>\n",
       "      <td>51.952544</td>\n",
       "      <td>22.404269</td>\n",
       "      <td>0.088970</td>\n",
       "      <td>22.535600</td>\n",
       "      <td>0.053497</td>\n",
       "      <td>0.094733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.455820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11001860</td>\n",
       "      <td>213.832550</td>\n",
       "      <td>51.954174</td>\n",
       "      <td>24.382738</td>\n",
       "      <td>23.401484</td>\n",
       "      <td>22.572845</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>b'GALAXY'</td>\n",
       "      <td>b''</td>\n",
       "      <td>...</td>\n",
       "      <td>213.832574</td>\n",
       "      <td>51.954175</td>\n",
       "      <td>22.242717</td>\n",
       "      <td>0.070760</td>\n",
       "      <td>22.100980</td>\n",
       "      <td>0.033256</td>\n",
       "      <td>0.073067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.442022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   objno_deep2    ra_deep2  dec_deep2       magb       magr       magi  \\\n",
       "0     11001673  213.868704  51.956445  23.487745  23.143082  22.582092   \n",
       "1     11001699  213.810471  51.942316  22.067692  20.034674  19.545080   \n",
       "2     11001770  213.848431  51.948876  24.144438  24.103180  24.020006   \n",
       "3     11001800  213.831758  51.952548  25.336836  23.508480  23.081087   \n",
       "4     11001860  213.832550  51.954174  24.382738  23.401484  22.572845   \n",
       "\n",
       "       pgal   sfd_ebv      class subclass  ...   ra_subaru dec_subaru  \\\n",
       "0  1.000000  0.010943        b''      b''  ...  213.868626  51.956443   \n",
       "1  3.000000  0.011014  b'GALAXY'      b''  ...  213.810455  51.942321   \n",
       "2  3.000000  0.010856  b'GALAXY'      b''  ...  -99.000000 -99.000000   \n",
       "3  0.509809  0.010823  b'GALAXY'      b''  ...  213.831766  51.952544   \n",
       "4  3.000000  0.010827  b'GALAXY'      b''  ...  213.832574  51.954175   \n",
       "\n",
       "           y       yerr  y_apercor  yerr_aper  yerr_apercor  y(sexflag)  \\\n",
       "0  21.869627   0.060918  21.926356   0.041955      0.141778         3.0   \n",
       "1  18.757229   0.005813  18.811085   0.004386      0.050987         3.0   \n",
       "2 -99.000000 -99.000000 -99.000000 -99.000000    -99.000000       -99.0   \n",
       "3  22.404269   0.088970  22.535600   0.053497      0.094733         0.0   \n",
       "4  22.242717   0.070760  22.100980   0.033256      0.073067         0.0   \n",
       "\n",
       "   y_radius_arcsec  subaru_source  \n",
       "0         0.656514              1  \n",
       "1         0.744269              1  \n",
       "2       -99.000000            -99  \n",
       "3         0.455820              1  \n",
       "4         0.442022              1  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_apercor</th>\n",
       "      <th>g_apercor</th>\n",
       "      <th>r_apercor</th>\n",
       "      <th>i_apercor</th>\n",
       "      <th>z_apercor</th>\n",
       "      <th>y_apercor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.543491</td>\n",
       "      <td>23.430495</td>\n",
       "      <td>23.100311</td>\n",
       "      <td>22.768970</td>\n",
       "      <td>22.223810</td>\n",
       "      <td>21.926356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.848978</td>\n",
       "      <td>28.989668</td>\n",
       "      <td>19.027422</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>18.811085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.324670</td>\n",
       "      <td>24.273606</td>\n",
       "      <td>24.150319</td>\n",
       "      <td>23.446252</td>\n",
       "      <td>23.574236</td>\n",
       "      <td>-99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>24.804309</td>\n",
       "      <td>23.636544</td>\n",
       "      <td>23.009222</td>\n",
       "      <td>22.689591</td>\n",
       "      <td>22.535600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.362068</td>\n",
       "      <td>24.136913</td>\n",
       "      <td>23.490342</td>\n",
       "      <td>22.777181</td>\n",
       "      <td>22.319676</td>\n",
       "      <td>22.100980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   u_apercor  g_apercor  r_apercor  i_apercor  z_apercor  y_apercor\n",
       "0  23.543491  23.430495  23.100311  22.768970  22.223810  21.926356\n",
       "1  30.848978  28.989668  19.027422  99.000000  99.000000  18.811085\n",
       "2  24.324670  24.273606  24.150319  23.446252  23.574236 -99.000000\n",
       "3  99.000000  24.804309  23.636544  23.009222  22.689591  22.535600\n",
       "4  24.362068  24.136913  23.490342  22.777181  22.319676  22.100980"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df[['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor']]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.115261\n",
       "1    0.290608\n",
       "2    0.605744\n",
       "3    1.306796\n",
       "4    0.957669\n",
       "Name: zhelio, dtype: float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df['zhelio']\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = df[['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor','zquality','cfhtls_source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16857, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mags = mags[mags['zquality'] >= 3]\n",
    "\n",
    "mags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = mags[mags > -10].dropna()\n",
    "mags = mags[mags < 90].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = mags[mags['cfhtls_source'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor','y_apercor']\n",
    "features_ext = mags.copy()\n",
    "for i, name1 in enumerate(params):\n",
    "    for j, name2 in enumerate(params):\n",
    "        if i >=j: continue #build only one pair, avoid zero colors\n",
    "        features_ext[name1 + '-' + name2] = features[name1] - features[name2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_apercor</th>\n",
       "      <th>g_apercor</th>\n",
       "      <th>r_apercor</th>\n",
       "      <th>i_apercor</th>\n",
       "      <th>z_apercor</th>\n",
       "      <th>y_apercor</th>\n",
       "      <th>zquality</th>\n",
       "      <th>cfhtls_source</th>\n",
       "      <th>u_apercor-g_apercor</th>\n",
       "      <th>u_apercor-r_apercor</th>\n",
       "      <th>...</th>\n",
       "      <th>g_apercor-r_apercor</th>\n",
       "      <th>g_apercor-i_apercor</th>\n",
       "      <th>g_apercor-z_apercor</th>\n",
       "      <th>g_apercor-y_apercor</th>\n",
       "      <th>r_apercor-i_apercor</th>\n",
       "      <th>r_apercor-z_apercor</th>\n",
       "      <th>r_apercor-y_apercor</th>\n",
       "      <th>i_apercor-z_apercor</th>\n",
       "      <th>i_apercor-y_apercor</th>\n",
       "      <th>z_apercor-y_apercor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>24.244393</td>\n",
       "      <td>23.979583</td>\n",
       "      <td>23.522136</td>\n",
       "      <td>22.911041</td>\n",
       "      <td>22.525773</td>\n",
       "      <td>22.329098</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264811</td>\n",
       "      <td>0.722258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457447</td>\n",
       "      <td>1.068542</td>\n",
       "      <td>1.453810</td>\n",
       "      <td>1.650485</td>\n",
       "      <td>0.611094</td>\n",
       "      <td>0.996362</td>\n",
       "      <td>1.193037</td>\n",
       "      <td>0.385268</td>\n",
       "      <td>0.581943</td>\n",
       "      <td>0.196675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>24.489104</td>\n",
       "      <td>23.916151</td>\n",
       "      <td>22.923651</td>\n",
       "      <td>21.873752</td>\n",
       "      <td>21.306495</td>\n",
       "      <td>21.251440</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.572953</td>\n",
       "      <td>1.565453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>2.042399</td>\n",
       "      <td>2.609656</td>\n",
       "      <td>2.664711</td>\n",
       "      <td>1.049899</td>\n",
       "      <td>1.617157</td>\n",
       "      <td>1.672212</td>\n",
       "      <td>0.567258</td>\n",
       "      <td>0.622312</td>\n",
       "      <td>0.055055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>24.873959</td>\n",
       "      <td>22.973893</td>\n",
       "      <td>21.465850</td>\n",
       "      <td>20.788420</td>\n",
       "      <td>20.462283</td>\n",
       "      <td>20.413696</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.900066</td>\n",
       "      <td>3.408108</td>\n",
       "      <td>...</td>\n",
       "      <td>1.508042</td>\n",
       "      <td>2.185473</td>\n",
       "      <td>2.511610</td>\n",
       "      <td>2.560196</td>\n",
       "      <td>0.677430</td>\n",
       "      <td>1.003568</td>\n",
       "      <td>1.052154</td>\n",
       "      <td>0.326138</td>\n",
       "      <td>0.374724</td>\n",
       "      <td>0.048586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>24.529042</td>\n",
       "      <td>24.338631</td>\n",
       "      <td>23.891189</td>\n",
       "      <td>23.206102</td>\n",
       "      <td>22.989344</td>\n",
       "      <td>23.112382</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190411</td>\n",
       "      <td>0.637853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447442</td>\n",
       "      <td>1.132529</td>\n",
       "      <td>1.349287</td>\n",
       "      <td>1.226250</td>\n",
       "      <td>0.685087</td>\n",
       "      <td>0.901845</td>\n",
       "      <td>0.778808</td>\n",
       "      <td>0.216758</td>\n",
       "      <td>0.093721</td>\n",
       "      <td>-0.123037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>23.641180</td>\n",
       "      <td>23.387447</td>\n",
       "      <td>22.975301</td>\n",
       "      <td>22.235199</td>\n",
       "      <td>21.809658</td>\n",
       "      <td>21.559483</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253733</td>\n",
       "      <td>0.665879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412146</td>\n",
       "      <td>1.152248</td>\n",
       "      <td>1.577789</td>\n",
       "      <td>1.827964</td>\n",
       "      <td>0.740102</td>\n",
       "      <td>1.165643</td>\n",
       "      <td>1.415818</td>\n",
       "      <td>0.425541</td>\n",
       "      <td>0.675717</td>\n",
       "      <td>0.250175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      u_apercor  g_apercor  r_apercor  i_apercor  z_apercor  y_apercor  \\\n",
       "1251  24.244393  23.979583  23.522136  22.911041  22.525773  22.329098   \n",
       "1253  24.489104  23.916151  22.923651  21.873752  21.306495  21.251440   \n",
       "1261  24.873959  22.973893  21.465850  20.788420  20.462283  20.413696   \n",
       "1271  24.529042  24.338631  23.891189  23.206102  22.989344  23.112382   \n",
       "1272  23.641180  23.387447  22.975301  22.235199  21.809658  21.559483   \n",
       "\n",
       "      zquality  cfhtls_source  u_apercor-g_apercor  u_apercor-r_apercor  ...  \\\n",
       "1251         4            0.0             0.264811             0.722258  ...   \n",
       "1253         4            0.0             0.572953             1.565453  ...   \n",
       "1261         3            0.0             1.900066             3.408108  ...   \n",
       "1271         4            0.0             0.190411             0.637853  ...   \n",
       "1272         4            0.0             0.253733             0.665879  ...   \n",
       "\n",
       "      g_apercor-r_apercor  g_apercor-i_apercor  g_apercor-z_apercor  \\\n",
       "1251             0.457447             1.068542             1.453810   \n",
       "1253             0.992500             2.042399             2.609656   \n",
       "1261             1.508042             2.185473             2.511610   \n",
       "1271             0.447442             1.132529             1.349287   \n",
       "1272             0.412146             1.152248             1.577789   \n",
       "\n",
       "      g_apercor-y_apercor  r_apercor-i_apercor  r_apercor-z_apercor  \\\n",
       "1251             1.650485             0.611094             0.996362   \n",
       "1253             2.664711             1.049899             1.617157   \n",
       "1261             2.560196             0.677430             1.003568   \n",
       "1271             1.226250             0.685087             0.901845   \n",
       "1272             1.827964             0.740102             1.165643   \n",
       "\n",
       "      r_apercor-y_apercor  i_apercor-z_apercor  i_apercor-y_apercor  \\\n",
       "1251             1.193037             0.385268             0.581943   \n",
       "1253             1.672212             0.567258             0.622312   \n",
       "1261             1.052154             0.326138             0.374724   \n",
       "1271             0.778808             0.216758             0.093721   \n",
       "1272             1.415818             0.425541             0.675717   \n",
       "\n",
       "      z_apercor-y_apercor  \n",
       "1251             0.196675  \n",
       "1253             0.055055  \n",
       "1261             0.048586  \n",
       "1271            -0.123037  \n",
       "1272             0.250175  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target[features_ext.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_paper = features_ext[['u_apercor-g_apercor','g_apercor-r_apercor', \\\n",
    "            'r_apercor-i_apercor','i_apercor-z_apercor','z_apercor-y_apercor','i_apercor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(features_paper, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([100,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,1e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([8, 16, 32 ,64])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:1])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[1:2])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[3:4])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[4:6])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.2         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isidro/.local/lib/python3.10/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/isidro/.local/lib/python3.10/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 964us/step - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Loss: 0.018080472946166992 , Elapsed time: 98.77496981620789\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.0440 - mean_squared_error: 0.0440\n",
      "Loss: 0.044034071266651154 , Elapsed time: 134.48073267936707\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 782us/step - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Loss: 0.03345372900366783 , Elapsed time: 25.408854246139526\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 960us/step - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Loss: 0.030502162873744965 , Elapsed time: 30.33362603187561\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 888us/step - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Loss: 0.024296419695019722 , Elapsed time: 30.537558794021606\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax      \n",
      "0  \t5     \t0.0180805\t0.0300734\t0.0440341\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 924us/step - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Loss: 0.01973072998225689 , Elapsed time: 84.05894351005554\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 873us/step - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Loss: 0.030623041093349457 , Elapsed time: 30.79111647605896\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 910us/step - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Loss: 0.035722777247428894 , Elapsed time: 30.65359926223755\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 889us/step - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Loss: 0.03686942532658577 , Elapsed time: 30.698820114135742\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.0180805\t0.0282053\t0.0368694\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 948us/step - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Loss: 0.035067833960056305 , Elapsed time: 30.833914279937744\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 947us/step - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Loss: 0.0285163763910532 , Elapsed time: 31.295408010482788\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 966us/step - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Loss: 0.017512476071715355 , Elapsed time: 96.77915287017822\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 962us/step - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Loss: 0.01796715147793293 , Elapsed time: 87.07529973983765\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t4     \t0.0175125\t0.0234289\t0.0350678\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 996us/step - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Loss: 0.02417268045246601 , Elapsed time: 99.05002570152283\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Loss: 0.01590644009411335 , Elapsed time: 97.76046347618103\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "40/40 [==============================] - 0s 860us/step - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Loss: 0.03578841686248779 , Elapsed time: 73.00610542297363\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t3     \t0.0159064\t0.0222694\t0.0357884\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 965us/step - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Loss: 0.021265676245093346 , Elapsed time: 97.82849788665771\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 977us/step - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Loss: 0.02420262061059475 , Elapsed time: 79.63459658622742\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 831us/step - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Loss: 0.024860991165041924 , Elapsed time: 93.82976388931274\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t3     \t0.0159064\t0.0207496\t0.024861 \n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 953us/step - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Loss: 0.01634877920150757 , Elapsed time: 93.43695020675659\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 804us/step - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Loss: 0.03506839647889137 , Elapsed time: 84.91842293739319\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t2     \t0.0159064\t0.0201485\t0.0350684\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 908us/step - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Loss: 0.030827496200799942 , Elapsed time: 93.97080969810486\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 902us/step - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Loss: 0.029163312166929245 , Elapsed time: 94.6440041065216\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 887us/step - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Loss: 0.01879580318927765 , Elapsed time: 94.52857708930969\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 985us/step - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Loss: 0.025893287733197212 , Elapsed time: 104.04872417449951\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t4     \t0.0159064\t0.0241173\t0.0308275\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Loss: 0.02978898212313652 , Elapsed time: 106.0671865940094\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Loss: 0.020626096054911613 , Elapsed time: 102.04582095146179\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 953us/step - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Loss: 0.02532210201025009 , Elapsed time: 101.56499481201172\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Loss: 0.027035929262638092 , Elapsed time: 103.34437727928162\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.0159064\t0.0237359\t0.029789 \n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 867us/step - loss: 0.0464 - mean_squared_error: 0.0464\n",
      "Loss: 0.04635708034038544 , Elapsed time: 102.43101978302002\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.001\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Loss: 0.024086786434054375 , Elapsed time: 102.09717774391174\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t2     \t0.0159064\t0.0255158\t0.0463571\n",
      "-- Best Individual =  [1, 1, 1, 1, 0, 1]\n",
      "-- Best Fitness =  0.01590644009411335\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 41.10240646600723 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 8  # number of generations\n",
    "gene_length = 6      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 4 , Number of neurons: 200\n",
      "Batch size 16 , Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.015906</td>\n",
       "      <td>0.015906</td>\n",
       "      <td>97.760463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.016349</td>\n",
       "      <td>0.016349</td>\n",
       "      <td>93.436950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.017512</td>\n",
       "      <td>0.017512</td>\n",
       "      <td>96.779153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.017967</td>\n",
       "      <td>0.017967</td>\n",
       "      <td>87.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.018080</td>\n",
       "      <td>0.018080</td>\n",
       "      <td>98.774970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.018796</td>\n",
       "      <td>0.018796</td>\n",
       "      <td>94.528577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.019731</td>\n",
       "      <td>0.019731</td>\n",
       "      <td>84.058944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.020626</td>\n",
       "      <td>0.020626</td>\n",
       "      <td>102.045821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>97.828498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.024087</td>\n",
       "      <td>0.024087</td>\n",
       "      <td>102.097178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.024173</td>\n",
       "      <td>0.024173</td>\n",
       "      <td>99.050026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.024203</td>\n",
       "      <td>0.024203</td>\n",
       "      <td>79.634597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>0.024296</td>\n",
       "      <td>0.024296</td>\n",
       "      <td>30.537559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.024861</td>\n",
       "      <td>0.024861</td>\n",
       "      <td>93.829764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.025322</td>\n",
       "      <td>0.025322</td>\n",
       "      <td>101.564995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.025893</td>\n",
       "      <td>0.025893</td>\n",
       "      <td>104.048724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>103.344377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.028516</td>\n",
       "      <td>0.028516</td>\n",
       "      <td>31.295408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.029163</td>\n",
       "      <td>0.029163</td>\n",
       "      <td>94.644004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.029789</td>\n",
       "      <td>0.029789</td>\n",
       "      <td>106.067187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.030502</td>\n",
       "      <td>0.030502</td>\n",
       "      <td>30.333626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>0.030623</td>\n",
       "      <td>0.030623</td>\n",
       "      <td>30.791116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.030827</td>\n",
       "      <td>0.030827</td>\n",
       "      <td>93.970810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.033454</td>\n",
       "      <td>0.033454</td>\n",
       "      <td>25.408854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.035068</td>\n",
       "      <td>0.035068</td>\n",
       "      <td>30.833914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.035068</td>\n",
       "      <td>0.035068</td>\n",
       "      <td>84.918423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.035723</td>\n",
       "      <td>0.035723</td>\n",
       "      <td>30.653599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.035788</td>\n",
       "      <td>0.035788</td>\n",
       "      <td>73.006105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>0.036869</td>\n",
       "      <td>0.036869</td>\n",
       "      <td>30.698820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.044034</td>\n",
       "      <td>0.044034</td>\n",
       "      <td>134.480733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>0.046357</td>\n",
       "      <td>0.046357</td>\n",
       "      <td>102.431020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             4        200         0.0010          16  0.015906  0.015906   \n",
       "1             4        200         0.0010          16  0.016349  0.016349   \n",
       "2             4        200         0.0010          16  0.017512  0.017512   \n",
       "3             3        200         0.0010          16  0.017967  0.017967   \n",
       "4             4        200         0.0010          16  0.018080  0.018080   \n",
       "5             4        200         0.0010          16  0.018796  0.018796   \n",
       "6             3        200         0.0010          16  0.019731  0.019731   \n",
       "7             4        200         0.0010          16  0.020626  0.020626   \n",
       "8             4        200         0.0010          16  0.021266  0.021266   \n",
       "9             4        200         0.0010          16  0.024087  0.024087   \n",
       "10            4        200         0.0010          16  0.024173  0.024173   \n",
       "11            4        100         0.0010          16  0.024203  0.024203   \n",
       "12            3        200         0.0010          64  0.024296  0.024296   \n",
       "13            4        200         0.0010          16  0.024861  0.024861   \n",
       "14            4        200         0.0010          16  0.025322  0.025322   \n",
       "15            4        200         0.0010          16  0.025893  0.025893   \n",
       "16            4        200         0.0010          16  0.027036  0.027036   \n",
       "17            3        200         0.0001          64  0.028516  0.028516   \n",
       "18            4        200         0.0010          16  0.029163  0.029163   \n",
       "19            4        200         0.0010          16  0.029789  0.029789   \n",
       "20            3        200         0.0001          64  0.030502  0.030502   \n",
       "21            3        200         0.0010          64  0.030623  0.030623   \n",
       "22            4        200         0.0010          16  0.030827  0.030827   \n",
       "23            4        100         0.0001          64  0.033454  0.033454   \n",
       "24            3        200         0.0001          64  0.035068  0.035068   \n",
       "25            3        200         0.0010          16  0.035068  0.035068   \n",
       "26            3        200         0.0001          64  0.035723  0.035723   \n",
       "27            3        100         0.0001          16  0.035788  0.035788   \n",
       "28            3        200         0.0001          64  0.036869  0.036869   \n",
       "29            3        100         0.0001           8  0.044034  0.044034   \n",
       "30            4        200         0.0010          16  0.046357  0.046357   \n",
       "\n",
       "    Elapsed time  \n",
       "0      97.760463  \n",
       "1      93.436950  \n",
       "2      96.779153  \n",
       "3      87.075300  \n",
       "4      98.774970  \n",
       "5      94.528577  \n",
       "6      84.058944  \n",
       "7     102.045821  \n",
       "8      97.828498  \n",
       "9     102.097178  \n",
       "10     99.050026  \n",
       "11     79.634597  \n",
       "12     30.537559  \n",
       "13     93.829764  \n",
       "14    101.564995  \n",
       "15    104.048724  \n",
       "16    103.344377  \n",
       "17     31.295408  \n",
       "18     94.644004  \n",
       "19    106.067187  \n",
       "20     30.333626  \n",
       "21     30.791116  \n",
       "22     93.970810  \n",
       "23     25.408854  \n",
       "24     30.833914  \n",
       "25     84.918423  \n",
       "26     30.653599  \n",
       "27     73.006105  \n",
       "28     30.698820  \n",
       "29    134.480733  \n",
       "30    102.431020  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_photoz.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 41.099 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
