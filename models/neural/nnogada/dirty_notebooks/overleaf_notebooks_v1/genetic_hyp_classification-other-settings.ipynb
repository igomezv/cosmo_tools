{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>delta</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135.689107</td>\n",
       "      <td>32.494632</td>\n",
       "      <td>23.87882</td>\n",
       "      <td>22.27530</td>\n",
       "      <td>20.39501</td>\n",
       "      <td>19.16573</td>\n",
       "      <td>18.79371</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144.826101</td>\n",
       "      <td>31.274185</td>\n",
       "      <td>24.77759</td>\n",
       "      <td>22.83188</td>\n",
       "      <td>22.58444</td>\n",
       "      <td>21.16812</td>\n",
       "      <td>21.61427</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142.188790</td>\n",
       "      <td>35.582444</td>\n",
       "      <td>25.26307</td>\n",
       "      <td>22.66389</td>\n",
       "      <td>20.60976</td>\n",
       "      <td>19.34857</td>\n",
       "      <td>18.94827</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>338.741038</td>\n",
       "      <td>-0.402828</td>\n",
       "      <td>22.13682</td>\n",
       "      <td>23.77656</td>\n",
       "      <td>21.61162</td>\n",
       "      <td>20.50454</td>\n",
       "      <td>19.25010</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345.282593</td>\n",
       "      <td>21.183866</td>\n",
       "      <td>19.43718</td>\n",
       "      <td>17.58028</td>\n",
       "      <td>16.49747</td>\n",
       "      <td>15.97711</td>\n",
       "      <td>15.54461</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        alpha      delta         u         g         r         i         z  \\\n",
       "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
       "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
       "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
       "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
       "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
       "\n",
       "    class  \n",
       "0  GALAXY  \n",
       "1  GALAXY  \n",
       "2  GALAXY  \n",
       "3  GALAXY  \n",
       "4  GALAXY  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/igomezv/nnogada/main/data/star_classification.csv\"\n",
    "data = pd.read_csv(url)\n",
    "cols = ['alpha','delta','u','g','r','i','z','class']\n",
    "data = data[cols]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        alpha      delta         u         g         r         i         z  \\\n",
      "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
      "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
      "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
      "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
      "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
      "\n",
      "   class  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n"
     ]
    }
   ],
   "source": [
    "data[\"class\"]=[0 if i == \"GALAXY\" else 1 if i == \"STAR\" else 2 for i in data[\"class\"]]\n",
    "print(data.head())\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function layers\n",
    "\n",
    "# f1 = lambda x: Dense(x, activation='relu')      #ReLU\n",
    "# f2 = lambda x: Dense(x, activation='elu')       #ELU\n",
    "# f3 = lambda x: keras.layers.LeakyReLU(0.3)      #LReLU\n",
    "# f4 = lambda x: Dense(x, kernel_initializer='lecun_normal', activation='selu')   #SELU\n",
    "\n",
    "# f_names = [\"ReLU\", \"ELU\", \"LReLU\", \"SELU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1, 2,3, 4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([150, 200, 100, 50]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-5,1e-4,1e-3, 0.01])   # Learning rates (8)\n",
    "# SC_BATCH      = np.array([64,128,256,512])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=6,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=1)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 50\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into X and Y and implement hot_ones in Y\n",
    "def prepare_dataset(data):\n",
    "    X, Y = np.empty((0)), np.empty((0))\n",
    "    X = data[:,0:7]\n",
    "    Y = data[:,7]\n",
    "    Y = to_categorical(Y, num_classes=3)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation and test sets\n",
    "X,Y = prepare_dataset(data)\n",
    "\n",
    "# Defines ratios, w.r.t. whole dataset.\n",
    "ratio_train = 0.8\n",
    "ratio_val = 0.1\n",
    "ratio_test = 0.1\n",
    "\n",
    "# Produces test split.\n",
    "x_, X_test, y_, Y_test = split(X, Y, test_size = ratio_test, random_state=0)\n",
    "\n",
    "# Adjusts val ratio, w.r.t. remaining dataset.\n",
    "ratio_remaining = 1 - ratio_test\n",
    "ratio_val_adjusted = ratio_val / ratio_remaining\n",
    "\n",
    "# Produces train and val splits.\n",
    "X_train, X_val, Y_train, Y_val = split(x_, y_, test_size=ratio_val_adjusted, random_state=0)\n",
    "\n",
    "# Normalize and scale the input sets.\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "X_val   = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:6])    # (8)\n",
    "# #     batch_size_bits    = BitArray(ga_individual_solution[10:12])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "#     batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(num_units, input_shape=(int(X_train.shape[1]),)))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(3, activation=tf.nn.softmax))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=128, shuffle=1, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Accuracy:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5      # probability for crossover\n",
    "    P_MUTATION = 0.5         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1  # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 625us/step - loss: 0.3876 - accuracy: 0.8574\n",
      "Accuracy: 0.8574000000953674 , Elapsed time: 39.32435989379883\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 672us/step - loss: 0.5157 - accuracy: 0.8027\n",
      "Accuracy: 0.8026999831199646 , Elapsed time: 47.99380922317505\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 678us/step - loss: 0.3748 - accuracy: 0.8654\n",
      "Accuracy: 0.8654000163078308 , Elapsed time: 46.44985771179199\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.01\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "Epoch 32: early stopping\n",
      "313/313 [==============================] - 0s 639us/step - loss: 0.3388 - accuracy: 0.8743\n",
      "Accuracy: 0.8743000030517578 , Elapsed time: 23.160427570343018\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 722us/step - loss: 0.5014 - accuracy: 0.8089\n",
      "Accuracy: 0.808899998664856 , Elapsed time: 57.14082860946655\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 2 , Number of neurons: 150 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 675us/step - loss: 0.5450 - accuracy: 0.7908\n",
      "Accuracy: 0.7907999753952026 , Elapsed time: 43.471015214920044\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "Epoch 43: early stopping\n",
      "313/313 [==============================] - 0s 648us/step - loss: 0.3235 - accuracy: 0.8805\n",
      "Accuracy: 0.8805000185966492 , Elapsed time: 37.01859450340271\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 1 , Number of neurons: 150 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 620us/step - loss: 0.6091 - accuracy: 0.7748\n",
      "Accuracy: 0.7748000025749207 , Elapsed time: 35.41899228096008\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 751us/step - loss: 0.3567 - accuracy: 0.8704\n",
      "Accuracy: 0.8704000115394592 , Elapsed time: 63.73485350608826\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 680us/step - loss: 0.3357 - accuracy: 0.8780\n",
      "Accuracy: 0.878000020980835 , Elapsed time: 46.45721101760864\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin     \tavg     \tmax     \n",
      "0  \t10    \t0.323536\t0.428843\t0.609138\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 651us/step - loss: 0.5396 - accuracy: 0.7964\n",
      "Accuracy: 0.7964000105857849 , Elapsed time: 46.77993369102478\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 639us/step - loss: 0.5844 - accuracy: 0.7820\n",
      "Accuracy: 0.7820000052452087 , Elapsed time: 39.42897582054138\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 763us/step - loss: 0.3571 - accuracy: 0.8703\n",
      "Accuracy: 0.8702999949455261 , Elapsed time: 63.6808545589447\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.01\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "Epoch 33: early stopping\n",
      "313/313 [==============================] - 0s 635us/step - loss: 0.3440 - accuracy: 0.8724\n",
      "Accuracy: 0.8723999857902527 , Elapsed time: 24.047585487365723\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 718us/step - loss: 0.5037 - accuracy: 0.8066\n",
      "Accuracy: 0.8065999746322632 , Elapsed time: 57.43522763252258\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t5     \t0.323536\t0.414126\t0.584445\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.01\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Epoch 31: early stopping\n",
      "313/313 [==============================] - 0s 668us/step - loss: 0.3376 - accuracy: 0.8737\n",
      "Accuracy: 0.8737000226974487 , Elapsed time: 24.846726179122925\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "313/313 [==============================] - 0s 664us/step - loss: 0.5132 - accuracy: 0.8031\n",
      "Accuracy: 0.8030999898910522 , Elapsed time: 47.95829629898071\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.01\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "Epoch 35: early stopping\n",
      "313/313 [==============================] - 0s 655us/step - loss: 0.3315 - accuracy: 0.8744\n",
      "Accuracy: 0.8744000196456909 , Elapsed time: 27.9225652217865\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Epoch 49: early stopping\n",
      "313/313 [==============================] - 0s 768us/step - loss: 0.3541 - accuracy: 0.8703\n",
      "Accuracy: 0.8702999949455261 , Elapsed time: 62.52861666679382\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.01\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Epoch 14: early stopping\n",
      "313/313 [==============================] - 0s 612us/step - loss: 0.3700 - accuracy: 0.8697\n",
      "Accuracy: 0.869700014591217 , Elapsed time: 10.48461389541626\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t5     \t0.323536\t0.382731\t0.513216\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.01\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "Epoch 25: early stopping\n",
      "313/313 [==============================] - 0s 616us/step - loss: 0.3528 - accuracy: 0.8696\n",
      "Accuracy: 0.8695999979972839 , Elapsed time: 18.233323097229004\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.01\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "Epoch 43: early stopping\n",
      "313/313 [==============================] - 0s 651us/step - loss: 0.3308 - accuracy: 0.8741\n",
      "Accuracy: 0.8741000294685364 , Elapsed time: 30.94716787338257\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 1 , Number of neurons: 50 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 610us/step - loss: 0.4019 - accuracy: 0.8577\n",
      "Accuracy: 0.857699990272522 , Elapsed time: 27.959879875183105\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 42.\n",
      "Epoch 48: early stopping\n",
      "313/313 [==============================] - 0s 633us/step - loss: 0.3774 - accuracy: 0.8646\n",
      "Accuracy: 0.8646000027656555 , Elapsed time: 30.068820238113403\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 718us/step - loss: 0.3605 - accuracy: 0.8665\n",
      "Accuracy: 0.8665000200271606 , Elapsed time: 64.26834154129028\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t5     \t0.323536\t0.359318\t0.401886\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 643us/step - loss: 0.3326 - accuracy: 0.8777\n",
      "Accuracy: 0.8776999711990356 , Elapsed time: 36.0666937828064\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Epoch 15: early stopping\n",
      "313/313 [==============================] - 0s 657us/step - loss: 0.3636 - accuracy: 0.8700\n",
      "Accuracy: 0.8700000047683716 , Elapsed time: 13.302876234054565\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "Epoch 43: early stopping\n",
      "313/313 [==============================] - 0s 687us/step - loss: 0.3248 - accuracy: 0.8799\n",
      "Accuracy: 0.8798999786376953 , Elapsed time: 34.14517140388489\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.01\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 22: early stopping\n",
      "313/313 [==============================] - 0s 671us/step - loss: 0.3526 - accuracy: 0.8735\n",
      "Accuracy: 0.8734999895095825 , Elapsed time: 17.684976816177368\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.01\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "Epoch 26: early stopping\n",
      "313/313 [==============================] - 0s 654us/step - loss: 0.3457 - accuracy: 0.8750\n",
      "Accuracy: 0.875 , Elapsed time: 19.21770215034485\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Epoch 49: early stopping\n",
      "313/313 [==============================] - 0s 621us/step - loss: 0.3306 - accuracy: 0.8792\n",
      "Accuracy: 0.8791999816894531 , Elapsed time: 35.31154227256775\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "Epoch 26: early stopping\n",
      "313/313 [==============================] - 0s 707us/step - loss: 0.3352 - accuracy: 0.8775\n",
      "Accuracy: 0.8774999976158142 , Elapsed time: 25.210360050201416\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.01\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "Epoch 20: early stopping\n",
      "313/313 [==============================] - 0s 639us/step - loss: 0.3558 - accuracy: 0.8665\n",
      "Accuracy: 0.8665000200271606 , Elapsed time: 14.837437629699707\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.01\n",
      "Restoring model weights from the end of the best epoch: 21.\n",
      "Epoch 27: early stopping\n",
      "313/313 [==============================] - 0s 671us/step - loss: 0.3484 - accuracy: 0.8715\n",
      "Accuracy: 0.8715000152587891 , Elapsed time: 19.727730989456177\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t9     \t0.323536\t0.34128 \t0.3636  \n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 639us/step - loss: 0.3377 - accuracy: 0.8770\n",
      "Accuracy: 0.8769999742507935 , Elapsed time: 36.6269474029541\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.01\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Epoch 21: early stopping\n",
      "313/313 [==============================] - 0s 636us/step - loss: 0.3604 - accuracy: 0.8724\n",
      "Accuracy: 0.8723999857902527 , Elapsed time: 14.019005537033081\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "Epoch 41: early stopping\n",
      "313/313 [==============================] - 0s 630us/step - loss: 0.3388 - accuracy: 0.8763\n",
      "Accuracy: 0.8762999773025513 , Elapsed time: 29.658820152282715\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.01\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "Epoch 28: early stopping\n",
      "313/313 [==============================] - 0s 623us/step - loss: 0.3485 - accuracy: 0.8694\n",
      "Accuracy: 0.8694000244140625 , Elapsed time: 20.52684473991394\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 628us/step - loss: 0.3430 - accuracy: 0.8718\n",
      "Accuracy: 0.8718000054359436 , Elapsed time: 36.2205491065979\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "Epoch 41: early stopping\n",
      "313/313 [==============================] - 0s 751us/step - loss: 0.3170 - accuracy: 0.8832\n",
      "Accuracy: 0.8831999897956848 , Elapsed time: 47.25014519691467\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 660us/step - loss: 0.3237 - accuracy: 0.8825\n",
      "Accuracy: 0.8824999928474426 , Elapsed time: 42.74823570251465\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 42 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001\n",
      "313/313 [==============================] - 0s 652us/step - loss: 0.3217 - accuracy: 0.8804\n",
      "Accuracy: 0.8804000020027161 , Elapsed time: 48.12601661682129\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t8     \t0.317017\t0.334958\t0.360395\n",
      "-- Best Individual =  [1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0]\n",
      "-- Best Fitness =  0.31701675057411194\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 25.126321625709533 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 10  # max of individuals per generation\n",
    "max_generations = 5   # number of generations\n",
    "gene_length = 11      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "# best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:2])    # (8)\n",
    "    num_units_bits     = BitArray(bi[2:4])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[4:6])   # (8)\n",
    "#     batch_size_bits    = BitArray(bi[10:12])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "#     best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    \n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.317017</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>47.250145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.323664</td>\n",
       "      <td>0.8825</td>\n",
       "      <td>42.748236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.323536</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>37.018595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.321731</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>48.126017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.324752</td>\n",
       "      <td>0.8799</td>\n",
       "      <td>34.145171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.330631</td>\n",
       "      <td>0.8792</td>\n",
       "      <td>35.311542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.335732</td>\n",
       "      <td>0.8780</td>\n",
       "      <td>46.457211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.332621</td>\n",
       "      <td>0.8777</td>\n",
       "      <td>36.066694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.335166</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>25.210360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.337668</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>36.626947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.338841</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>29.658820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.345714</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>19.217702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.331463</td>\n",
       "      <td>0.8744</td>\n",
       "      <td>27.922565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.338846</td>\n",
       "      <td>0.8743</td>\n",
       "      <td>23.160428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.330785</td>\n",
       "      <td>0.8741</td>\n",
       "      <td>30.947168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.337616</td>\n",
       "      <td>0.8737</td>\n",
       "      <td>24.846726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.352578</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>17.684977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.360395</td>\n",
       "      <td>0.8724</td>\n",
       "      <td>14.019006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.344019</td>\n",
       "      <td>0.8724</td>\n",
       "      <td>24.047585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.343042</td>\n",
       "      <td>0.8718</td>\n",
       "      <td>36.220549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.348408</td>\n",
       "      <td>0.8715</td>\n",
       "      <td>19.727731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.356666</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>63.734854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.354076</td>\n",
       "      <td>0.8703</td>\n",
       "      <td>62.528617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.357109</td>\n",
       "      <td>0.8703</td>\n",
       "      <td>63.680855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>0.8700</td>\n",
       "      <td>13.302876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.369990</td>\n",
       "      <td>0.8697</td>\n",
       "      <td>10.484614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.352786</td>\n",
       "      <td>0.8696</td>\n",
       "      <td>18.233323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.348516</td>\n",
       "      <td>0.8694</td>\n",
       "      <td>20.526845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.355791</td>\n",
       "      <td>0.8665</td>\n",
       "      <td>14.837438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.360546</td>\n",
       "      <td>0.8665</td>\n",
       "      <td>64.268342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.374842</td>\n",
       "      <td>0.8654</td>\n",
       "      <td>46.449858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.377356</td>\n",
       "      <td>0.8646</td>\n",
       "      <td>30.068820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.401886</td>\n",
       "      <td>0.8577</td>\n",
       "      <td>27.959880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.387570</td>\n",
       "      <td>0.8574</td>\n",
       "      <td>39.324360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.501427</td>\n",
       "      <td>0.8089</td>\n",
       "      <td>57.140829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.503709</td>\n",
       "      <td>0.8066</td>\n",
       "      <td>57.435228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.513216</td>\n",
       "      <td>0.8031</td>\n",
       "      <td>47.958296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.515667</td>\n",
       "      <td>0.8027</td>\n",
       "      <td>47.993809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.539619</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>46.779934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.545006</td>\n",
       "      <td>0.7908</td>\n",
       "      <td>43.471015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.584445</td>\n",
       "      <td>0.7820</td>\n",
       "      <td>39.428976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.609138</td>\n",
       "      <td>0.7748</td>\n",
       "      <td>35.418992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate      Loss  Accuracy  Elapsed time\n",
       "0             3        200        0.00100  0.317017    0.8832     47.250145\n",
       "1             3        100        0.00100  0.323664    0.8825     42.748236\n",
       "2             3        100        0.00100  0.323536    0.8805     37.018595\n",
       "3             4        100        0.00100  0.321731    0.8804     48.126017\n",
       "4             4         50        0.00100  0.324752    0.8799     34.145171\n",
       "5             3         50        0.00100  0.330631    0.8792     35.311542\n",
       "6             2        200        0.00100  0.335732    0.8780     46.457211\n",
       "7             3         50        0.00100  0.332621    0.8777     36.066694\n",
       "8             4        100        0.00100  0.335166    0.8775     25.210360\n",
       "9             3         50        0.00100  0.337668    0.8770     36.626947\n",
       "10            3         50        0.00100  0.338841    0.8763     29.658820\n",
       "11            3         50        0.01000  0.345714    0.8750     19.217702\n",
       "12            4         50        0.01000  0.331463    0.8744     27.922565\n",
       "13            3         50        0.01000  0.338846    0.8743     23.160428\n",
       "14            3         50        0.01000  0.330785    0.8741     30.947168\n",
       "15            4         50        0.01000  0.337616    0.8737     24.846726\n",
       "16            4         50        0.01000  0.352578    0.8735     17.684977\n",
       "17            2         50        0.01000  0.360395    0.8724     14.019006\n",
       "18            3         50        0.01000  0.344019    0.8724     24.047585\n",
       "19            3         50        0.00100  0.343042    0.8718     36.220549\n",
       "20            3         50        0.01000  0.348408    0.8715     19.727731\n",
       "21            4        200        0.00010  0.356666    0.8704     63.734854\n",
       "22            4        200        0.00010  0.354076    0.8703     62.528617\n",
       "23            4        200        0.00010  0.357109    0.8703     63.680855\n",
       "24            3        100        0.01000  0.363600    0.8700     13.302876\n",
       "25            3         50        0.01000  0.369990    0.8697     10.484614\n",
       "26            3         50        0.01000  0.352786    0.8696     18.233323\n",
       "27            3         50        0.01000  0.348516    0.8694     20.526845\n",
       "28            3         50        0.01000  0.355791    0.8665     14.837438\n",
       "29            4        200        0.00010  0.360546    0.8665     64.268342\n",
       "30            2        200        0.00010  0.374842    0.8654     46.449858\n",
       "31            1        100        0.00100  0.377356    0.8646     30.068820\n",
       "32            1         50        0.00100  0.401886    0.8577     27.959880\n",
       "33            4         50        0.00010  0.387570    0.8574     39.324360\n",
       "34            4        150        0.00001  0.501427    0.8089     57.140829\n",
       "35            4        150        0.00001  0.503709    0.8066     57.435228\n",
       "36            4        100        0.00001  0.513216    0.8031     47.958296\n",
       "37            4        100        0.00001  0.515667    0.8027     47.993809\n",
       "38            2        200        0.00001  0.539619    0.7964     46.779934\n",
       "39            2        150        0.00001  0.545006    0.7908     43.471015\n",
       "40            4         50        0.00001  0.584445    0.7820     39.428976\n",
       "41            1        150        0.00001  0.609138    0.7748     35.418992"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_sdss.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Loss\", \"Accuracy\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Accuracy\", \"Elapsed time\"], ascending=[0,1], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 25.124 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
