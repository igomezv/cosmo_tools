{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>delta</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135.689107</td>\n",
       "      <td>32.494632</td>\n",
       "      <td>23.87882</td>\n",
       "      <td>22.27530</td>\n",
       "      <td>20.39501</td>\n",
       "      <td>19.16573</td>\n",
       "      <td>18.79371</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144.826101</td>\n",
       "      <td>31.274185</td>\n",
       "      <td>24.77759</td>\n",
       "      <td>22.83188</td>\n",
       "      <td>22.58444</td>\n",
       "      <td>21.16812</td>\n",
       "      <td>21.61427</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142.188790</td>\n",
       "      <td>35.582444</td>\n",
       "      <td>25.26307</td>\n",
       "      <td>22.66389</td>\n",
       "      <td>20.60976</td>\n",
       "      <td>19.34857</td>\n",
       "      <td>18.94827</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>338.741038</td>\n",
       "      <td>-0.402828</td>\n",
       "      <td>22.13682</td>\n",
       "      <td>23.77656</td>\n",
       "      <td>21.61162</td>\n",
       "      <td>20.50454</td>\n",
       "      <td>19.25010</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345.282593</td>\n",
       "      <td>21.183866</td>\n",
       "      <td>19.43718</td>\n",
       "      <td>17.58028</td>\n",
       "      <td>16.49747</td>\n",
       "      <td>15.97711</td>\n",
       "      <td>15.54461</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        alpha      delta         u         g         r         i         z  \\\n",
       "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
       "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
       "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
       "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
       "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
       "\n",
       "    class  \n",
       "0  GALAXY  \n",
       "1  GALAXY  \n",
       "2  GALAXY  \n",
       "3  GALAXY  \n",
       "4  GALAXY  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/igomezv/nnogada/main/data/star_classification.csv\"\n",
    "data = pd.read_csv(url)\n",
    "cols = ['alpha','delta','u','g','r','i','z','class']\n",
    "data = data[cols]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        alpha      delta         u         g         r         i         z  \\\n",
      "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
      "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
      "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
      "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
      "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
      "\n",
      "   class  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n"
     ]
    }
   ],
   "source": [
    "data[\"class\"]=[0 if i == \"GALAXY\" else 1 if i == \"STAR\" else 2 for i in data[\"class\"]]\n",
    "print(data.head())\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function layers\n",
    "\n",
    "# f1 = lambda x: Dense(x, activation='relu')      #ReLU\n",
    "# f2 = lambda x: Dense(x, activation='elu')       #ELU\n",
    "# f3 = lambda x: keras.layers.LeakyReLU(0.3)      #LReLU\n",
    "# f4 = lambda x: Dense(x, kernel_initializer='lecun_normal', activation='selu')   #SELU\n",
    "\n",
    "# f_names = [\"ReLU\", \"ELU\", \"LReLU\", \"SELU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([3, 4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([200, 100]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-5,1e-4,1e-3, 0.01])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([128,256])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=50,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=1)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 50\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into X and Y and implement hot_ones in Y\n",
    "def prepare_dataset(data):\n",
    "    X, Y = np.empty((0)), np.empty((0))\n",
    "    X = data[:,0:7]\n",
    "    Y = data[:,7]\n",
    "    Y = to_categorical(Y, num_classes=3)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation and test sets\n",
    "X,Y = prepare_dataset(data)\n",
    "\n",
    "# Defines ratios, w.r.t. whole dataset.\n",
    "ratio_train = 0.8\n",
    "ratio_val = 0.1\n",
    "ratio_test = 0.1\n",
    "\n",
    "# Produces test split.\n",
    "x_, X_test, y_, Y_test = split(X, Y, test_size = ratio_test, random_state=0)\n",
    "\n",
    "# Adjusts val ratio, w.r.t. remaining dataset.\n",
    "ratio_remaining = 1 - ratio_test\n",
    "ratio_val_adjusted = ratio_val / ratio_remaining\n",
    "\n",
    "# Produces train and val splits.\n",
    "X_train, X_val, Y_train, Y_val = split(x_, y_, test_size=ratio_val_adjusted, random_state=0)\n",
    "\n",
    "# Normalize and scale the input sets.\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "X_val   = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:1])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[1:2])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[2:4])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[4:5])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(num_units, input_shape=(int(X_train.shape[1]),)))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(3, activation=tf.nn.softmax))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=1, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Accuracy:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5      # probability for crossover\n",
    "    P_MUTATION = 0.2         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1  # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8783\n",
      "Accuracy: 0.8783000111579895 , Elapsed time: 64.25140953063965\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 884us/step - loss: 0.3438 - accuracy: 0.8787\n",
      "Accuracy: 0.8787000179290771 , Elapsed time: 86.16920208930969\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 851us/step - loss: 0.3415 - accuracy: 0.8731\n",
      "Accuracy: 0.8730999827384949 , Elapsed time: 53.168445110321045\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 886us/step - loss: 0.3348 - accuracy: 0.8748\n",
      "Accuracy: 0.8748000264167786 , Elapsed time: 39.79793620109558\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 771us/step - loss: 0.3605 - accuracy: 0.8694\n",
      "Accuracy: 0.8694000244140625 , Elapsed time: 53.707754135131836\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 936us/step - loss: 0.3446 - accuracy: 0.8756\n",
      "Accuracy: 0.8755999803543091 , Elapsed time: 71.32200622558594\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 793us/step - loss: 0.3745 - accuracy: 0.8632\n",
      "Accuracy: 0.8632000088691711 , Elapsed time: 47.30152726173401\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 904us/step - loss: 0.4017 - accuracy: 0.8468\n",
      "Accuracy: 0.8468000292778015 , Elapsed time: 33.940917015075684\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin     \tavg     \tmax     \n",
      "0  \t8     \t0.334817\t0.354725\t0.401661\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3633 - accuracy: 0.8677\n",
      "Accuracy: 0.8676999807357788 , Elapsed time: 56.50383377075195\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4086 - accuracy: 0.8535\n",
      "Accuracy: 0.8535000085830688 , Elapsed time: 35.19305443763733\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3399 - accuracy: 0.8762\n",
      "Accuracy: 0.8762000203132629 , Elapsed time: 92.006680727005\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3293 - accuracy: 0.8801\n",
      "Accuracy: 0.8801000118255615 , Elapsed time: 49.253037452697754\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.329268\t0.349901\t0.408606\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3354 - accuracy: 0.8806\n",
      "Accuracy: 0.8805999755859375 , Elapsed time: 61.50123476982117\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3681 - accuracy: 0.8664\n",
      "Accuracy: 0.8664000034332275 , Elapsed time: 82.82337284088135\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3518 - accuracy: 0.8726\n",
      "Accuracy: 0.8726000189781189 , Elapsed time: 52.375640869140625\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.9528 - accuracy: 0.5966\n",
      "Accuracy: 0.5965999960899353 , Elapsed time: 50.16310143470764\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t4     \t0.329268\t0.419601\t0.952813\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3420 - accuracy: 0.8734\n",
      "Accuracy: 0.8733999729156494 , Elapsed time: 59.19420647621155\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8800\n",
      "Accuracy: 0.8799999952316284 , Elapsed time: 77.8236153125763\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3432 - accuracy: 0.8779\n",
      "Accuracy: 0.8779000043869019 , Elapsed time: 51.71810698509216\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3307 - accuracy: 0.8785\n",
      "Accuracy: 0.8784999847412109 , Elapsed time: 49.64261054992676\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3609 - accuracy: 0.8671\n",
      "Accuracy: 0.8671000003814697 , Elapsed time: 88.97667455673218\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3789 - accuracy: 0.8602\n",
      "Accuracy: 0.8601999878883362 , Elapsed time: 49.099567890167236\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3402 - accuracy: 0.8743\n",
      "Accuracy: 0.8743000030517578 , Elapsed time: 50.30659103393555\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t7     \t0.330664\t0.345768\t0.378896\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3424 - accuracy: 0.8703\n",
      "Accuracy: 0.8702999949455261 , Elapsed time: 51.52969789505005\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8807\n",
      "Accuracy: 0.8806999921798706 , Elapsed time: 49.68199634552002\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t2     \t0.321242\t0.340842\t0.360943\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3709 - accuracy: 0.8617\n",
      "Accuracy: 0.8616999983787537 , Elapsed time: 76.84814953804016\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3269 - accuracy: 0.8801\n",
      "Accuracy: 0.8801000118255615 , Elapsed time: 52.47679829597473\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3547 - accuracy: 0.8714\n",
      "Accuracy: 0.871399998664856 , Elapsed time: 52.29362201690674\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3618 - accuracy: 0.8678\n",
      "Accuracy: 0.8677999973297119 , Elapsed time: 52.4060492515564\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.321242\t0.343394\t0.370866\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.6451 - accuracy: 0.7372\n",
      "Accuracy: 0.7372000217437744 , Elapsed time: 51.82656383514404\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 997us/step - loss: 0.3386 - accuracy: 0.8737\n",
      "Accuracy: 0.8737000226974487 , Elapsed time: 35.17683267593384\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3271 - accuracy: 0.8808\n",
      "Accuracy: 0.8808000087738037 , Elapsed time: 49.87650990486145\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3420 - accuracy: 0.8718\n",
      "Accuracy: 0.8718000054359436 , Elapsed time: 49.25705075263977\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t4     \t0.327089\t0.378296\t0.645109\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3320 - accuracy: 0.8759\n",
      "Accuracy: 0.8758999705314636 , Elapsed time: 52.22206377983093\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3254 - accuracy: 0.8781\n",
      "Accuracy: 0.8780999779701233 , Elapsed time: 54.148587226867676\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3446 - accuracy: 0.8737\n",
      "Accuracy: 0.8737000226974487 , Elapsed time: 60.73683953285217\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t3     \t0.325362\t0.338097\t0.344617\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3427 - accuracy: 0.8738\n",
      "Accuracy: 0.8737999796867371 , Elapsed time: 53.58824920654297\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3424 - accuracy: 0.8722\n",
      "Accuracy: 0.8722000122070312 , Elapsed time: 52.73407459259033\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.8660\n",
      "Accuracy: 0.8659999966621399 , Elapsed time: 51.87851405143738\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8680\n",
      "Accuracy: 0.8679999709129333 , Elapsed time: 63.014023780822754\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t4     \t0.325362\t0.345055\t0.372453\n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3526 - accuracy: 0.8697\n",
      "Accuracy: 0.869700014591217 , Elapsed time: 51.24173092842102\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 42 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8782\n",
      "Accuracy: 0.8781999945640564 , Elapsed time: 52.77311897277832\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 43 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3465 - accuracy: 0.8735\n",
      "Accuracy: 0.8734999895095825 , Elapsed time: 53.96856141090393\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 44 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3190 - accuracy: 0.8827\n",
      "Accuracy: 0.8827000260353088 , Elapsed time: 55.13547945022583\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 45 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8679\n",
      "Accuracy: 0.867900013923645 , Elapsed time: 53.1320686340332\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t5     \t0.318984\t0.338785\t0.369328\n",
      "\n",
      "--------------- Starting trial: 46 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8783\n",
      "Accuracy: 0.8783000111579895 , Elapsed time: 53.97825765609741\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 47 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3508 - accuracy: 0.8675\n",
      "Accuracy: 0.8675000071525574 , Elapsed time: 53.32335042953491\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 48 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3461 - accuracy: 0.8713\n",
      "Accuracy: 0.8712999820709229 , Elapsed time: 53.97509217262268\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 49 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3318 - accuracy: 0.8765\n",
      "Accuracy: 0.8765000104904175 , Elapsed time: 51.37056064605713\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 50 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3466 - accuracy: 0.8727\n",
      "Accuracy: 0.8726999759674072 , Elapsed time: 55.06818461418152\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 51 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3661 - accuracy: 0.8642\n",
      "Accuracy: 0.8641999959945679 , Elapsed time: 66.33246088027954\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t6     \t0.32765 \t0.342347\t0.366129\n",
      "-- Best Individual =  [0, 0, 1, 1, 1]\n",
      "-- Best Fitness =  0.33481746912002563\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 47.773662610848746 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 8  # max of individuals per generation\n",
    "max_generations = 10   # number of generations\n",
    "gene_length = 5      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01 , Batch size: 256\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:4])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[4:5])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    \n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1], \n",
    "          ', Learning rate:', best_learning_rate[-1], ', Batch size:', best_batch_size[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.318984</td>\n",
       "      <td>0.8827</td>\n",
       "      <td>55.135479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.327089</td>\n",
       "      <td>0.8808</td>\n",
       "      <td>49.876510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.321242</td>\n",
       "      <td>0.8807</td>\n",
       "      <td>49.681996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.335424</td>\n",
       "      <td>0.8806</td>\n",
       "      <td>61.501235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.329268</td>\n",
       "      <td>0.8801</td>\n",
       "      <td>49.253037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.326924</td>\n",
       "      <td>0.8801</td>\n",
       "      <td>52.476798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.335487</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>77.823615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.343779</td>\n",
       "      <td>0.8787</td>\n",
       "      <td>86.169202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.330664</td>\n",
       "      <td>0.8785</td>\n",
       "      <td>49.642611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.327650</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>53.978258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.336371</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>64.251410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.327809</td>\n",
       "      <td>0.8782</td>\n",
       "      <td>52.773119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.325362</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>54.148587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.343155</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>51.718107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.331810</td>\n",
       "      <td>0.8765</td>\n",
       "      <td>51.370561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.339938</td>\n",
       "      <td>0.8762</td>\n",
       "      <td>92.006681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.331961</td>\n",
       "      <td>0.8759</td>\n",
       "      <td>52.222064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.344644</td>\n",
       "      <td>0.8756</td>\n",
       "      <td>71.322006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.334817</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>39.797936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.340162</td>\n",
       "      <td>0.8743</td>\n",
       "      <td>50.306591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.342706</td>\n",
       "      <td>0.8738</td>\n",
       "      <td>53.588249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.338632</td>\n",
       "      <td>0.8737</td>\n",
       "      <td>35.176833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.344617</td>\n",
       "      <td>0.8737</td>\n",
       "      <td>60.736840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.346520</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>53.968561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.342019</td>\n",
       "      <td>0.8734</td>\n",
       "      <td>59.194206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.341506</td>\n",
       "      <td>0.8731</td>\n",
       "      <td>53.168445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.346625</td>\n",
       "      <td>0.8727</td>\n",
       "      <td>55.068185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.351761</td>\n",
       "      <td>0.8726</td>\n",
       "      <td>52.375641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.342447</td>\n",
       "      <td>0.8722</td>\n",
       "      <td>52.734075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.341990</td>\n",
       "      <td>0.8718</td>\n",
       "      <td>49.257051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.354695</td>\n",
       "      <td>0.8714</td>\n",
       "      <td>52.293622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.346103</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>53.975092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.342383</td>\n",
       "      <td>0.8703</td>\n",
       "      <td>51.529698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.352644</td>\n",
       "      <td>0.8697</td>\n",
       "      <td>51.241731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.360491</td>\n",
       "      <td>0.8694</td>\n",
       "      <td>53.707754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.372453</td>\n",
       "      <td>0.8680</td>\n",
       "      <td>63.014024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.369328</td>\n",
       "      <td>0.8679</td>\n",
       "      <td>53.132069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.361770</td>\n",
       "      <td>0.8678</td>\n",
       "      <td>52.406049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>256</td>\n",
       "      <td>0.363338</td>\n",
       "      <td>0.8677</td>\n",
       "      <td>56.503834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.350824</td>\n",
       "      <td>0.8675</td>\n",
       "      <td>53.323350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.360943</td>\n",
       "      <td>0.8671</td>\n",
       "      <td>88.976675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.368145</td>\n",
       "      <td>0.8664</td>\n",
       "      <td>82.823373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.368703</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>51.878514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.366129</td>\n",
       "      <td>0.8642</td>\n",
       "      <td>66.332461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.374532</td>\n",
       "      <td>0.8632</td>\n",
       "      <td>47.301527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.370866</td>\n",
       "      <td>0.8617</td>\n",
       "      <td>76.848150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>256</td>\n",
       "      <td>0.378896</td>\n",
       "      <td>0.8602</td>\n",
       "      <td>49.099568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>256</td>\n",
       "      <td>0.408606</td>\n",
       "      <td>0.8535</td>\n",
       "      <td>35.193054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.401661</td>\n",
       "      <td>0.8468</td>\n",
       "      <td>33.940917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.645109</td>\n",
       "      <td>0.7372</td>\n",
       "      <td>51.826564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss  Accuracy  \\\n",
       "0             3        200         0.0100         256  0.318984    0.8827   \n",
       "1             3        200         0.0100         256  0.327089    0.8808   \n",
       "2             3        200         0.0100         256  0.321242    0.8807   \n",
       "3             4        200         0.0100         256  0.335424    0.8806   \n",
       "4             3        200         0.0100         256  0.329268    0.8801   \n",
       "5             3        200         0.0100         256  0.326924    0.8801   \n",
       "6             3        200         0.0100         128  0.335487    0.8800   \n",
       "7             4        200         0.0100         128  0.343779    0.8787   \n",
       "8             3        200         0.0100         256  0.330664    0.8785   \n",
       "9             3        200         0.0100         256  0.327650    0.8783   \n",
       "10            4        100         0.0100         128  0.336371    0.8783   \n",
       "11            3        200         0.0100         256  0.327809    0.8782   \n",
       "12            3        200         0.0100         256  0.325362    0.8781   \n",
       "13            3        200         0.0100         256  0.343155    0.8779   \n",
       "14            3        200         0.0100         256  0.331810    0.8765   \n",
       "15            4        200         0.0100         128  0.339938    0.8762   \n",
       "16            3        200         0.0100         256  0.331961    0.8759   \n",
       "17            4        200         0.0100         128  0.344644    0.8756   \n",
       "18            3        200         0.0100         256  0.334817    0.8748   \n",
       "19            3        200         0.0100         256  0.340162    0.8743   \n",
       "20            3        200         0.0100         256  0.342706    0.8738   \n",
       "21            3        100         0.0100         256  0.338632    0.8737   \n",
       "22            4        200         0.0100         256  0.344617    0.8737   \n",
       "23            3        200         0.0100         256  0.346520    0.8735   \n",
       "24            4        200         0.0100         256  0.342019    0.8734   \n",
       "25            4        100         0.0100         128  0.341506    0.8731   \n",
       "26            3        200         0.0100         256  0.346625    0.8727   \n",
       "27            3        200         0.0100         256  0.351761    0.8726   \n",
       "28            3        200         0.0100         256  0.342447    0.8722   \n",
       "29            3        200         0.0100         256  0.341990    0.8718   \n",
       "30            3        200         0.0100         256  0.354695    0.8714   \n",
       "31            3        200         0.0100         256  0.346103    0.8713   \n",
       "32            3        200         0.0100         256  0.342383    0.8703   \n",
       "33            3        200         0.0100         256  0.352644    0.8697   \n",
       "34            4        100         0.0001         128  0.360491    0.8694   \n",
       "35            4        200         0.0100         256  0.372453    0.8680   \n",
       "36            3        200         0.0100         256  0.369328    0.8679   \n",
       "37            3        200         0.0100         256  0.361770    0.8678   \n",
       "38            4        200         0.0001         256  0.363338    0.8677   \n",
       "39            3        200         0.0100         256  0.350824    0.8675   \n",
       "40            4        200         0.0100         128  0.360943    0.8671   \n",
       "41            3        200         0.0100         128  0.368145    0.8664   \n",
       "42            3        200         0.0100         256  0.368703    0.8660   \n",
       "43            4        200         0.0100         256  0.366129    0.8642   \n",
       "44            3        100         0.0001         128  0.374532    0.8632   \n",
       "45            3        200         0.0100         128  0.370866    0.8617   \n",
       "46            3        200         0.0001         256  0.378896    0.8602   \n",
       "47            3        100         0.0001         256  0.408606    0.8535   \n",
       "48            4        100         0.0100         256  0.401661    0.8468   \n",
       "49            3        200         0.0100         256  0.645109    0.7372   \n",
       "\n",
       "    Elapsed time  \n",
       "0      55.135479  \n",
       "1      49.876510  \n",
       "2      49.681996  \n",
       "3      61.501235  \n",
       "4      49.253037  \n",
       "5      52.476798  \n",
       "6      77.823615  \n",
       "7      86.169202  \n",
       "8      49.642611  \n",
       "9      53.978258  \n",
       "10     64.251410  \n",
       "11     52.773119  \n",
       "12     54.148587  \n",
       "13     51.718107  \n",
       "14     51.370561  \n",
       "15     92.006681  \n",
       "16     52.222064  \n",
       "17     71.322006  \n",
       "18     39.797936  \n",
       "19     50.306591  \n",
       "20     53.588249  \n",
       "21     35.176833  \n",
       "22     60.736840  \n",
       "23     53.968561  \n",
       "24     59.194206  \n",
       "25     53.168445  \n",
       "26     55.068185  \n",
       "27     52.375641  \n",
       "28     52.734075  \n",
       "29     49.257051  \n",
       "30     52.293622  \n",
       "31     53.975092  \n",
       "32     51.529698  \n",
       "33     51.241731  \n",
       "34     53.707754  \n",
       "35     63.014024  \n",
       "36     53.132069  \n",
       "37     52.406049  \n",
       "38     56.503834  \n",
       "39     53.323350  \n",
       "40     88.976675  \n",
       "41     82.823373  \n",
       "42     51.878514  \n",
       "43     66.332461  \n",
       "44     47.301527  \n",
       "45     76.848150  \n",
       "46     49.099568  \n",
       "47     35.193054  \n",
       "48     33.940917  \n",
       "49     51.826564  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_sdss_1.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Accuracy\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Accuracy\", \"Elapsed time\"], ascending=[0,1], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 47.771 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
