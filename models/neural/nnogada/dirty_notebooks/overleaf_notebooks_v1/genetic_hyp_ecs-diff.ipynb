{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elitism succesfully imported\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "from neurapprox.elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm for splitting the dataset into training and validation \n",
    "def split(X,Y,porcent): #porcent must be between 0 and 1, it is the asigned porcent to the training dataset.\n",
    "    n=floor(porcent*len(X))\n",
    "    index=random.sample(range(len(X)),n)\n",
    "    X_learn=[]\n",
    "    Y_learn=[]\n",
    "    for i in index:\n",
    "        X_learn.append(X[i])\n",
    "        Y_learn.append(Y[i])\n",
    "    X_val=np.delete(X,index, axis=0)\n",
    "    Y_val=np.delete(Y,index, axis=0)\n",
    "    \n",
    "    X_learn=np.array(X_learn)\n",
    "    Y_learn=np.array(Y_learn)\n",
    "    return X_learn,Y_learn,X_val,Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_m=np.arange(0.1,0.51,0.1)\n",
    "H_0=np.arange(66,81,1)\n",
    "t=np.linspace(0,-12,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RHS(Omega_i, lna, gamma=0):\n",
    "    x, y, z, H = Omega_i\n",
    "    #x, y, z = Omega_i\n",
    "    pi = 3*x + 4*y\n",
    "    return [x*(-3 + pi), y*(-4 + pi), z*pi, -0.5*H*pi]\n",
    "    #return [x*(-3 + pi), y*(-4 + pi), z*pi]\n",
    "\n",
    "def EDO(t,Om,H0):\n",
    "    #t,Or,Om,Ol=X\n",
    "    Or=0.0001\n",
    "    Ol=1-Or-Om\n",
    "    #H0 = 70.\n",
    "    y0 = [Om, Or, Ol, H0]\n",
    "    result = odeint(RHS, y0, t)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets generate the cartesian product between the intervals\n",
    "Y0=[]\n",
    "#este ciclo llena la lista fijando un Om y pasando todos los Or\n",
    "for i in O_m:\n",
    "    for j in H_0:\n",
    "        Y0.extend(EDO(t,i,j))\n",
    "Y0=np.array(Y0)\n",
    "\n",
    "X0=[]\n",
    "for Om in O_m:\n",
    "    for H0 in H_0:\n",
    "        for T in t:\n",
    "            X0.append([T,Om,H0])\n",
    "X0=np.array(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "Y2 = scaler.fit_transform(Y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feactures= \n",
      " [[  0.           0.1         66.        ]\n",
      " [ -0.24489796   0.1         66.        ]\n",
      " [ -0.48979592   0.1         66.        ]\n",
      " ...\n",
      " [-11.51020408   0.5         80.        ]\n",
      " [-11.75510204   0.5         80.        ]\n",
      " [-12.           0.5         80.        ]]\n",
      "\n",
      "\n",
      "labels= \n",
      " [[9.47515373e-02 0.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      " [1.83608702e-01 1.41110961e-04 9.01997379e-01 1.62358493e-10]\n",
      " [3.22333902e-01 4.33937896e-04 7.48914062e-01 4.77144941e-10]\n",
      " ...\n",
      " [4.20164900e-02 9.58104047e-01 5.58400899e-11 3.78984027e-01]\n",
      " [3.19464665e-02 9.68145182e-01 5.73926010e-11 6.15280156e-01]\n",
      " [2.39153274e-02 9.76153281e-01 5.75107557e-11 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Now, here are the datasets\n",
    "print('feactures= \\n',X0)\n",
    "print('\\n')\n",
    "print('labels= \\n',Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = 0.8\n",
    "X_train, Y_train, X_test, Y_test = split(X0, Y2, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-5,1e-4,1e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([4, 8,16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=100,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=1)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 500\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces train and val splits.\n",
    "X_test, Y_test, X_val, Y_val = split(X_test, Y_test, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:1])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[1:2])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[2:3])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[3:4])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(4, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 3), (375, 3), (375, 3))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.8        # probability for crossover\n",
    "    P_MUTATION = 0.2         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 737us/step - loss: 7.6169e-04 - mean_squared_error: 7.6169e-04\n",
      "Loss: 0.0007616882212460041 , Elapsed time: 147.03385615348816\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "12/12 [==============================] - 0s 687us/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Loss: 0.005084887612611055 , Elapsed time: 142.30754041671753\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch: 248.\n",
      "Epoch 348: early stopping\n",
      "12/12 [==============================] - 0s 686us/step - loss: 0.0017 - mean_squared_error: 0.0017\n",
      "Loss: 0.0017312535783275962 , Elapsed time: 99.78737473487854\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "12/12 [==============================] - 0s 699us/step - loss: 0.0023 - mean_squared_error: 0.0023\n",
      "Loss: 0.0023384743835777044 , Elapsed time: 278.9852919578552\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 662us/step - loss: 0.0012 - mean_squared_error: 0.0012\n",
      "Loss: 0.0012029989156872034 , Elapsed time: 142.9903280735016\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "12/12 [==============================] - 0s 804us/step - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Loss: 0.0056494902819395065 , Elapsed time: 152.04800057411194\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "12/12 [==============================] - 0s 654us/step - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Loss: 0.0062111117877066135 , Elapsed time: 143.3927924633026\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "12/12 [==============================] - 0s 675us/step - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Loss: 0.006060490384697914 , Elapsed time: 144.38608074188232\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "12/12 [==============================] - 0s 680us/step - loss: 0.0034 - mean_squared_error: 0.0034\n",
      "Loss: 0.003356358502060175 , Elapsed time: 138.95138502120972\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 703us/step - loss: 2.2469e-04 - mean_squared_error: 2.2469e-04\n",
      "Loss: 0.00022468996758107096 , Elapsed time: 142.46502304077148\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 684us/step - loss: 5.5931e-04 - mean_squared_error: 5.5931e-04\n",
      "Loss: 0.0005593076348304749 , Elapsed time: 277.9677221775055\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin       \tavg       \tmax       \n",
      "0  \t11    \t0.00022469\t0.00301643\t0.00621111\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch: 303.\n",
      "Epoch 403: early stopping\n",
      "12/12 [==============================] - 0s 645us/step - loss: 0.0012 - mean_squared_error: 0.0012  \n",
      "Loss: 0.0012127278605476022 , Elapsed time: 110.41931700706482\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 671us/step - loss: 8.4899e-04 - mean_squared_error: 8.4899e-04\n",
      "Loss: 0.000848993775434792 , Elapsed time: 137.88119292259216\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "12/12 [==============================] - 0s 709us/step - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Loss: 0.0072709196247160435 , Elapsed time: 137.2206790447235\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "12/12 [==============================] - 0s 685us/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Loss: 0.005075405351817608 , Elapsed time: 139.2672667503357\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 740us/step - loss: 5.3413e-04 - mean_squared_error: 5.3413e-04\n",
      "Loss: 0.0005341271171346307 , Elapsed time: 147.03767776489258\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 674us/step - loss: 6.7294e-04 - mean_squared_error: 6.7294e-04\n",
      "Loss: 0.0006729417364113033 , Elapsed time: 138.19949507713318\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t6     \t0.00022469\t0.00168016\t0.00727092\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 797us/step - loss: 2.6779e-04 - mean_squared_error: 2.6779e-04\n",
      "Loss: 0.00026779129984788597 , Elapsed time: 278.9462423324585\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 629us/step - loss: 0.0010 - mean_squared_error: 0.0010  \n",
      "Loss: 0.0010290408972650766 , Elapsed time: 139.41754627227783\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 726us/step - loss: 6.3333e-04 - mean_squared_error: 6.3333e-04\n",
      "Loss: 0.0006333278142847121 , Elapsed time: 144.75910997390747\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 728us/step - loss: 6.4724e-04 - mean_squared_error: 6.4724e-04\n",
      "Loss: 0.0006472436361946166 , Elapsed time: 143.3836226463318\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 664us/step - loss: 9.1539e-04 - mean_squared_error: 9.1539e-04\n",
      "Loss: 0.0009153860155493021 , Elapsed time: 137.8561236858368\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch: 346.\n",
      "Epoch 446: early stopping\n",
      "12/12 [==============================] - 0s 622us/step - loss: 3.8052e-04 - mean_squared_error: 3.8052e-04\n",
      "Loss: 0.00038051846786402166 , Elapsed time: 247.5713505744934\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 668us/step - loss: 0.0012 - mean_squared_error: 0.0012  \n",
      "Loss: 0.0012110016541555524 , Elapsed time: 144.42532563209534\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 724us/step - loss: 2.2027e-04 - mean_squared_error: 2.2027e-04\n",
      "Loss: 0.00022027162776794285 , Elapsed time: 145.50635719299316\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 731us/step - loss: 8.0637e-04 - mean_squared_error: 8.0637e-04\n",
      "Loss: 0.000806368887424469 , Elapsed time: 144.77261233329773\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 685us/step - loss: 8.5270e-04 - mean_squared_error: 8.5270e-04\n",
      "Loss: 0.0008527007885277271 , Elapsed time: 136.8758363723755\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t10    \t0.000220272\t0.000653486\t0.001211  \n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 4 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 658us/step - loss: 8.1148e-04 - mean_squared_error: 8.1148e-04\n",
      "Loss: 0.0008114770171232522 , Elapsed time: 143.42208647727966\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 728us/step - loss: 3.9982e-04 - mean_squared_error: 3.9982e-04\n",
      "Loss: 0.00039981608279049397 , Elapsed time: 263.6616373062134\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 697us/step - loss: 8.6554e-04 - mean_squared_error: 8.6554e-04\n",
      "Loss: 0.0008655435522086918 , Elapsed time: 143.65442895889282\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 788us/step - loss: 2.6148e-04 - mean_squared_error: 2.6148e-04\n",
      "Loss: 0.0002614751865621656 , Elapsed time: 138.3100037574768\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 738us/step - loss: 3.6393e-04 - mean_squared_error: 3.6393e-04\n",
      "Loss: 0.00036392835318110883 , Elapsed time: 144.85750102996826\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 642us/step - loss: 7.4450e-04 - mean_squared_error: 7.4450e-04\n",
      "Loss: 0.0007444983348250389 , Elapsed time: 137.30814814567566\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 685us/step - loss: 2.0966e-04 - mean_squared_error: 2.0966e-04\n",
      "Loss: 0.0002096571697620675 , Elapsed time: 143.85205841064453\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 711us/step - loss: 7.3643e-04 - mean_squared_error: 7.3643e-04\n",
      "Loss: 0.0007364311022683978 , Elapsed time: 145.71665215492249\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t8     \t0.000209657\t0.00059654 \t0.00102904\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 775us/step - loss: 4.4857e-04 - mean_squared_error: 4.4857e-04\n",
      "Loss: 0.0004485718091018498 , Elapsed time: 144.04013562202454\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 853us/step - loss: 1.8587e-04 - mean_squared_error: 1.8587e-04\n",
      "Loss: 0.00018587360682431608 , Elapsed time: 143.94554448127747\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 751us/step - loss: 9.0306e-04 - mean_squared_error: 9.0306e-04\n",
      "Loss: 0.0009030597284436226 , Elapsed time: 144.52763509750366\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 722us/step - loss: 4.7909e-04 - mean_squared_error: 4.7909e-04\n",
      "Loss: 0.0004790877574123442 , Elapsed time: 144.10616517066956\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch: 391.\n",
      "Epoch 491: early stopping\n",
      "12/12 [==============================] - 0s 691us/step - loss: 1.9052e-04 - mean_squared_error: 1.9052e-04\n",
      "Loss: 0.00019051716662943363 , Elapsed time: 271.23757576942444\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 675us/step - loss: 5.9903e-04 - mean_squared_error: 5.9903e-04\n",
      "Loss: 0.0005990272620692849 , Elapsed time: 144.51921463012695\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t6     \t0.000185874\t0.000507882\t0.00102904\n",
      "\n",
      "--------------- Starting trial: 42 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 686us/step - loss: 4.6156e-04 - mean_squared_error: 4.6156e-04\n",
      "Loss: 0.0004615564248524606 , Elapsed time: 143.71356630325317\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 43 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 686us/step - loss: 4.4910e-04 - mean_squared_error: 4.4910e-04\n",
      "Loss: 0.000449102750280872 , Elapsed time: 144.2361981868744\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 44 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 701us/step - loss: 6.0699e-04 - mean_squared_error: 6.0699e-04\n",
      "Loss: 0.0006069927476346493 , Elapsed time: 262.0982458591461\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 45 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 721us/step - loss: 6.5240e-04 - mean_squared_error: 6.5240e-04\n",
      "Loss: 0.0006523967022076249 , Elapsed time: 144.85947036743164\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 46 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch: 229.\n",
      "Epoch 329: early stopping\n",
      "12/12 [==============================] - 0s 713us/step - loss: 0.0013 - mean_squared_error: 0.0013\n",
      "Loss: 0.0013004953507333994 , Elapsed time: 95.4624514579773\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 47 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch: 348.\n",
      "Epoch 448: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 751us/step - loss: 8.5368e-04 - mean_squared_error: 8.5368e-04\n",
      "Loss: 0.0008536807727068663 , Elapsed time: 133.88571166992188\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 48 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch: 295.\n",
      "Epoch 395: early stopping\n",
      "12/12 [==============================] - 0s 731us/step - loss: 0.0011 - mean_squared_error: 0.0011\n",
      "Loss: 0.0010695105884224176 , Elapsed time: 118.01053214073181\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 49 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "12/12 [==============================] - 0s 651us/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "Loss: 0.0015860326820984483 , Elapsed time: 140.9035005569458\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t8     \t0.000185874\t0.000692512\t0.00158603\n",
      "-- Best Individual =  [1, 1, 1, 1]\n",
      "-- Best Fitness =  0.00018587360682431608\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 128.6726490219434 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 11   # max of individuals per generation\n",
    "max_generations = 5    # number of generations\n",
    "gene_length = 4       # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 4\n",
      "Number of neurons: 100 , Batch size 8 , Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    \n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1])\n",
    "    print('Number of neurons:', best_num_units[-1], ', Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>143.945544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>271.237576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>143.852058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>145.506357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>142.465023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>138.310004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>278.946242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>144.857501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>247.571351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>263.661637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>144.040136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>144.236198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>143.713566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>144.106165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>147.037678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>277.967722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>144.519215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>262.098246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>144.759110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>143.383623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>144.859470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>138.199495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>145.716652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>137.308148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>147.033856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>144.772612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>143.422086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>137.881193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>136.875836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>133.885712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>143.654429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>144.527635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>137.856124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>139.417546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>118.010532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>142.990328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>144.425326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>110.419317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>95.462451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>140.903501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>99.787375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>278.985292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>138.951385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>139.267267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.005085</td>\n",
       "      <td>0.005085</td>\n",
       "      <td>142.307540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.005649</td>\n",
       "      <td>0.005649</td>\n",
       "      <td>152.048001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.006060</td>\n",
       "      <td>0.006060</td>\n",
       "      <td>144.386081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>143.392792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.007271</td>\n",
       "      <td>0.007271</td>\n",
       "      <td>137.220679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             4        100        0.00010           8  0.000186  0.000186   \n",
       "1             4        100        0.00010           4  0.000191  0.000191   \n",
       "2             4        100        0.00010           8  0.000210  0.000210   \n",
       "3             4        100        0.00010           8  0.000220  0.000220   \n",
       "4             4        100        0.00010           8  0.000225  0.000225   \n",
       "5             3        100        0.00010           8  0.000261  0.000261   \n",
       "6             4         50        0.00010           4  0.000268  0.000268   \n",
       "7             4        100        0.00010           8  0.000364  0.000364   \n",
       "8             4         50        0.00010           4  0.000381  0.000381   \n",
       "9             3        100        0.00010           4  0.000400  0.000400   \n",
       "10            4        100        0.00010           8  0.000449  0.000449   \n",
       "11            4        100        0.00010           8  0.000449  0.000449   \n",
       "12            4        100        0.00010           8  0.000462  0.000462   \n",
       "13            4        100        0.00010           8  0.000479  0.000479   \n",
       "14            4        100        0.00010           8  0.000534  0.000534   \n",
       "15            3         50        0.00010           4  0.000559  0.000559   \n",
       "16            4        100        0.00010           8  0.000599  0.000599   \n",
       "17            3        100        0.00010           4  0.000607  0.000607   \n",
       "18            4        100        0.00010           8  0.000633  0.000633   \n",
       "19            4        100        0.00010           8  0.000647  0.000647   \n",
       "20            4        100        0.00010           8  0.000652  0.000652   \n",
       "21            3        100        0.00010           8  0.000673  0.000673   \n",
       "22            4        100        0.00010           8  0.000736  0.000736   \n",
       "23            3        100        0.00010           8  0.000744  0.000744   \n",
       "24            4        100        0.00010           8  0.000762  0.000762   \n",
       "25            4        100        0.00010           8  0.000806  0.000806   \n",
       "26            4         50        0.00010           8  0.000811  0.000811   \n",
       "27            3        100        0.00010           8  0.000849  0.000849   \n",
       "28            3        100        0.00010           8  0.000853  0.000853   \n",
       "29            4        100        0.00010           8  0.000854  0.000854   \n",
       "30            4        100        0.00010           8  0.000866  0.000866   \n",
       "31            4        100        0.00010           8  0.000903  0.000903   \n",
       "32            3        100        0.00010           8  0.000915  0.000915   \n",
       "33            3        100        0.00010           8  0.001029  0.001029   \n",
       "34            4        100        0.00010           8  0.001070  0.001070   \n",
       "35            3        100        0.00010           8  0.001203  0.001203   \n",
       "36            4        100        0.00010           8  0.001211  0.001211   \n",
       "37            3        100        0.00010           8  0.001213  0.001213   \n",
       "38            4        100        0.00010           8  0.001300  0.001300   \n",
       "39            3        100        0.00010           8  0.001586  0.001586   \n",
       "40            4         50        0.00010           8  0.001731  0.001731   \n",
       "41            4        100        0.00001           4  0.002338  0.002338   \n",
       "42            3        100        0.00001           8  0.003356  0.003356   \n",
       "43            3        100        0.00001           8  0.005075  0.005075   \n",
       "44            3        100        0.00001           8  0.005085  0.005085   \n",
       "45            4        100        0.00001           8  0.005649  0.005649   \n",
       "46            4         50        0.00001           8  0.006060  0.006060   \n",
       "47            3        100        0.00001           8  0.006211  0.006211   \n",
       "48            3        100        0.00001           8  0.007271  0.007271   \n",
       "\n",
       "    Elapsed time  \n",
       "0     143.945544  \n",
       "1     271.237576  \n",
       "2     143.852058  \n",
       "3     145.506357  \n",
       "4     142.465023  \n",
       "5     138.310004  \n",
       "6     278.946242  \n",
       "7     144.857501  \n",
       "8     247.571351  \n",
       "9     263.661637  \n",
       "10    144.040136  \n",
       "11    144.236198  \n",
       "12    143.713566  \n",
       "13    144.106165  \n",
       "14    147.037678  \n",
       "15    277.967722  \n",
       "16    144.519215  \n",
       "17    262.098246  \n",
       "18    144.759110  \n",
       "19    143.383623  \n",
       "20    144.859470  \n",
       "21    138.199495  \n",
       "22    145.716652  \n",
       "23    137.308148  \n",
       "24    147.033856  \n",
       "25    144.772612  \n",
       "26    143.422086  \n",
       "27    137.881193  \n",
       "28    136.875836  \n",
       "29    133.885712  \n",
       "30    143.654429  \n",
       "31    144.527635  \n",
       "32    137.856124  \n",
       "33    139.417546  \n",
       "34    118.010532  \n",
       "35    142.990328  \n",
       "36    144.425326  \n",
       "37    110.419317  \n",
       "38     95.462451  \n",
       "39    140.903501  \n",
       "40     99.787375  \n",
       "41    278.985292  \n",
       "42    138.951385  \n",
       "43    139.267267  \n",
       "44    142.307540  \n",
       "45    152.048001  \n",
       "46    144.386081  \n",
       "47    143.392792  \n",
       "48    137.220679  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_ecsdiff.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 128.670 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
