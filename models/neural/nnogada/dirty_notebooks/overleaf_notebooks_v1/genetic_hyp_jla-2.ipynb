{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/igomezv/nnogada/main/data/jla.csv'\n",
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 1), (148, 1), (592, 2), (148, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([1,2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,150,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-4,5e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([2, 4, 8, 16])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=200,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 200\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:2])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[2:4])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[4:5])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[5:7])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "#     model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.8         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1013 - mean_squared_error: 0.1013\n",
      "Loss: 0.10129734873771667 , Elapsed time: 105.21967220306396\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0505 - mean_squared_error: 0.0505\n",
      "Loss: 0.05051962658762932 , Elapsed time: 19.645350456237793\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0575 - mean_squared_error: 0.0575\n",
      "Loss: 0.05750845745205879 , Elapsed time: 29.325452089309692\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2627 - mean_squared_error: 0.2627\n",
      "Loss: 0.26267099380493164 , Elapsed time: 60.54411482810974\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Loss: 0.034958988428115845 , Elapsed time: 67.47471809387207\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin     \tavg     \tmax     \n",
      "0  \t5     \t0.034959\t0.101391\t0.262671\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Loss: 0.035576172173023224 , Elapsed time: 68.45074605941772\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0678 - mean_squared_error: 0.0678\n",
      "Loss: 0.06784854084253311 , Elapsed time: 29.628061532974243\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0445 - mean_squared_error: 0.0445\n",
      "Loss: 0.044494472444057465 , Elapsed time: 70.65735483169556\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0419 - mean_squared_error: 0.0419\n",
      "Loss: 0.041904110461473465 , Elapsed time: 53.38654804229736\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.034959\t0.0449565\t0.0678485\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Loss: 0.035907190293073654 , Elapsed time: 72.57767581939697\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0496 - mean_squared_error: 0.0496\n",
      "Loss: 0.04962281510233879 , Elapsed time: 68.99512028694153\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Loss: 0.03274567425251007 , Elapsed time: 81.6473400592804\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t3     \t0.0327457\t0.0377622\t0.0496228\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 1s 4ms/step - loss: 0.0404 - mean_squared_error: 0.0404\n",
      "Loss: 0.040374528616666794 , Elapsed time: 69.23440384864807\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Loss: 0.03658952936530113 , Elapsed time: 77.36590814590454\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0384 - mean_squared_error: 0.0384\n",
      "Loss: 0.038370538502931595 , Elapsed time: 65.54067707061768\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t3     \t0.0327457\t0.0367313\t0.0403745\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Loss: 0.0311958659440279 , Elapsed time: 93.1959478855133\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0593 - mean_squared_error: 0.0593\n",
      "Loss: 0.059346314519643784 , Elapsed time: 65.79305815696716\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 4 , Number of neurons: 150 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Loss: 0.03185492753982544 , Elapsed time: 115.467294216156\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.4140 - mean_squared_error: 1.4140\n",
      "Loss: 1.4139583110809326 , Elapsed time: 27.703692197799683\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t4     \t0.0311959\t0.31382  \t1.41396  \n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0560 - mean_squared_error: 0.0560\n",
      "Loss: 0.05595771223306656 , Elapsed time: 87.63361668586731\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4890 - mean_squared_error: 0.4890\n",
      "Loss: 0.4889577329158783 , Elapsed time: 19.797006368637085\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0600 - mean_squared_error: 0.0600\n",
      "Loss: 0.05996232479810715 , Elapsed time: 70.86935639381409\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 1 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5235 - mean_squared_error: 0.5235\n",
      "Loss: 0.5235332250595093 , Elapsed time: 26.067688941955566\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.0311959\t0.231921 \t0.523533 \n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0593 - mean_squared_error: 0.0593\n",
      "Loss: 0.05934857577085495 , Elapsed time: 72.57060694694519\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.005\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0396 - mean_squared_error: 0.0396\n",
      "Loss: 0.03963995724916458 , Elapsed time: 53.05216956138611\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Loss: 0.035651400685310364 , Elapsed time: 84.48363995552063\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 1 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6391 - mean_squared_error: 0.6391\n",
      "Loss: 0.6390981078147888 , Elapsed time: 21.625296354293823\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t4     \t0.0311959\t0.160987 \t0.639098 \n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Loss: 0.03183432295918465 , Elapsed time: 91.440269947052\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Loss: 0.03644319996237755 , Elapsed time: 87.55647373199463\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Loss: 0.03584475442767143 , Elapsed time: 103.79236078262329\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0380 - mean_squared_error: 0.0380\n",
      "Loss: 0.03796852007508278 , Elapsed time: 33.19493126869202\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.0311959\t0.0346573\t0.0379685\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Loss: 0.033477433025836945 , Elapsed time: 99.46378064155579\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Loss: 0.03876449540257454 , Elapsed time: 91.38022041320801\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Loss: 0.033436570316553116 , Elapsed time: 115.13422846794128\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Loss: 0.03981959447264671 , Elapsed time: 96.61584806442261\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t4     \t0.0311959\t0.0353388\t0.0398196\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Loss: 0.03446583449840546 , Elapsed time: 101.00955963134766\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Loss: 0.033642079681158066 , Elapsed time: 86.6879620552063\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Loss: 0.035040032118558884 , Elapsed time: 71.19388937950134\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0399 - mean_squared_error: 0.0399\n",
      "Loss: 0.03990975394845009 , Elapsed time: 21.636767148971558\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t4     \t0.0311959\t0.0348507\t0.0399098\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Loss: 0.030927926301956177 , Elapsed time: 85.06098318099976\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Loss: 0.0345277339220047 , Elapsed time: 87.7073540687561\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 42 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Loss: 0.030271008610725403 , Elapsed time: 96.6287841796875\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 43 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Loss: 0.031804997473955154 , Elapsed time: 106.3297233581543\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t4     \t0.030271 \t0.0317455\t0.0345277\n",
      "-- Best Individual =  [1, 1, 0, 1, 0, 0, 1]\n",
      "-- Best Fitness =  0.0311958659440279\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 50.8882106145223 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5   # max of individuals per generation\n",
    "max_generations = 10  # number of generations\n",
    "gene_length = 7      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 2 , Number of neurons: 100\n",
      "Batch size 4 , Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030271</td>\n",
       "      <td>0.030271</td>\n",
       "      <td>96.628784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>85.060983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031196</td>\n",
       "      <td>0.031196</td>\n",
       "      <td>93.195948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>106.329723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031834</td>\n",
       "      <td>0.031834</td>\n",
       "      <td>91.440270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031855</td>\n",
       "      <td>0.031855</td>\n",
       "      <td>115.467294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032746</td>\n",
       "      <td>0.032746</td>\n",
       "      <td>81.647340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033437</td>\n",
       "      <td>0.033437</td>\n",
       "      <td>115.134228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.033477</td>\n",
       "      <td>99.463781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033642</td>\n",
       "      <td>0.033642</td>\n",
       "      <td>86.687962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034466</td>\n",
       "      <td>0.034466</td>\n",
       "      <td>101.009560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034528</td>\n",
       "      <td>0.034528</td>\n",
       "      <td>87.707354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034959</td>\n",
       "      <td>0.034959</td>\n",
       "      <td>67.474718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035040</td>\n",
       "      <td>0.035040</td>\n",
       "      <td>71.193889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035576</td>\n",
       "      <td>0.035576</td>\n",
       "      <td>68.450746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035651</td>\n",
       "      <td>0.035651</td>\n",
       "      <td>84.483640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035845</td>\n",
       "      <td>0.035845</td>\n",
       "      <td>103.792361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035907</td>\n",
       "      <td>0.035907</td>\n",
       "      <td>72.577676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036443</td>\n",
       "      <td>0.036443</td>\n",
       "      <td>87.556474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036590</td>\n",
       "      <td>0.036590</td>\n",
       "      <td>77.365908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.037969</td>\n",
       "      <td>0.037969</td>\n",
       "      <td>33.194931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038371</td>\n",
       "      <td>0.038371</td>\n",
       "      <td>65.540677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.038764</td>\n",
       "      <td>0.038764</td>\n",
       "      <td>91.380220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.039640</td>\n",
       "      <td>0.039640</td>\n",
       "      <td>53.052170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.039820</td>\n",
       "      <td>0.039820</td>\n",
       "      <td>96.615848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.039910</td>\n",
       "      <td>0.039910</td>\n",
       "      <td>21.636767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.040375</td>\n",
       "      <td>0.040375</td>\n",
       "      <td>69.234404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>8</td>\n",
       "      <td>0.041904</td>\n",
       "      <td>0.041904</td>\n",
       "      <td>53.386548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.044494</td>\n",
       "      <td>0.044494</td>\n",
       "      <td>70.657355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.049623</td>\n",
       "      <td>0.049623</td>\n",
       "      <td>68.995120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.050520</td>\n",
       "      <td>0.050520</td>\n",
       "      <td>19.645350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055958</td>\n",
       "      <td>0.055958</td>\n",
       "      <td>87.633617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.057508</td>\n",
       "      <td>0.057508</td>\n",
       "      <td>29.325452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.059346</td>\n",
       "      <td>0.059346</td>\n",
       "      <td>65.793058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.059349</td>\n",
       "      <td>0.059349</td>\n",
       "      <td>72.570607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.059962</td>\n",
       "      <td>0.059962</td>\n",
       "      <td>70.869356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>16</td>\n",
       "      <td>0.067849</td>\n",
       "      <td>0.067849</td>\n",
       "      <td>29.628062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.101297</td>\n",
       "      <td>0.101297</td>\n",
       "      <td>105.219672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.262671</td>\n",
       "      <td>0.262671</td>\n",
       "      <td>60.544115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.488958</td>\n",
       "      <td>0.488958</td>\n",
       "      <td>19.797006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.523533</td>\n",
       "      <td>0.523533</td>\n",
       "      <td>26.067689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>0.639098</td>\n",
       "      <td>0.639098</td>\n",
       "      <td>21.625296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>1.413958</td>\n",
       "      <td>1.413958</td>\n",
       "      <td>27.703692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             4        100         0.0001           4  0.030271  0.030271   \n",
       "1             4        100         0.0001           4  0.030928  0.030928   \n",
       "2             4        100         0.0001           4  0.031196  0.031196   \n",
       "3             4        200         0.0001           4  0.031805  0.031805   \n",
       "4             4        100         0.0001           4  0.031834  0.031834   \n",
       "5             4        150         0.0001           4  0.031855  0.031855   \n",
       "6             3        100         0.0001           4  0.032746  0.032746   \n",
       "7             4        100         0.0001           4  0.033437  0.033437   \n",
       "8             4        100         0.0001           4  0.033477  0.033477   \n",
       "9             4        100         0.0001           4  0.033642  0.033642   \n",
       "10            4        100         0.0001           4  0.034466  0.034466   \n",
       "11            4        100         0.0001           4  0.034528  0.034528   \n",
       "12            2        100         0.0001           4  0.034959  0.034959   \n",
       "13            2        100         0.0001           4  0.035040  0.035040   \n",
       "14            2        100         0.0001           4  0.035576  0.035576   \n",
       "15            3         50         0.0001           4  0.035651  0.035651   \n",
       "16            4        100         0.0001           4  0.035845  0.035845   \n",
       "17            2         50         0.0001           4  0.035907  0.035907   \n",
       "18            2        100         0.0001           4  0.036443  0.036443   \n",
       "19            2        100         0.0001           4  0.036590  0.036590   \n",
       "20            4        100         0.0001          16  0.037969  0.037969   \n",
       "21            2         50         0.0001           4  0.038371  0.038371   \n",
       "22            2        100         0.0001           4  0.038764  0.038764   \n",
       "23            1        200         0.0050           4  0.039640  0.039640   \n",
       "24            2        100         0.0001           4  0.039820  0.039820   \n",
       "25            4        100         0.0001          16  0.039910  0.039910   \n",
       "26            2         50         0.0001           4  0.040375  0.040375   \n",
       "27            4        200         0.0050           8  0.041904  0.041904   \n",
       "28            3        100         0.0050           4  0.044494  0.044494   \n",
       "29            1        100         0.0050           4  0.049623  0.049623   \n",
       "30            2        200         0.0001          16  0.050520  0.050520   \n",
       "31            3        100         0.0050           4  0.055958  0.055958   \n",
       "32            4        200         0.0050          16  0.057508  0.057508   \n",
       "33            1        200         0.0001           4  0.059346  0.059346   \n",
       "34            1        200         0.0001           4  0.059349  0.059349   \n",
       "35            1        200         0.0001           4  0.059962  0.059962   \n",
       "36            4        150         0.0050          16  0.067849  0.067849   \n",
       "37            3        100         0.0050           2  0.101297  0.101297   \n",
       "38            1        100         0.0001           4  0.262671  0.262671   \n",
       "39            1        200         0.0001          16  0.488958  0.488958   \n",
       "40            1        100         0.0001          16  0.523533  0.523533   \n",
       "41            1        200         0.0001          16  0.639098  0.639098   \n",
       "42            1        100         0.0001          16  1.413958  1.413958   \n",
       "\n",
       "    Elapsed time  \n",
       "0      96.628784  \n",
       "1      85.060983  \n",
       "2      93.195948  \n",
       "3     106.329723  \n",
       "4      91.440270  \n",
       "5     115.467294  \n",
       "6      81.647340  \n",
       "7     115.134228  \n",
       "8      99.463781  \n",
       "9      86.687962  \n",
       "10    101.009560  \n",
       "11     87.707354  \n",
       "12     67.474718  \n",
       "13     71.193889  \n",
       "14     68.450746  \n",
       "15     84.483640  \n",
       "16    103.792361  \n",
       "17     72.577676  \n",
       "18     87.556474  \n",
       "19     77.365908  \n",
       "20     33.194931  \n",
       "21     65.540677  \n",
       "22     91.380220  \n",
       "23     53.052170  \n",
       "24     96.615848  \n",
       "25     21.636767  \n",
       "26     69.234404  \n",
       "27     53.386548  \n",
       "28     70.657355  \n",
       "29     68.995120  \n",
       "30     19.645350  \n",
       "31     87.633617  \n",
       "32     29.325452  \n",
       "33     65.793058  \n",
       "34     72.570607  \n",
       "35     70.869356  \n",
       "36     29.628062  \n",
       "37    105.219672  \n",
       "38     60.544115  \n",
       "39     19.797006  \n",
       "40     26.067689  \n",
       "41     21.625296  \n",
       "42     27.703692  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla2.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 50.880 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
