{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "# from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>delta</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135.689107</td>\n",
       "      <td>32.494632</td>\n",
       "      <td>23.87882</td>\n",
       "      <td>22.27530</td>\n",
       "      <td>20.39501</td>\n",
       "      <td>19.16573</td>\n",
       "      <td>18.79371</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144.826101</td>\n",
       "      <td>31.274185</td>\n",
       "      <td>24.77759</td>\n",
       "      <td>22.83188</td>\n",
       "      <td>22.58444</td>\n",
       "      <td>21.16812</td>\n",
       "      <td>21.61427</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142.188790</td>\n",
       "      <td>35.582444</td>\n",
       "      <td>25.26307</td>\n",
       "      <td>22.66389</td>\n",
       "      <td>20.60976</td>\n",
       "      <td>19.34857</td>\n",
       "      <td>18.94827</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>338.741038</td>\n",
       "      <td>-0.402828</td>\n",
       "      <td>22.13682</td>\n",
       "      <td>23.77656</td>\n",
       "      <td>21.61162</td>\n",
       "      <td>20.50454</td>\n",
       "      <td>19.25010</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345.282593</td>\n",
       "      <td>21.183866</td>\n",
       "      <td>19.43718</td>\n",
       "      <td>17.58028</td>\n",
       "      <td>16.49747</td>\n",
       "      <td>15.97711</td>\n",
       "      <td>15.54461</td>\n",
       "      <td>GALAXY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        alpha      delta         u         g         r         i         z  \\\n",
       "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
       "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
       "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
       "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
       "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
       "\n",
       "    class  \n",
       "0  GALAXY  \n",
       "1  GALAXY  \n",
       "2  GALAXY  \n",
       "3  GALAXY  \n",
       "4  GALAXY  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/igomezv/nnogada/main/data/star_classification.csv\"\n",
    "data = pd.read_csv(url)\n",
    "cols = ['alpha','delta','u','g','r','i','z','class']\n",
    "data = data[cols]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        alpha      delta         u         g         r         i         z  \\\n",
      "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
      "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
      "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
      "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
      "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
      "\n",
      "   class  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n"
     ]
    }
   ],
   "source": [
    "data[\"class\"]=[0 if i == \"GALAXY\" else 1 if i == \"STAR\" else 2 for i in data[\"class\"]]\n",
    "print(data.head())\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function layers\n",
    "\n",
    "# f1 = lambda x: Dense(x, activation='relu')      #ReLU\n",
    "# f2 = lambda x: Dense(x, activation='elu')       #ELU\n",
    "# f3 = lambda x: keras.layers.LeakyReLU(0.3)      #LReLU\n",
    "# f4 = lambda x: Dense(x, kernel_initializer='lecun_normal', activation='selu')   #SELU\n",
    "\n",
    "# f_names = [\"ReLU\", \"ELU\", \"LReLU\", \"SELU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([3, 4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([200, 100]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-5,1e-4,1e-3, 0.01])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([128,256]) \n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=50,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=1)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 50\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into X and Y and implement hot_ones in Y\n",
    "def prepare_dataset(data):\n",
    "    X, Y = np.empty((0)), np.empty((0))\n",
    "    X = data[:,0:7]\n",
    "    Y = data[:,7]\n",
    "    Y = to_categorical(Y, num_classes=3)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation and test sets\n",
    "X,Y = prepare_dataset(data)\n",
    "\n",
    "# Defines ratios, w.r.t. whole dataset.\n",
    "ratio_train = 0.8\n",
    "ratio_val = 0.1\n",
    "ratio_test = 0.1\n",
    "\n",
    "# Produces test split.\n",
    "x_, X_test, y_, Y_test = split(X, Y, test_size = ratio_test, random_state=0)\n",
    "\n",
    "# Adjusts val ratio, w.r.t. remaining dataset.\n",
    "ratio_remaining = 1 - ratio_test\n",
    "ratio_val_adjusted = ratio_val / ratio_remaining\n",
    "\n",
    "# Produces train and val splits.\n",
    "X_train, X_val, Y_train, Y_val = split(x_, y_, test_size=ratio_val_adjusted, random_state=0)\n",
    "\n",
    "# Normalize and scale the input sets.\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "X_val   = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:1])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[1:2])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[2:4])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[4:5])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(num_units, input_shape=(int(X_train.shape[1]),)))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(3, activation=tf.nn.softmax))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=1, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Accuracy:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5      # probability for crossover\n",
    "    P_MUTATION = 0.8         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1  # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 926us/step - loss: 0.3531 - accuracy: 0.8700\n",
      "Accuracy: 0.8700000047683716 , Elapsed time: 64.72314786911011\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 935us/step - loss: 0.3508 - accuracy: 0.8734\n",
      "Accuracy: 0.8733999729156494 , Elapsed time: 106.05557703971863\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3322 - accuracy: 0.8792\n",
      "Accuracy: 0.8791999816894531 , Elapsed time: 93.7284927368164\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3723 - accuracy: 0.8625\n",
      "Accuracy: 0.862500011920929 , Elapsed time: 90.73195815086365\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3640 - accuracy: 0.8668\n",
      "Accuracy: 0.8668000102043152 , Elapsed time: 99.61862325668335\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin     \tavg     \tmax     \n",
      "0  \t5     \t0.332216\t0.354495\t0.372314\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8693\n",
      "Accuracy: 0.8693000078201294 , Elapsed time: 147.9516978263855\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3704 - accuracy: 0.8596\n",
      "Accuracy: 0.8596000075340271 , Elapsed time: 81.5924301147461\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 4 , Number of neurons: 200 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3553 - accuracy: 0.8719\n",
      "Accuracy: 0.8719000220298767 , Elapsed time: 133.3344485759735\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3763 - accuracy: 0.8634\n",
      "Accuracy: 0.8633999824523926 , Elapsed time: 105.04908299446106\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t4     \t0.332216\t0.360867\t0.376251\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3463 - accuracy: 0.8735\n",
      "Accuracy: 0.8734999895095825 , Elapsed time: 100.73330068588257\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3286 - accuracy: 0.8799\n",
      "Accuracy: 0.8798999786376953 , Elapsed time: 98.52139711380005\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3473 - accuracy: 0.8772\n",
      "Accuracy: 0.8772000074386597 , Elapsed time: 87.75819492340088\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3357 - accuracy: 0.8800\n",
      "Accuracy: 0.8799999952316284 , Elapsed time: 87.97191381454468\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t4     \t0.328556\t0.338012\t0.347253\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3363 - accuracy: 0.8801\n",
      "Accuracy: 0.8801000118255615 , Elapsed time: 77.5968029499054\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3324 - accuracy: 0.8800\n",
      "Accuracy: 0.8799999952316284 , Elapsed time: 94.89722418785095\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t2     \t0.328556\t0.335143\t0.346302\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3462 - accuracy: 0.8782\n",
      "Accuracy: 0.8781999945640564 , Elapsed time: 108.02852606773376\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3367 - accuracy: 0.8765\n",
      "Accuracy: 0.8765000104904175 , Elapsed time: 105.91834044456482\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3287 - accuracy: 0.8816\n",
      "Accuracy: 0.881600022315979 , Elapsed time: 88.206871509552\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3627 - accuracy: 0.8680\n",
      "Accuracy: 0.8679999709129333 , Elapsed time: 143.154278755188\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t4     \t0.328702\t0.341317\t0.362716\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3450 - accuracy: 0.8735\n",
      "Accuracy: 0.8734999895095825 , Elapsed time: 102.11642527580261\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3367 - accuracy: 0.8734\n",
      "Accuracy: 0.8733999729156494 , Elapsed time: 85.86418032646179\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3363 - accuracy: 0.8776\n",
      "Accuracy: 0.8776000142097473 , Elapsed time: 68.71705055236816\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3452 - accuracy: 0.8737\n",
      "Accuracy: 0.8737000226974487 , Elapsed time: 73.5491726398468\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.328702\t0.338379\t0.345228\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3540 - accuracy: 0.8704\n",
      "Accuracy: 0.8704000115394592 , Elapsed time: 76.45562815666199\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3307 - accuracy: 0.8820\n",
      "Accuracy: 0.8820000290870667 , Elapsed time: 80.97073745727539\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8803\n",
      "Accuracy: 0.880299985408783 , Elapsed time: 143.0320155620575\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t3     \t0.328702\t0.334672\t0.354001\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3241 - accuracy: 0.8822\n",
      "Accuracy: 0.8822000026702881 , Elapsed time: 79.40880537033081\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3392 - accuracy: 0.8761\n",
      "Accuracy: 0.8761000037193298 , Elapsed time: 79.3957896232605\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3374 - accuracy: 0.8729\n",
      "Accuracy: 0.8729000091552734 , Elapsed time: 86.02069163322449\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3336 - accuracy: 0.8728\n",
      "Accuracy: 0.8727999925613403 , Elapsed time: 100.16922926902771\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.324146\t0.332615\t0.339168\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 0.3423 - accuracy: 0.8763\n",
      "Accuracy: 0.8762999773025513 , Elapsed time: 99.82644081115723\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3594 - accuracy: 0.8638\n",
      "Accuracy: 0.8637999892234802 , Elapsed time: 136.5521318912506\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3410 - accuracy: 0.8733\n",
      "Accuracy: 0.8733000159263611 , Elapsed time: 74.17702841758728\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8776\n",
      "Accuracy: 0.8776000142097473 , Elapsed time: 85.05027341842651\n",
      "-------------------------------------------------\n",
      "\n",
      "8  \t4     \t0.328702\t0.341029\t0.359371\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3103 - accuracy: 0.8854\n",
      "Accuracy: 0.8853999972343445 , Elapsed time: 136.08332133293152\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3406 - accuracy: 0.8767\n",
      "Accuracy: 0.8766999840736389 , Elapsed time: 134.17762970924377\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3330 - accuracy: 0.8770\n",
      "Accuracy: 0.8769999742507935 , Elapsed time: 86.18205738067627\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3462 - accuracy: 0.8746\n",
      "Accuracy: 0.8745999932289124 , Elapsed time: 85.79585266113281\n",
      "-------------------------------------------------\n",
      "\n",
      "9  \t4     \t0.310342\t0.331762\t0.346184\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3157 - accuracy: 0.8856\n",
      "Accuracy: 0.8855999708175659 , Elapsed time: 123.1174635887146\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3141 - accuracy: 0.8875\n",
      "Accuracy: 0.887499988079071 , Elapsed time: 126.75196719169617\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.01\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3433 - accuracy: 0.8710\n",
      "Accuracy: 0.8709999918937683 , Elapsed time: 102.27882814407349\n",
      "-------------------------------------------------\n",
      "\n",
      "10 \t3     \t0.310342\t0.322435\t0.34328 \n",
      "-- Best Individual =  [0, 0, 1, 0, 0]\n",
      "-- Best Fitness =  0.31034204363822937\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 68.02823906739553 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 5  # max of individuals per generation\n",
    "max_generations = 10   # number of generations\n",
    "gene_length = 5      # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 3 , Number of neurons: 200 , Learning rate: 0.001 , Batch size: 128\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:4])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[4:5])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    \n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1], \n",
    "          ', Learning rate:', best_learning_rate[-1], ', Batch size:', best_batch_size[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.314123</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>126.751967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.315728</td>\n",
       "      <td>0.8856</td>\n",
       "      <td>123.117464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.310342</td>\n",
       "      <td>0.8854</td>\n",
       "      <td>136.083321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.324146</td>\n",
       "      <td>0.8822</td>\n",
       "      <td>79.408805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.330727</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>80.970737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.328702</td>\n",
       "      <td>0.8816</td>\n",
       "      <td>88.206872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.331229</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>143.032016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.336273</td>\n",
       "      <td>0.8801</td>\n",
       "      <td>77.596803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.335735</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>87.971914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.332368</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>94.897224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.328556</td>\n",
       "      <td>0.8799</td>\n",
       "      <td>98.521397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.332216</td>\n",
       "      <td>0.8792</td>\n",
       "      <td>93.728493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.346204</td>\n",
       "      <td>0.8782</td>\n",
       "      <td>108.028526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.336282</td>\n",
       "      <td>0.8776</td>\n",
       "      <td>68.717051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.333800</td>\n",
       "      <td>0.8776</td>\n",
       "      <td>85.050273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.347253</td>\n",
       "      <td>0.8772</td>\n",
       "      <td>87.758195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.332989</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>86.182057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.340593</td>\n",
       "      <td>0.8767</td>\n",
       "      <td>134.177630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.336748</td>\n",
       "      <td>0.8765</td>\n",
       "      <td>105.918340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.342315</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>99.826441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.339168</td>\n",
       "      <td>0.8761</td>\n",
       "      <td>79.395790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.346184</td>\n",
       "      <td>0.8746</td>\n",
       "      <td>85.795853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.345228</td>\n",
       "      <td>0.8737</td>\n",
       "      <td>73.549173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.346302</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>100.733301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.344957</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>102.116425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.336727</td>\n",
       "      <td>0.8734</td>\n",
       "      <td>85.864180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.350814</td>\n",
       "      <td>0.8734</td>\n",
       "      <td>106.055577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.340958</td>\n",
       "      <td>0.8733</td>\n",
       "      <td>74.177028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.337439</td>\n",
       "      <td>0.8729</td>\n",
       "      <td>86.020692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.333619</td>\n",
       "      <td>0.8728</td>\n",
       "      <td>100.169229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.355277</td>\n",
       "      <td>0.8719</td>\n",
       "      <td>133.334449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.343280</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>102.278828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.354001</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>76.455628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.353130</td>\n",
       "      <td>0.8700</td>\n",
       "      <td>64.723148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.370140</td>\n",
       "      <td>0.8693</td>\n",
       "      <td>147.951698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.362716</td>\n",
       "      <td>0.8680</td>\n",
       "      <td>143.154279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.364001</td>\n",
       "      <td>0.8668</td>\n",
       "      <td>99.618623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.359371</td>\n",
       "      <td>0.8638</td>\n",
       "      <td>136.552132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.376251</td>\n",
       "      <td>0.8634</td>\n",
       "      <td>105.049083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.372314</td>\n",
       "      <td>0.8625</td>\n",
       "      <td>90.731958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.370450</td>\n",
       "      <td>0.8596</td>\n",
       "      <td>81.592430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss  Accuracy  \\\n",
       "0             3        200         0.0010         128  0.314123    0.8875   \n",
       "1             3        200         0.0010         128  0.315728    0.8856   \n",
       "2             3        200         0.0010         128  0.310342    0.8854   \n",
       "3             3        100         0.0100         128  0.324146    0.8822   \n",
       "4             3        100         0.0100         128  0.330727    0.8820   \n",
       "5             3        100         0.0100         128  0.328702    0.8816   \n",
       "6             4        100         0.0100         128  0.331229    0.8803   \n",
       "7             3        100         0.0100         128  0.336273    0.8801   \n",
       "8             4        100         0.0100         128  0.335735    0.8800   \n",
       "9             4        100         0.0100         128  0.332368    0.8800   \n",
       "10            4        100         0.0100         128  0.328556    0.8799   \n",
       "11            4        100         0.0100         128  0.332216    0.8792   \n",
       "12            4        100         0.0100         128  0.346204    0.8782   \n",
       "13            3        100         0.0100         128  0.336282    0.8776   \n",
       "14            3        100         0.0100         128  0.333800    0.8776   \n",
       "15            4        100         0.0100         128  0.347253    0.8772   \n",
       "16            3        100         0.0100         128  0.332989    0.8770   \n",
       "17            3        200         0.0100         128  0.340593    0.8767   \n",
       "18            4        100         0.0100         128  0.336748    0.8765   \n",
       "19            3        100         0.0100         128  0.342315    0.8763   \n",
       "20            3        100         0.0100         128  0.339168    0.8761   \n",
       "21            3        100         0.0100         128  0.346184    0.8746   \n",
       "22            3        100         0.0100         128  0.345228    0.8737   \n",
       "23            4        100         0.0100         128  0.346302    0.8735   \n",
       "24            4        100         0.0100         128  0.344957    0.8735   \n",
       "25            3        100         0.0100         128  0.336727    0.8734   \n",
       "26            4        200         0.0100         128  0.350814    0.8734   \n",
       "27            3        100         0.0100         128  0.340958    0.8733   \n",
       "28            3        100         0.0100         128  0.337439    0.8729   \n",
       "29            3        100         0.0100         128  0.333619    0.8728   \n",
       "30            4        200         0.0001         128  0.355277    0.8719   \n",
       "31            4        100         0.0100         128  0.343280    0.8710   \n",
       "32            3        100         0.0100         128  0.354001    0.8704   \n",
       "33            4        100         0.0100         128  0.353130    0.8700   \n",
       "34            4        200         0.0100         128  0.370140    0.8693   \n",
       "35            4        100         0.0001         128  0.362716    0.8680   \n",
       "36            4        100         0.0001         128  0.364001    0.8668   \n",
       "37            3        200         0.0100         128  0.359371    0.8638   \n",
       "38            4        100         0.0001         128  0.376251    0.8634   \n",
       "39            3        200         0.0100         256  0.372314    0.8625   \n",
       "40            4        100         0.0100         128  0.370450    0.8596   \n",
       "\n",
       "    Elapsed time  \n",
       "0     126.751967  \n",
       "1     123.117464  \n",
       "2     136.083321  \n",
       "3      79.408805  \n",
       "4      80.970737  \n",
       "5      88.206872  \n",
       "6     143.032016  \n",
       "7      77.596803  \n",
       "8      87.971914  \n",
       "9      94.897224  \n",
       "10     98.521397  \n",
       "11     93.728493  \n",
       "12    108.028526  \n",
       "13     68.717051  \n",
       "14     85.050273  \n",
       "15     87.758195  \n",
       "16     86.182057  \n",
       "17    134.177630  \n",
       "18    105.918340  \n",
       "19     99.826441  \n",
       "20     79.395790  \n",
       "21     85.795853  \n",
       "22     73.549173  \n",
       "23    100.733301  \n",
       "24    102.116425  \n",
       "25     85.864180  \n",
       "26    106.055577  \n",
       "27     74.177028  \n",
       "28     86.020692  \n",
       "29    100.169229  \n",
       "30    133.334449  \n",
       "31    102.278828  \n",
       "32     76.455628  \n",
       "33     64.723148  \n",
       "34    147.951698  \n",
       "35    143.154279  \n",
       "36     99.618623  \n",
       "37    136.552132  \n",
       "38    105.049083  \n",
       "39     90.731958  \n",
       "40     81.592430  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_sdss_3.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Accuracy\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Accuracy\", \"Elapsed time\"], ascending=[0,1], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 68.021 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
